{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 1.2 - Линейный классификатор (Linear classifier)\n",
    "\n",
    "В этом задании мы реализуем другую модель машинного обучения - линейный классификатор. Линейный классификатор подбирает для каждого класса веса, на которые нужно умножить значение каждого признака и потом сложить вместе.\n",
    "Тот класс, у которого эта сумма больше, и является предсказанием модели.\n",
    "\n",
    "В этом задании вы:\n",
    "- потренируетесь считать градиенты различных многомерных функций\n",
    "- реализуете подсчет градиентов через линейную модель и функцию потерь softmax\n",
    "- реализуете процесс тренировки линейного классификатора\n",
    "- подберете параметры тренировки на практике\n",
    "\n",
    "На всякий случай, еще раз ссылка на туториал по numpy:  \n",
    "http://cs231n.github.io/python-numpy-tutorial/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import load_svhn, random_split_train_val\n",
    "from gradient_check import check_gradient\n",
    "from metrics import multiclass_accuracy \n",
    "import linear_classifer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Как всегда, первым делом загружаем данные\n",
    "\n",
    "Мы будем использовать все тот же SVHN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_linear_classifier(train_X, test_X):\n",
    "    train_flat = train_X.reshape(train_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    test_flat = test_X.reshape(test_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    \n",
    "    # Subtract mean\n",
    "    mean_image = np.mean(train_flat, axis = 0)\n",
    "    train_flat -= mean_image\n",
    "    test_flat -= mean_image\n",
    "    \n",
    "    # Add another channel with ones as a bias term\n",
    "    train_flat_with_ones = np.hstack([train_flat, np.ones((train_X.shape[0], 1))])\n",
    "    test_flat_with_ones = np.hstack([test_flat, np.ones((test_X.shape[0], 1))])    \n",
    "    return train_flat_with_ones, test_flat_with_ones\n",
    "    \n",
    "train_X, train_y, test_X, test_y = load_svhn(\"data\", max_train=10000, max_test=1000)    \n",
    "train_X, test_X = prepare_for_linear_classifier(train_X, test_X)\n",
    "# Split train into train and val\n",
    "train_X, train_y, val_X, val_y = random_split_train_val(train_X, train_y, num_val = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Играемся с градиентами!\n",
    "\n",
    "В этом курсе мы будем писать много функций, которые вычисляют градиенты аналитическим методом.\n",
    "\n",
    "Все функции, в которых мы будем вычислять градиенты, будут написаны по одной и той же схеме.  \n",
    "Они будут получать на вход точку, где нужно вычислить значение и градиент функции, а на выходе будут выдавать кортеж (tuple) из двух значений - собственно значения функции в этой точке (всегда одно число) и аналитического значения градиента в той же точке (той же размерности, что и вход).\n",
    "```\n",
    "def f(x):\n",
    "    \"\"\"\n",
    "    Computes function and analytic gradient at x\n",
    "    \n",
    "    x: np array of float, input to the function\n",
    "    \n",
    "    Returns:\n",
    "    value: float, value of the function \n",
    "    grad: np array of float, same shape as x\n",
    "    \"\"\"\n",
    "    ...\n",
    "    \n",
    "    return value, grad\n",
    "```\n",
    "\n",
    "Необходимым инструментом во время реализации кода, вычисляющего градиенты, является функция его проверки. Эта функция вычисляет градиент численным методом и сверяет результат с градиентом, вычисленным аналитическим методом.\n",
    "\n",
    "Мы начнем с того, чтобы реализовать вычисление численного градиента (numeric gradient) в функции `check_gradient` в `gradient_check.py`. Эта функция будет принимать на вход функции формата, заданного выше, использовать значение `value` для вычисления численного градиента и сравнит его с аналитическим - они должны сходиться.\n",
    "\n",
    "Напишите часть функции, которая вычисляет градиент с помощью численной производной для каждой координаты. Для вычисления производной используйте так называемую two-point formula (https://en.wikipedia.org/wiki/Numerical_differentiation):\n",
    "\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/22fc2c0a66c63560a349604f8b6b39221566236d)\n",
    "\n",
    "Все функции приведенные в следующей клетке должны проходить gradient check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradients are not different at (0,). Analytic: 6.00000, Numeric: 6.00000\n",
      "Gradient check passed!\n",
      "Gradients are not different at (0,). Analytic: 1.00000, Numeric: 1.00000\n",
      "Gradients are not different at (1,). Analytic: 1.00000, Numeric: 1.00000\n",
      "Gradient check passed!\n",
      "Gradients are not different at (0, 0). Analytic: 1.00000, Numeric: 1.00000\n",
      "Gradients are not different at (0, 1). Analytic: 1.00000, Numeric: 1.00000\n",
      "Gradients are not different at (1, 0). Analytic: 1.00000, Numeric: 1.00000\n",
      "Gradients are not different at (1, 1). Analytic: 1.00000, Numeric: 1.00000\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Implement check_gradient function in gradient_check.py\n",
    "# All the functions below should pass the gradient check\n",
    "\n",
    "def square(x):\n",
    "    return float(x*x), 2*x\n",
    "\n",
    "check_gradient(square, np.array([3.0]))\n",
    "\n",
    "def array_sum(x):\n",
    "    assert x.shape == (2,), x.shape\n",
    "    return np.sum(x), np.ones_like(x)\n",
    "\n",
    "check_gradient(array_sum, np.array([3.0, 2.0]))\n",
    "\n",
    "def array_2d_sum(x):\n",
    "    assert x.shape == (2,2)\n",
    "    return np.sum(x), np.ones_like(x)\n",
    "\n",
    "check_gradient(array_2d_sum, np.array([[3.0, 2.0], [1.0, 0.0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Начинаем писать свои функции, считающие аналитический градиент\n",
    "\n",
    "Теперь реализуем функцию softmax, которая получает на вход оценки для каждого класса и преобразует их в вероятности от 0 до 1:\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/e348290cf48ddbb6e9a6ef4e39363568b67c09d3)\n",
    "\n",
    "**Важно:** Практический аспект вычисления этой функции заключается в том, что в ней учавствует вычисление экспоненты от потенциально очень больших чисел - это может привести к очень большим значениям в числителе и знаменателе за пределами диапазона float.\n",
    "\n",
    "К счастью, у этой проблемы есть простое решение -- перед вычислением softmax вычесть из всех оценок максимальное значение среди всех оценок:\n",
    "```\n",
    "predictions -= np.max(predictions)\n",
    "```\n",
    "(подробнее здесь - http://cs231n.github.io/linear-classify/#softmax, секция `Practical issues: Numeric stability`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Implement softmax and cross-entropy for single sample\n",
    "probs = linear_classifer.softmax(np.array([-10, 0, 10]))\n",
    "\n",
    "# Make sure it works for big numbers too!\n",
    "probs = linear_classifer.softmax(np.array([1000, 0, 0]))\n",
    "assert np.isclose(probs[0], 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кроме этого, мы реализуем cross-entropy loss, которую мы будем использовать как функцию ошибки (error function).\n",
    "В общем виде cross-entropy определена следующим образом:\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/0cb6da032ab424eefdca0884cd4113fe578f4293)\n",
    "\n",
    "где x - все классы, p(x) - истинная вероятность принадлежности сэмпла классу x, а q(x) - вероятность принадлежности классу x, предсказанная моделью.  \n",
    "В нашем случае сэмпл принадлежит только одному классу, индекс которого передается функции. Для него p(x) равна 1, а для остальных классов - 0. \n",
    "\n",
    "Это позволяет реализовать функцию проще!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.006760443547122"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = linear_classifer.softmax(np.array([-5, 0, 5]))\n",
    "linear_classifer.cross_entropy_loss(probs, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После того как мы реализовали сами функции, мы можем реализовать градиент.\n",
    "\n",
    "Оказывается, что вычисление градиента становится гораздо проще, если объединить эти функции в одну, которая сначала вычисляет вероятности через softmax, а потом использует их для вычисления функции ошибки через cross-entropy loss.\n",
    "\n",
    "Эта функция `softmax_with_cross_entropy` будет возвращает и значение ошибки, и градиент по входным параметрам. Мы проверим корректность реализации с помощью `check_gradient`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradients are not different at (0,). Analytic: 0.57612, Numeric: 0.57612\n",
      "Gradients are not different at (1,). Analytic: -0.78806, Numeric: -0.78806\n",
      "Gradients are not different at (2,). Analytic: 0.21194, Numeric: 0.21194\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement combined function or softmax and cross entropy and produces gradient\n",
    "loss, grad = linear_classifer.softmax_with_cross_entropy(np.array([1, 0, 0]), 1)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, 1), np.array([1, 0, 0], np.float))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве метода тренировки мы будем использовать стохастический градиентный спуск (stochastic gradient descent или SGD), который работает с батчами сэмплов. \n",
    "\n",
    "Поэтому все наши фукнции будут получать не один пример, а батч, то есть входом будет не вектор из `num_classes` оценок, а матрица размерности `batch_size, num_classes`. Индекс примера в батче всегда будет первым измерением.\n",
    "\n",
    "Следующий шаг - переписать наши функции так, чтобы они поддерживали батчи.\n",
    "\n",
    "Финальное значение функции ошибки должно остаться числом, и оно равно среднему значению ошибки среди всех примеров в батче."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradients are not different at (0, 0). Analytic: 0.20603, Numeric: 0.20603\n",
      "Gradients are not different at (0, 1). Analytic: 0.56005, Numeric: 0.56005\n",
      "Gradients are not different at (0, 2). Analytic: -0.97212, Numeric: -0.97212\n",
      "Gradients are not different at (0, 3). Analytic: 0.20603, Numeric: 0.20603\n",
      "Gradient check passed!\n",
      "Gradients are not different at (0, 0). Analytic: 0.68145, Numeric: 0.68145\n",
      "Gradients are not different at (0, 1). Analytic: 0.03393, Numeric: 0.03393\n",
      "Gradients are not different at (0, 2). Analytic: 0.03393, Numeric: 0.03393\n",
      "Gradients are not different at (0, 3). Analytic: -0.74931, Numeric: -0.74931\n",
      "Gradients are not different at (1, 0). Analytic: 0.10923, Numeric: 0.10923\n",
      "Gradients are not different at (1, 1). Analytic: 0.29692, Numeric: 0.29692\n",
      "Gradients are not different at (1, 2). Analytic: 0.29692, Numeric: 0.29692\n",
      "Gradients are not different at (1, 3). Analytic: -0.70308, Numeric: -0.70308\n",
      "Gradients are not different at (2, 0). Analytic: 0.15216, Numeric: 0.15216\n",
      "Gradients are not different at (2, 1). Analytic: 0.41362, Numeric: 0.41362\n",
      "Gradients are not different at (2, 2). Analytic: -0.97941, Numeric: -0.97941\n",
      "Gradients are not different at (2, 3). Analytic: 0.41362, Numeric: 0.41362\n",
      "Gradient check passed!\n"
     ]
    }
   ],
   "source": [
    "# TODO Extend combined function so it can receive a 2d array with batch of samples\n",
    "np.random.seed(42)\n",
    "# Test batch_size = 1\n",
    "num_classes = 4\n",
    "batch_size = 1\n",
    "predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(np.float)\n",
    "target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(np.int)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, target_index), predictions)\n",
    "\n",
    "# Test batch_size = 3\n",
    "num_classes = 4\n",
    "batch_size = 3\n",
    "predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(np.float)\n",
    "target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(np.int)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, target_index), predictions)\n",
    "\n",
    "# Make sure maximum subtraction for numberic stability is done separately for every sample in the batch\n",
    "probs = linear_classifer.softmax(np.array([[20,0,0], [1000, 0, 0]]))\n",
    "assert np.all(np.isclose(probs[:, 0], 1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.eye(3)[[2, 1, 1, 0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Наконец, реализуем сам линейный классификатор!\n",
    "\n",
    "softmax и cross-entropy получают на вход оценки, которые выдает линейный классификатор.\n",
    "\n",
    "Он делает это очень просто: для каждого класса есть набор весов, на которые надо умножить пиксели картинки и сложить. Получившееся число и является оценкой класса, идущей на вход softmax.\n",
    "\n",
    "Таким образом, линейный классификатор можно представить как умножение вектора с пикселями на матрицу W размера `num_features, num_classes`. Такой подход легко расширяется на случай батча векторов с пикселями X размера `batch_size, num_features`:\n",
    "\n",
    "`predictions = X * W`, где `*` - матричное умножение.\n",
    "\n",
    "Реализуйте функцию подсчета линейного классификатора и градиентов по весам `linear_softmax` в файле `linear_classifer.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradients are not different at (0, 0). Analytic: -0.88080, Numeric: -0.88080\n",
      "Gradients are not different at (0, 1). Analytic: 0.88080, Numeric: 0.88080\n",
      "Gradients are not different at (1, 0). Analytic: -0.83337, Numeric: -0.83337\n",
      "Gradients are not different at (1, 1). Analytic: 0.83337, Numeric: 0.83337\n",
      "Gradients are not different at (2, 0). Analytic: 0.92822, Numeric: 0.92822\n",
      "Gradients are not different at (2, 1). Analytic: -0.92822, Numeric: -0.92822\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement linear_softmax function that uses softmax with cross-entropy for linear classifier\n",
    "batch_size = 2\n",
    "num_classes = 2\n",
    "num_features = 3\n",
    "np.random.seed(42)\n",
    "W = np.random.randint(-1, 3, size=(num_features, num_classes)).astype(np.float)\n",
    "X = np.random.randint(-1, 3, size=(batch_size, num_features)).astype(np.float)\n",
    "target_index = np.ones(batch_size, dtype=np.int)\n",
    "\n",
    "loss, dW = linear_classifer.linear_softmax(X, W, target_index)\n",
    "check_gradient(lambda w: linear_classifer.linear_softmax(X, w, target_index), W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### И теперь регуляризация\n",
    "\n",
    "Мы будем использовать L2 regularization для весов как часть общей функции ошибки.\n",
    "\n",
    "Напомним, L2 regularization определяется как\n",
    "\n",
    "l2_reg_loss = regularization_strength * sum<sub>ij</sub> W[i, j]<sup>2</sup>\n",
    "\n",
    "Реализуйте функцию для его вычисления и вычисления соотвествующих градиентов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradients are not different at (0, 0). Analytic: 0.02000, Numeric: 0.02000\n",
      "Gradients are not different at (0, 1). Analytic: 0.04000, Numeric: 0.04000\n",
      "Gradients are not different at (1, 0). Analytic: -0.02000, Numeric: -0.02000\n",
      "Gradients are not different at (1, 1). Analytic: 0.02000, Numeric: 0.02000\n",
      "Gradients are not different at (2, 0). Analytic: 0.02000, Numeric: 0.02000\n",
      "Gradients are not different at (2, 1). Analytic: 0.04000, Numeric: 0.04000\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement l2_regularization function that implements loss for L2 regularization\n",
    "linear_classifer.l2_regularization(W, 0.01)\n",
    "check_gradient(lambda w: linear_classifer.l2_regularization(w, 0.01), W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Тренировка!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Градиенты в порядке, реализуем процесс тренировки!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss: 2.700090\n",
      "Epoch 1, loss: 2.655994\n",
      "Epoch 2, loss: 3.100785\n",
      "Epoch 3, loss: 3.109346\n",
      "Epoch 4, loss: 3.467313\n",
      "Epoch 5, loss: 2.954317\n",
      "Epoch 6, loss: 3.353101\n",
      "Epoch 7, loss: 2.635696\n",
      "Epoch 8, loss: 2.396908\n",
      "Epoch 9, loss: 3.450503\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement LinearSoftmaxClassifier.fit function\n",
    "classifier = linear_classifer.LinearSoftmaxClassifier()\n",
    "loss_history = classifier.fit(train_X, train_y, epochs=10, learning_rate=1e-3, batch_size=300, reg=1e1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fa6a3fa88b0>]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# let's look at the loss history!\n",
    "plt.plot(loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check how it performs on validation set\n",
    "pred = classifier.predict(val_X)\n",
    "accuracy = multiclass_accuracy(pred, val_y)\n",
    "print(\"Accuracy: \", accuracy)\n",
    "\n",
    "# Now, let's train more and see if it performs better\n",
    "loss_history = classifier.fit(train_X, train_y, epochs=100, learning_rate=1e-3, batch_size=300, reg=1e1)\n",
    "pred = classifier.predict(val_X)\n",
    "accuracy = multiclass_accuracy(pred, val_y)\n",
    "print(\"Accuracy after training for 100 epochs: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Как и раньше, используем кросс-валидацию для подбора гиперпараметтов.\n",
    "\n",
    "В этот раз, чтобы тренировка занимала разумное время, мы будем использовать только одно разделение на тренировочные (training) и проверочные (validation) данные.\n",
    "\n",
    "Теперь нам нужно подобрать не один, а два гиперпараметра! Не ограничивайте себя изначальными значениями в коде.  \n",
    "Добейтесь точности более чем **20%** на проверочных данных (validation data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss: 25759.592558\n",
      "Epoch 1, loss: 25460.946816\n",
      "Epoch 2, loss: 25151.515980\n",
      "Epoch 3, loss: 25739.526796\n",
      "Epoch 4, loss: 25623.892221\n",
      "Epoch 5, loss: 24492.844342\n",
      "Epoch 6, loss: 24865.140192\n",
      "Epoch 7, loss: 24484.315318\n",
      "Epoch 8, loss: 25725.197654\n",
      "Epoch 9, loss: 23592.384122\n",
      "Epoch 10, loss: 24061.580241\n",
      "Epoch 11, loss: 26129.167136\n",
      "Epoch 12, loss: 23797.954559\n",
      "Epoch 13, loss: 23698.613272\n",
      "Epoch 14, loss: 24952.172215\n",
      "Epoch 15, loss: 24722.113038\n",
      "Epoch 16, loss: 23558.893476\n",
      "Epoch 17, loss: 23934.593421\n",
      "Epoch 18, loss: 24645.247478\n",
      "Epoch 19, loss: 24968.534430\n",
      "Epoch 20, loss: 23108.452481\n",
      "Epoch 21, loss: 23279.041648\n",
      "Epoch 22, loss: 24038.211386\n",
      "Epoch 23, loss: 24041.039260\n",
      "Epoch 24, loss: 24339.125064\n",
      "Epoch 25, loss: 24752.907949\n",
      "Epoch 26, loss: 23670.110499\n",
      "Epoch 27, loss: 24364.829429\n",
      "Epoch 28, loss: 24665.140888\n",
      "Epoch 29, loss: 24334.846817\n",
      "Epoch 30, loss: 24086.028118\n",
      "Epoch 31, loss: 23117.325172\n",
      "Epoch 32, loss: 24218.457961\n",
      "Epoch 33, loss: 23122.599286\n",
      "Epoch 34, loss: 23754.669108\n",
      "Epoch 35, loss: 23861.991387\n",
      "Epoch 36, loss: 24259.116497\n",
      "Epoch 37, loss: 23567.098392\n",
      "Epoch 38, loss: 24497.221617\n",
      "Epoch 39, loss: 24346.689337\n",
      "Epoch 40, loss: 23272.153696\n",
      "Epoch 41, loss: 23828.587491\n",
      "Epoch 42, loss: 23955.513989\n",
      "Epoch 43, loss: 23271.407068\n",
      "Epoch 44, loss: 23797.635817\n",
      "Epoch 45, loss: 23436.620854\n",
      "Epoch 46, loss: 23810.138564\n",
      "Epoch 47, loss: 23562.291535\n",
      "Epoch 48, loss: 24503.162554\n",
      "Epoch 49, loss: 23540.983022\n",
      "Epoch 50, loss: 22671.272809\n",
      "Epoch 51, loss: 23364.521161\n",
      "Epoch 52, loss: 23110.950655\n",
      "Epoch 53, loss: 24268.286159\n",
      "Epoch 54, loss: 23001.078302\n",
      "Epoch 55, loss: 24398.584890\n",
      "Epoch 56, loss: 23377.321802\n",
      "Epoch 57, loss: 23494.349912\n",
      "Epoch 58, loss: 23233.217107\n",
      "Epoch 59, loss: 23455.651736\n",
      "Epoch 60, loss: 22143.896671\n",
      "Epoch 61, loss: 24088.656997\n",
      "Epoch 62, loss: 24013.892045\n",
      "Epoch 63, loss: 22551.754194\n",
      "Epoch 64, loss: 22879.207978\n",
      "Epoch 65, loss: 22946.186508\n",
      "Epoch 66, loss: 22713.096257\n",
      "Epoch 67, loss: 23231.714819\n",
      "Epoch 68, loss: 24241.160306\n",
      "Epoch 69, loss: 22367.600385\n",
      "Epoch 70, loss: 24028.287235\n",
      "Epoch 71, loss: 23833.005822\n",
      "Epoch 72, loss: 22079.085620\n",
      "Epoch 73, loss: 23717.548153\n",
      "Epoch 74, loss: 21858.704068\n",
      "Epoch 75, loss: 24321.829335\n",
      "Epoch 76, loss: 22980.941950\n",
      "Epoch 77, loss: 22340.122019\n",
      "Epoch 78, loss: 23007.631941\n",
      "Epoch 79, loss: 23524.879436\n",
      "Epoch 80, loss: 23123.877971\n",
      "Epoch 81, loss: 23305.433028\n",
      "Epoch 82, loss: 23639.932948\n",
      "Epoch 83, loss: 21942.649612\n",
      "Epoch 84, loss: 24396.071743\n",
      "Epoch 85, loss: 21339.196719\n",
      "Epoch 86, loss: 23395.577708\n",
      "Epoch 87, loss: 23000.229534\n",
      "Epoch 88, loss: 23125.426469\n",
      "Epoch 89, loss: 22122.214356\n",
      "Epoch 90, loss: 22641.882000\n",
      "Epoch 91, loss: 23578.632750\n",
      "Epoch 92, loss: 23956.187250\n",
      "Epoch 93, loss: 21808.650100\n",
      "Epoch 94, loss: 24122.737155\n",
      "Epoch 95, loss: 23642.717546\n",
      "Epoch 96, loss: 24329.619961\n",
      "Epoch 97, loss: 21301.937170\n",
      "Epoch 98, loss: 23540.445811\n",
      "Epoch 99, loss: 23835.162290\n",
      "Epoch 100, loss: 23465.978404\n",
      "Epoch 101, loss: 22299.831948\n",
      "Epoch 102, loss: 22315.992648\n",
      "Epoch 103, loss: 24053.107449\n",
      "Epoch 104, loss: 22997.347599\n",
      "Epoch 105, loss: 22904.128529\n",
      "Epoch 106, loss: 22344.306760\n",
      "Epoch 107, loss: 23201.452564\n",
      "Epoch 108, loss: 23432.101071\n",
      "Epoch 109, loss: 22796.070644\n",
      "Epoch 110, loss: 23124.518951\n",
      "Epoch 111, loss: 23355.836904\n",
      "Epoch 112, loss: 22841.867122\n",
      "Epoch 113, loss: 22656.867778\n",
      "Epoch 114, loss: 23211.530989\n",
      "Epoch 115, loss: 22500.144921\n",
      "Epoch 116, loss: 22937.373629\n",
      "Epoch 117, loss: 22028.555764\n",
      "Epoch 118, loss: 23420.748176\n",
      "Epoch 119, loss: 23370.250751\n",
      "Epoch 120, loss: 23123.204558\n",
      "Epoch 121, loss: 22169.230950\n",
      "Epoch 122, loss: 23707.685874\n",
      "Epoch 123, loss: 22540.950152\n",
      "Epoch 124, loss: 22405.339537\n",
      "Epoch 125, loss: 21911.564058\n",
      "Epoch 126, loss: 22544.669550\n",
      "Epoch 127, loss: 23482.337510\n",
      "Epoch 128, loss: 23437.968627\n",
      "Epoch 129, loss: 22025.889789\n",
      "Epoch 130, loss: 23550.285577\n",
      "Epoch 131, loss: 22060.422643\n",
      "Epoch 132, loss: 24278.598460\n",
      "Epoch 133, loss: 22617.913526\n",
      "Epoch 134, loss: 21585.877750\n",
      "Epoch 135, loss: 23770.661423\n",
      "Epoch 136, loss: 22768.515647\n",
      "Epoch 137, loss: 22089.616620\n",
      "Epoch 138, loss: 21337.375995\n",
      "Epoch 139, loss: 22014.581321\n",
      "Epoch 140, loss: 23665.695823\n",
      "Epoch 141, loss: 21854.870375\n",
      "Epoch 142, loss: 23458.348308\n",
      "Epoch 143, loss: 21487.165414\n",
      "Epoch 144, loss: 23070.541745\n",
      "Epoch 145, loss: 22409.507654\n",
      "Epoch 146, loss: 22765.044019\n",
      "Epoch 147, loss: 24791.578292\n",
      "Epoch 148, loss: 22284.512856\n",
      "Epoch 149, loss: 21776.430570\n",
      "Epoch 150, loss: 23092.440247\n",
      "Epoch 151, loss: 22768.223531\n",
      "Epoch 152, loss: 22129.842585\n",
      "Epoch 153, loss: 23101.639215\n",
      "Epoch 154, loss: 21851.105317\n",
      "Epoch 155, loss: 23112.957854\n",
      "Epoch 156, loss: 21384.420135\n",
      "Epoch 157, loss: 22276.510427\n",
      "Epoch 158, loss: 23329.536839\n",
      "Epoch 159, loss: 21797.554140\n",
      "Epoch 160, loss: 21813.664527\n",
      "Epoch 161, loss: 23322.708894\n",
      "Epoch 162, loss: 22595.602167\n",
      "Epoch 163, loss: 21135.937095\n",
      "Epoch 164, loss: 22246.369038\n",
      "Epoch 165, loss: 22963.461719\n",
      "Epoch 166, loss: 22147.227612\n",
      "Epoch 167, loss: 22514.433629\n",
      "Epoch 168, loss: 22415.316357\n",
      "Epoch 169, loss: 22999.917568\n",
      "Epoch 170, loss: 22054.089394\n",
      "Epoch 171, loss: 22861.424085\n",
      "Epoch 172, loss: 21679.542998\n",
      "Epoch 173, loss: 23609.431250\n",
      "Epoch 174, loss: 22401.456818\n",
      "Epoch 175, loss: 22556.366724\n",
      "Epoch 176, loss: 21299.932713\n",
      "Epoch 177, loss: 22371.639586\n",
      "Epoch 178, loss: 22220.627181\n",
      "Epoch 179, loss: 22872.469401\n",
      "Epoch 180, loss: 23202.531619\n",
      "Epoch 181, loss: 22151.654730\n",
      "Epoch 182, loss: 22079.036121\n",
      "Epoch 183, loss: 21488.388999\n",
      "Epoch 184, loss: 21996.071065\n",
      "Epoch 185, loss: 21522.667130\n",
      "Epoch 186, loss: 23290.770954\n",
      "Epoch 187, loss: 21269.771048\n",
      "Epoch 188, loss: 23703.665381\n",
      "Epoch 189, loss: 21828.659653\n",
      "Epoch 190, loss: 21308.232036\n",
      "Epoch 191, loss: 21487.884103\n",
      "Epoch 192, loss: 22479.544688\n",
      "Epoch 193, loss: 23046.892656\n",
      "Epoch 194, loss: 22571.289721\n",
      "Epoch 195, loss: 22724.360123\n",
      "Epoch 196, loss: 22439.646179\n",
      "Epoch 197, loss: 21678.711309\n",
      "Epoch 198, loss: 21435.680160\n",
      "Epoch 199, loss: 22661.130299\n",
      "0.001 0.0001 0.187\n",
      "Epoch 0, loss: 25461.515065\n",
      "Epoch 1, loss: 26584.857078\n",
      "Epoch 2, loss: 25826.549321\n",
      "Epoch 3, loss: 26218.576884\n",
      "Epoch 4, loss: 26595.993973\n",
      "Epoch 5, loss: 23740.460647\n",
      "Epoch 6, loss: 25240.034029\n",
      "Epoch 7, loss: 24229.306433\n",
      "Epoch 8, loss: 25126.515093\n",
      "Epoch 9, loss: 25721.594504\n",
      "Epoch 10, loss: 24530.571213\n",
      "Epoch 11, loss: 23922.127050\n",
      "Epoch 12, loss: 24913.597792\n",
      "Epoch 13, loss: 23888.322290\n",
      "Epoch 14, loss: 24671.915608\n",
      "Epoch 15, loss: 24309.222811\n",
      "Epoch 16, loss: 25001.785951\n",
      "Epoch 17, loss: 24241.513146\n",
      "Epoch 18, loss: 25653.162434\n",
      "Epoch 19, loss: 24148.001181\n",
      "Epoch 20, loss: 23946.475316\n",
      "Epoch 21, loss: 25126.706720\n",
      "Epoch 22, loss: 23765.146125\n",
      "Epoch 23, loss: 24198.145039\n",
      "Epoch 24, loss: 23423.971208\n",
      "Epoch 25, loss: 24279.631251\n",
      "Epoch 26, loss: 23006.084066\n",
      "Epoch 27, loss: 23492.871335\n",
      "Epoch 28, loss: 25946.961738\n",
      "Epoch 29, loss: 24128.470096\n",
      "Epoch 30, loss: 25127.840078\n",
      "Epoch 31, loss: 23703.617885\n",
      "Epoch 32, loss: 24447.348960\n",
      "Epoch 33, loss: 23473.013222\n",
      "Epoch 34, loss: 23600.101083\n",
      "Epoch 35, loss: 23336.791773\n",
      "Epoch 36, loss: 22777.742999\n",
      "Epoch 37, loss: 21719.069073\n",
      "Epoch 38, loss: 24877.467489\n",
      "Epoch 39, loss: 23781.120273\n",
      "Epoch 40, loss: 24499.745388\n",
      "Epoch 41, loss: 23994.115301\n",
      "Epoch 42, loss: 22877.241138\n",
      "Epoch 43, loss: 24283.858727\n",
      "Epoch 44, loss: 24237.860589\n",
      "Epoch 45, loss: 23547.760543\n",
      "Epoch 46, loss: 23843.522976\n",
      "Epoch 47, loss: 24469.892589\n",
      "Epoch 48, loss: 22357.518648\n",
      "Epoch 49, loss: 23526.448330\n",
      "Epoch 50, loss: 23289.235787\n",
      "Epoch 51, loss: 22819.601053\n",
      "Epoch 52, loss: 24226.141508\n",
      "Epoch 53, loss: 24497.421410\n",
      "Epoch 54, loss: 22477.979853\n",
      "Epoch 55, loss: 24005.851236\n",
      "Epoch 56, loss: 23877.483230\n",
      "Epoch 57, loss: 24488.672797\n",
      "Epoch 58, loss: 23627.656995\n",
      "Epoch 59, loss: 23175.784901\n",
      "Epoch 60, loss: 22672.670254\n",
      "Epoch 61, loss: 23166.092032\n",
      "Epoch 62, loss: 24104.808934\n",
      "Epoch 63, loss: 23183.398619\n",
      "Epoch 64, loss: 24488.389594\n",
      "Epoch 65, loss: 22156.912677\n",
      "Epoch 66, loss: 22321.758458\n",
      "Epoch 67, loss: 24695.714181\n",
      "Epoch 68, loss: 22149.353511\n",
      "Epoch 69, loss: 23969.245919\n",
      "Epoch 70, loss: 23228.615730\n",
      "Epoch 71, loss: 23618.181190\n",
      "Epoch 72, loss: 21660.888493\n",
      "Epoch 73, loss: 22707.503578\n",
      "Epoch 74, loss: 24288.464568\n",
      "Epoch 75, loss: 23279.296711\n",
      "Epoch 76, loss: 23648.869608\n",
      "Epoch 77, loss: 22888.956172\n",
      "Epoch 78, loss: 22839.554022\n",
      "Epoch 79, loss: 23335.634799\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80, loss: 22553.929906\n",
      "Epoch 81, loss: 23053.876223\n",
      "Epoch 82, loss: 23118.220127\n",
      "Epoch 83, loss: 22655.573412\n",
      "Epoch 84, loss: 24240.324176\n",
      "Epoch 85, loss: 23646.835150\n",
      "Epoch 86, loss: 21541.942275\n",
      "Epoch 87, loss: 24746.440206\n",
      "Epoch 88, loss: 22981.770711\n",
      "Epoch 89, loss: 21651.638458\n",
      "Epoch 90, loss: 23445.242805\n",
      "Epoch 91, loss: 21981.157493\n",
      "Epoch 92, loss: 24679.581035\n",
      "Epoch 93, loss: 22829.122291\n",
      "Epoch 94, loss: 22986.733934\n",
      "Epoch 95, loss: 24407.743759\n",
      "Epoch 96, loss: 22141.831393\n",
      "Epoch 97, loss: 22153.829933\n",
      "Epoch 98, loss: 24163.670127\n",
      "Epoch 99, loss: 22899.109047\n",
      "Epoch 100, loss: 23814.504837\n",
      "Epoch 101, loss: 22931.513681\n",
      "Epoch 102, loss: 21371.880859\n",
      "Epoch 103, loss: 22660.708277\n",
      "Epoch 104, loss: 22160.489656\n",
      "Epoch 105, loss: 22466.172849\n",
      "Epoch 106, loss: 22548.639147\n",
      "Epoch 107, loss: 22656.009650\n",
      "Epoch 108, loss: 22870.969768\n",
      "Epoch 109, loss: 22685.621847\n",
      "Epoch 110, loss: 22887.902765\n",
      "Epoch 111, loss: 23952.995662\n",
      "Epoch 112, loss: 21817.728167\n",
      "Epoch 113, loss: 23341.750652\n",
      "Epoch 114, loss: 20968.023149\n",
      "Epoch 115, loss: 23501.442216\n",
      "Epoch 116, loss: 23283.216729\n",
      "Epoch 117, loss: 23013.781297\n",
      "Epoch 118, loss: 22518.269996\n",
      "Epoch 119, loss: 22975.876856\n",
      "Epoch 120, loss: 22615.807517\n",
      "Epoch 121, loss: 23320.017203\n",
      "Epoch 122, loss: 22312.463536\n",
      "Epoch 123, loss: 22570.765239\n",
      "Epoch 124, loss: 21929.797995\n",
      "Epoch 125, loss: 22323.057487\n",
      "Epoch 126, loss: 21758.764337\n",
      "Epoch 127, loss: 22657.668840\n",
      "Epoch 128, loss: 23142.054814\n",
      "Epoch 129, loss: 22121.720060\n",
      "Epoch 130, loss: 21214.096378\n",
      "Epoch 131, loss: 23228.802623\n",
      "Epoch 132, loss: 23085.848671\n",
      "Epoch 133, loss: 22680.331451\n",
      "Epoch 134, loss: 22854.169567\n",
      "Epoch 135, loss: 21690.114644\n",
      "Epoch 136, loss: 23660.812821\n",
      "Epoch 137, loss: 21536.028773\n",
      "Epoch 138, loss: 23865.244259\n",
      "Epoch 139, loss: 24205.008902\n",
      "Epoch 140, loss: 21898.406805\n",
      "Epoch 141, loss: 22781.603775\n",
      "Epoch 142, loss: 21241.772731\n",
      "Epoch 143, loss: 23527.627185\n",
      "Epoch 144, loss: 22701.329120\n",
      "Epoch 145, loss: 22154.966519\n",
      "Epoch 146, loss: 22990.552005\n",
      "Epoch 147, loss: 20538.487182\n",
      "Epoch 148, loss: 23100.584235\n",
      "Epoch 149, loss: 21894.470775\n",
      "Epoch 150, loss: 21738.783417\n",
      "Epoch 151, loss: 22976.749975\n",
      "Epoch 152, loss: 23575.538101\n",
      "Epoch 153, loss: 22705.303313\n",
      "Epoch 154, loss: 21333.281035\n",
      "Epoch 155, loss: 23297.019397\n",
      "Epoch 156, loss: 22947.755111\n",
      "Epoch 157, loss: 22267.225919\n",
      "Epoch 158, loss: 22421.216490\n",
      "Epoch 159, loss: 23339.496404\n",
      "Epoch 160, loss: 21525.516420\n",
      "Epoch 161, loss: 22378.909737\n",
      "Epoch 162, loss: 23007.449873\n",
      "Epoch 163, loss: 22071.720441\n",
      "Epoch 164, loss: 22564.914865\n",
      "Epoch 165, loss: 23510.835692\n",
      "Epoch 166, loss: 22048.265537\n",
      "Epoch 167, loss: 23440.550807\n",
      "Epoch 168, loss: 21473.698120\n",
      "Epoch 169, loss: 21904.128626\n",
      "Epoch 170, loss: 23252.363345\n",
      "Epoch 171, loss: 21215.090794\n",
      "Epoch 172, loss: 21576.231678\n",
      "Epoch 173, loss: 23576.317208\n",
      "Epoch 174, loss: 22940.361294\n",
      "Epoch 175, loss: 22173.505695\n",
      "Epoch 176, loss: 23608.237513\n",
      "Epoch 177, loss: 22136.154941\n",
      "Epoch 178, loss: 21849.995285\n",
      "Epoch 179, loss: 21786.853604\n",
      "Epoch 180, loss: 21500.600894\n",
      "Epoch 181, loss: 22671.720806\n",
      "Epoch 182, loss: 20833.004732\n",
      "Epoch 183, loss: 21091.679040\n",
      "Epoch 184, loss: 21953.126296\n",
      "Epoch 185, loss: 23500.642180\n",
      "Epoch 186, loss: 21042.602933\n",
      "Epoch 187, loss: 22833.959839\n",
      "Epoch 188, loss: 22464.550369\n",
      "Epoch 189, loss: 22002.609233\n",
      "Epoch 190, loss: 19832.153069\n",
      "Epoch 191, loss: 23050.266775\n",
      "Epoch 192, loss: 22585.207248\n",
      "Epoch 193, loss: 22943.659342\n",
      "Epoch 194, loss: 22299.163754\n",
      "Epoch 195, loss: 22693.801499\n",
      "Epoch 196, loss: 22085.783529\n",
      "Epoch 197, loss: 22601.424404\n",
      "Epoch 198, loss: 22344.172621\n",
      "Epoch 199, loss: 21758.125002\n",
      "0.001 1e-05 0.196\n",
      "Epoch 0, loss: 25980.196442\n",
      "Epoch 1, loss: 25349.962750\n",
      "Epoch 2, loss: 24862.114238\n",
      "Epoch 3, loss: 25986.330987\n",
      "Epoch 4, loss: 24776.285261\n",
      "Epoch 5, loss: 26357.458844\n",
      "Epoch 6, loss: 25935.489241\n",
      "Epoch 7, loss: 24975.519630\n",
      "Epoch 8, loss: 24653.281656\n",
      "Epoch 9, loss: 24770.224746\n",
      "Epoch 10, loss: 25559.717162\n",
      "Epoch 11, loss: 23562.908949\n",
      "Epoch 12, loss: 24865.837187\n",
      "Epoch 13, loss: 24572.421174\n",
      "Epoch 14, loss: 23655.060675\n",
      "Epoch 15, loss: 24276.756977\n",
      "Epoch 16, loss: 22492.720070\n",
      "Epoch 17, loss: 26080.478474\n",
      "Epoch 18, loss: 23652.761179\n",
      "Epoch 19, loss: 22313.253986\n",
      "Epoch 20, loss: 25725.107396\n",
      "Epoch 21, loss: 24246.929446\n",
      "Epoch 22, loss: 23739.808192\n",
      "Epoch 23, loss: 26130.308653\n",
      "Epoch 24, loss: 24064.273214\n",
      "Epoch 25, loss: 24160.944649\n",
      "Epoch 26, loss: 26003.591575\n",
      "Epoch 27, loss: 23844.326350\n",
      "Epoch 28, loss: 24408.718510\n",
      "Epoch 29, loss: 22857.205593\n",
      "Epoch 30, loss: 24359.741817\n",
      "Epoch 31, loss: 23214.013605\n",
      "Epoch 32, loss: 23453.335449\n",
      "Epoch 33, loss: 24866.388983\n",
      "Epoch 34, loss: 23345.765076\n",
      "Epoch 35, loss: 23629.371121\n",
      "Epoch 36, loss: 24474.082557\n",
      "Epoch 37, loss: 24168.006972\n",
      "Epoch 38, loss: 23321.295267\n",
      "Epoch 39, loss: 24566.204064\n",
      "Epoch 40, loss: 23488.408618\n",
      "Epoch 41, loss: 23129.834558\n",
      "Epoch 42, loss: 23685.159860\n",
      "Epoch 43, loss: 25232.505400\n",
      "Epoch 44, loss: 23270.905529\n",
      "Epoch 45, loss: 24941.202364\n",
      "Epoch 46, loss: 22536.517952\n",
      "Epoch 47, loss: 24621.809859\n",
      "Epoch 48, loss: 24493.664365\n",
      "Epoch 49, loss: 21664.655506\n",
      "Epoch 50, loss: 23902.591053\n",
      "Epoch 51, loss: 23192.183956\n",
      "Epoch 52, loss: 24147.631542\n",
      "Epoch 53, loss: 24093.295238\n",
      "Epoch 54, loss: 22565.250198\n",
      "Epoch 55, loss: 23841.286283\n",
      "Epoch 56, loss: 24299.696357\n",
      "Epoch 57, loss: 24092.708545\n",
      "Epoch 58, loss: 24635.954895\n",
      "Epoch 59, loss: 24638.381006\n",
      "Epoch 60, loss: 22212.127814\n",
      "Epoch 61, loss: 23370.949867\n",
      "Epoch 62, loss: 22855.527264\n",
      "Epoch 63, loss: 23466.001042\n",
      "Epoch 64, loss: 23869.164651\n",
      "Epoch 65, loss: 23605.325322\n",
      "Epoch 66, loss: 23176.133414\n",
      "Epoch 67, loss: 23133.080316\n",
      "Epoch 68, loss: 23142.131848\n",
      "Epoch 69, loss: 24466.959227\n",
      "Epoch 70, loss: 22984.177498\n",
      "Epoch 71, loss: 21759.959084\n",
      "Epoch 72, loss: 23406.131995\n",
      "Epoch 73, loss: 22634.818781\n",
      "Epoch 74, loss: 23566.340070\n",
      "Epoch 75, loss: 23075.355188\n",
      "Epoch 76, loss: 24020.861952\n",
      "Epoch 77, loss: 23846.327330\n",
      "Epoch 78, loss: 22169.537928\n",
      "Epoch 79, loss: 22881.721349\n",
      "Epoch 80, loss: 23432.059234\n",
      "Epoch 81, loss: 22753.644891\n",
      "Epoch 82, loss: 23909.272915\n",
      "Epoch 83, loss: 23612.407355\n",
      "Epoch 84, loss: 22632.245819\n",
      "Epoch 85, loss: 21769.400538\n",
      "Epoch 86, loss: 22243.849644\n",
      "Epoch 87, loss: 22650.923144\n",
      "Epoch 88, loss: 24150.345409\n",
      "Epoch 89, loss: 23195.833301\n",
      "Epoch 90, loss: 23223.223532\n",
      "Epoch 91, loss: 22825.737345\n",
      "Epoch 92, loss: 22245.334692\n",
      "Epoch 93, loss: 21442.024303\n",
      "Epoch 94, loss: 24077.273862\n",
      "Epoch 95, loss: 22485.294724\n",
      "Epoch 96, loss: 23341.639891\n",
      "Epoch 97, loss: 22843.661270\n",
      "Epoch 98, loss: 23525.932242\n",
      "Epoch 99, loss: 22136.102159\n",
      "Epoch 100, loss: 22548.009596\n",
      "Epoch 101, loss: 23483.055623\n",
      "Epoch 102, loss: 23274.542632\n",
      "Epoch 103, loss: 23239.982819\n",
      "Epoch 104, loss: 22951.491250\n",
      "Epoch 105, loss: 23350.304096\n",
      "Epoch 106, loss: 22519.172804\n",
      "Epoch 107, loss: 22896.702248\n",
      "Epoch 108, loss: 22210.913167\n",
      "Epoch 109, loss: 22928.836510\n",
      "Epoch 110, loss: 21931.956039\n",
      "Epoch 111, loss: 22644.529658\n",
      "Epoch 112, loss: 21652.896750\n",
      "Epoch 113, loss: 22514.331128\n",
      "Epoch 114, loss: 23640.056642\n",
      "Epoch 115, loss: 23117.858314\n",
      "Epoch 116, loss: 22899.475931\n",
      "Epoch 117, loss: 23641.460216\n",
      "Epoch 118, loss: 22139.391562\n",
      "Epoch 119, loss: 23578.896786\n",
      "Epoch 120, loss: 22292.176157\n",
      "Epoch 121, loss: 22022.525116\n",
      "Epoch 122, loss: 21803.281288\n",
      "Epoch 123, loss: 22456.192000\n",
      "Epoch 124, loss: 24040.320146\n",
      "Epoch 125, loss: 22392.114921\n",
      "Epoch 126, loss: 21931.519376\n",
      "Epoch 127, loss: 21757.680903\n",
      "Epoch 128, loss: 21408.412198\n",
      "Epoch 129, loss: 24434.442227\n",
      "Epoch 130, loss: 22516.030075\n",
      "Epoch 131, loss: 23050.361490\n",
      "Epoch 132, loss: 22340.703040\n",
      "Epoch 133, loss: 22580.316257\n",
      "Epoch 134, loss: 22353.438791\n",
      "Epoch 135, loss: 22529.453835\n",
      "Epoch 136, loss: 23646.977625\n",
      "Epoch 137, loss: 21457.233080\n",
      "Epoch 138, loss: 22713.223197\n",
      "Epoch 139, loss: 22963.612244\n",
      "Epoch 140, loss: 22277.065604\n",
      "Epoch 141, loss: 23141.006485\n",
      "Epoch 142, loss: 22073.600762\n",
      "Epoch 143, loss: 23230.982326\n",
      "Epoch 144, loss: 21763.973931\n",
      "Epoch 145, loss: 22365.035912\n",
      "Epoch 146, loss: 22480.725297\n",
      "Epoch 147, loss: 21730.014566\n",
      "Epoch 148, loss: 22312.735212\n",
      "Epoch 149, loss: 22129.011456\n",
      "Epoch 150, loss: 22970.109622\n",
      "Epoch 151, loss: 23313.470789\n",
      "Epoch 152, loss: 22233.130084\n",
      "Epoch 153, loss: 21849.682086\n",
      "Epoch 154, loss: 22810.777463\n",
      "Epoch 155, loss: 22816.544561\n",
      "Epoch 156, loss: 23425.374939\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 157, loss: 21935.608074\n",
      "Epoch 158, loss: 23741.002812\n",
      "Epoch 159, loss: 22052.421501\n",
      "Epoch 160, loss: 22557.731787\n",
      "Epoch 161, loss: 22025.251628\n",
      "Epoch 162, loss: 22923.501722\n",
      "Epoch 163, loss: 21843.206848\n",
      "Epoch 164, loss: 21366.387291\n",
      "Epoch 165, loss: 22452.824600\n",
      "Epoch 166, loss: 22103.328974\n",
      "Epoch 167, loss: 20777.278914\n",
      "Epoch 168, loss: 23043.394622\n",
      "Epoch 169, loss: 22284.423798\n",
      "Epoch 170, loss: 22242.394801\n",
      "Epoch 171, loss: 23179.659105\n",
      "Epoch 172, loss: 23084.114440\n",
      "Epoch 173, loss: 21792.140920\n",
      "Epoch 174, loss: 22687.013654\n",
      "Epoch 175, loss: 22758.207578\n",
      "Epoch 176, loss: 23052.082280\n",
      "Epoch 177, loss: 21685.351101\n",
      "Epoch 178, loss: 23082.597472\n",
      "Epoch 179, loss: 22422.818575\n",
      "Epoch 180, loss: 24318.673341\n",
      "Epoch 181, loss: 21608.887221\n",
      "Epoch 182, loss: 22211.874202\n",
      "Epoch 183, loss: 22292.991850\n",
      "Epoch 184, loss: 21814.303465\n",
      "Epoch 185, loss: 22273.698682\n",
      "Epoch 186, loss: 23270.890820\n",
      "Epoch 187, loss: 21437.470598\n",
      "Epoch 188, loss: 23528.904386\n",
      "Epoch 189, loss: 23074.608650\n",
      "Epoch 190, loss: 21581.738736\n",
      "Epoch 191, loss: 22396.722018\n",
      "Epoch 192, loss: 20668.218917\n",
      "Epoch 193, loss: 21413.559743\n",
      "Epoch 194, loss: 22978.797876\n",
      "Epoch 195, loss: 21437.512802\n",
      "Epoch 196, loss: 22034.546244\n",
      "Epoch 197, loss: 22901.595957\n",
      "Epoch 198, loss: 22048.917057\n",
      "Epoch 199, loss: 23610.910912\n",
      "0.001 1e-06 0.197\n",
      "Epoch 0, loss: 20645.004856\n",
      "Epoch 1, loss: 20450.317094\n",
      "Epoch 2, loss: 20295.852344\n",
      "Epoch 3, loss: 20155.793417\n",
      "Epoch 4, loss: 20048.208574\n",
      "Epoch 5, loss: 19950.996634\n",
      "Epoch 6, loss: 19861.099088\n",
      "Epoch 7, loss: 19794.952912\n",
      "Epoch 8, loss: 19740.489225\n",
      "Epoch 9, loss: 19679.471883\n",
      "Epoch 10, loss: 19638.835217\n",
      "Epoch 11, loss: 19594.457752\n",
      "Epoch 12, loss: 19558.079324\n",
      "Epoch 13, loss: 19526.520838\n",
      "Epoch 14, loss: 19497.435868\n",
      "Epoch 15, loss: 19468.440180\n",
      "Epoch 16, loss: 19443.446215\n",
      "Epoch 17, loss: 19421.259732\n",
      "Epoch 18, loss: 19393.851926\n",
      "Epoch 19, loss: 19369.977890\n",
      "Epoch 20, loss: 19356.615458\n",
      "Epoch 21, loss: 19334.853026\n",
      "Epoch 22, loss: 19318.543697\n",
      "Epoch 23, loss: 19301.870414\n",
      "Epoch 24, loss: 19289.883759\n",
      "Epoch 25, loss: 19271.646443\n",
      "Epoch 26, loss: 19250.770903\n",
      "Epoch 27, loss: 19244.195725\n",
      "Epoch 28, loss: 19231.975842\n",
      "Epoch 29, loss: 19218.941935\n",
      "Epoch 30, loss: 19198.948621\n",
      "Epoch 31, loss: 19192.770445\n",
      "Epoch 32, loss: 19178.843515\n",
      "Epoch 33, loss: 19170.701557\n",
      "Epoch 34, loss: 19159.909155\n",
      "Epoch 35, loss: 19141.869529\n",
      "Epoch 36, loss: 19138.239068\n",
      "Epoch 37, loss: 19137.040521\n",
      "Epoch 38, loss: 19121.955119\n",
      "Epoch 39, loss: 19113.488510\n",
      "Epoch 40, loss: 19098.802788\n",
      "Epoch 41, loss: 19094.863130\n",
      "Epoch 42, loss: 19088.223942\n",
      "Epoch 43, loss: 19077.050168\n",
      "Epoch 44, loss: 19060.030360\n",
      "Epoch 45, loss: 19060.637393\n",
      "Epoch 46, loss: 19052.958062\n",
      "Epoch 47, loss: 19042.003542\n",
      "Epoch 48, loss: 19039.569024\n",
      "Epoch 49, loss: 19031.918527\n",
      "Epoch 50, loss: 19022.136880\n",
      "Epoch 51, loss: 19011.525061\n",
      "Epoch 52, loss: 19012.877182\n",
      "Epoch 53, loss: 19007.481618\n",
      "Epoch 54, loss: 18994.099396\n",
      "Epoch 55, loss: 18991.404502\n",
      "Epoch 56, loss: 18986.550207\n",
      "Epoch 57, loss: 18981.619975\n",
      "Epoch 58, loss: 18971.310543\n",
      "Epoch 59, loss: 18966.792756\n",
      "Epoch 60, loss: 18963.158226\n",
      "Epoch 61, loss: 18951.121580\n",
      "Epoch 62, loss: 18949.406105\n",
      "Epoch 63, loss: 18934.703540\n",
      "Epoch 64, loss: 18933.517572\n",
      "Epoch 65, loss: 18928.040687\n",
      "Epoch 66, loss: 18924.572120\n",
      "Epoch 67, loss: 18919.122881\n",
      "Epoch 68, loss: 18910.974690\n",
      "Epoch 69, loss: 18912.610761\n",
      "Epoch 70, loss: 18905.433501\n",
      "Epoch 71, loss: 18895.008354\n",
      "Epoch 72, loss: 18894.257975\n",
      "Epoch 73, loss: 18884.722705\n",
      "Epoch 74, loss: 18882.506871\n",
      "Epoch 75, loss: 18880.015161\n",
      "Epoch 76, loss: 18876.137784\n",
      "Epoch 77, loss: 18870.919727\n",
      "Epoch 78, loss: 18869.658321\n",
      "Epoch 79, loss: 18860.224122\n",
      "Epoch 80, loss: 18849.597443\n",
      "Epoch 81, loss: 18849.718811\n",
      "Epoch 82, loss: 18846.921496\n",
      "Epoch 83, loss: 18836.747043\n",
      "Epoch 84, loss: 18841.702183\n",
      "Epoch 85, loss: 18834.206809\n",
      "Epoch 86, loss: 18830.436870\n",
      "Epoch 87, loss: 18824.387204\n",
      "Epoch 88, loss: 18824.192837\n",
      "Epoch 89, loss: 18819.706481\n",
      "Epoch 90, loss: 18813.910260\n",
      "Epoch 91, loss: 18810.881891\n",
      "Epoch 92, loss: 18808.134421\n",
      "Epoch 93, loss: 18802.951137\n",
      "Epoch 94, loss: 18792.143728\n",
      "Epoch 95, loss: 18799.930871\n",
      "Epoch 96, loss: 18789.936370\n",
      "Epoch 97, loss: 18785.723500\n",
      "Epoch 98, loss: 18784.880140\n",
      "Epoch 99, loss: 18778.589214\n",
      "Epoch 100, loss: 18779.179356\n",
      "Epoch 101, loss: 18774.191526\n",
      "Epoch 102, loss: 18764.435091\n",
      "Epoch 103, loss: 18771.086158\n",
      "Epoch 104, loss: 18761.063789\n",
      "Epoch 105, loss: 18754.804688\n",
      "Epoch 106, loss: 18756.300043\n",
      "Epoch 107, loss: 18753.082354\n",
      "Epoch 108, loss: 18745.319394\n",
      "Epoch 109, loss: 18745.652825\n",
      "Epoch 110, loss: 18741.058597\n",
      "Epoch 111, loss: 18741.501742\n",
      "Epoch 112, loss: 18734.174843\n",
      "Epoch 113, loss: 18734.247879\n",
      "Epoch 114, loss: 18733.073501\n",
      "Epoch 115, loss: 18726.873046\n",
      "Epoch 116, loss: 18724.300099\n",
      "Epoch 117, loss: 18724.248478\n",
      "Epoch 118, loss: 18712.793150\n",
      "Epoch 119, loss: 18706.345343\n",
      "Epoch 120, loss: 18709.101805\n",
      "Epoch 121, loss: 18709.344050\n",
      "Epoch 122, loss: 18703.500065\n",
      "Epoch 123, loss: 18701.143961\n",
      "Epoch 124, loss: 18695.334493\n",
      "Epoch 125, loss: 18700.232866\n",
      "Epoch 126, loss: 18689.543553\n",
      "Epoch 127, loss: 18687.898734\n",
      "Epoch 128, loss: 18686.456053\n",
      "Epoch 129, loss: 18683.574314\n",
      "Epoch 130, loss: 18680.249600\n",
      "Epoch 131, loss: 18677.569247\n",
      "Epoch 132, loss: 18677.348047\n",
      "Epoch 133, loss: 18676.893717\n",
      "Epoch 134, loss: 18675.003413\n",
      "Epoch 135, loss: 18665.985404\n",
      "Epoch 136, loss: 18662.808389\n",
      "Epoch 137, loss: 18660.659824\n",
      "Epoch 138, loss: 18658.665401\n",
      "Epoch 139, loss: 18656.800062\n",
      "Epoch 140, loss: 18650.687957\n",
      "Epoch 141, loss: 18650.040120\n",
      "Epoch 142, loss: 18647.086232\n",
      "Epoch 143, loss: 18643.283831\n",
      "Epoch 144, loss: 18645.543735\n",
      "Epoch 145, loss: 18642.114413\n",
      "Epoch 146, loss: 18637.248162\n",
      "Epoch 147, loss: 18633.643516\n",
      "Epoch 148, loss: 18632.437505\n",
      "Epoch 149, loss: 18631.849348\n",
      "Epoch 150, loss: 18630.458756\n",
      "Epoch 151, loss: 18624.360957\n",
      "Epoch 152, loss: 18628.075667\n",
      "Epoch 153, loss: 18622.450967\n",
      "Epoch 154, loss: 18618.782205\n",
      "Epoch 155, loss: 18608.730862\n",
      "Epoch 156, loss: 18619.690752\n",
      "Epoch 157, loss: 18610.542806\n",
      "Epoch 158, loss: 18611.910274\n",
      "Epoch 159, loss: 18612.386125\n",
      "Epoch 160, loss: 18604.911693\n",
      "Epoch 161, loss: 18597.896603\n",
      "Epoch 162, loss: 18598.367837\n",
      "Epoch 163, loss: 18590.995726\n",
      "Epoch 164, loss: 18594.815954\n",
      "Epoch 165, loss: 18587.862318\n",
      "Epoch 166, loss: 18596.920242\n",
      "Epoch 167, loss: 18587.882019\n",
      "Epoch 168, loss: 18590.331163\n",
      "Epoch 169, loss: 18581.107040\n",
      "Epoch 170, loss: 18579.576585\n",
      "Epoch 171, loss: 18579.927150\n",
      "Epoch 172, loss: 18578.672491\n",
      "Epoch 173, loss: 18577.969478\n",
      "Epoch 174, loss: 18572.936555\n",
      "Epoch 175, loss: 18566.369696\n",
      "Epoch 176, loss: 18568.298668\n",
      "Epoch 177, loss: 18565.955884\n",
      "Epoch 178, loss: 18563.284628\n",
      "Epoch 179, loss: 18563.253586\n",
      "Epoch 180, loss: 18562.093866\n",
      "Epoch 181, loss: 18556.496199\n",
      "Epoch 182, loss: 18549.647798\n",
      "Epoch 183, loss: 18557.996691\n",
      "Epoch 184, loss: 18548.103121\n",
      "Epoch 185, loss: 18542.434900\n",
      "Epoch 186, loss: 18548.225877\n",
      "Epoch 187, loss: 18544.369600\n",
      "Epoch 188, loss: 18538.211982\n",
      "Epoch 189, loss: 18538.451417\n",
      "Epoch 190, loss: 18538.655163\n",
      "Epoch 191, loss: 18531.684100\n",
      "Epoch 192, loss: 18533.230486\n",
      "Epoch 193, loss: 18527.070664\n",
      "Epoch 194, loss: 18529.760051\n",
      "Epoch 195, loss: 18524.513870\n",
      "Epoch 196, loss: 18526.536026\n",
      "Epoch 197, loss: 18518.146085\n",
      "Epoch 198, loss: 18518.276596\n",
      "Epoch 199, loss: 18518.336962\n",
      "0.0001 0.0001 0.247\n",
      "Epoch 0, loss: 20650.416608\n",
      "Epoch 1, loss: 20448.145943\n",
      "Epoch 2, loss: 20287.750035\n",
      "Epoch 3, loss: 20151.277240\n",
      "Epoch 4, loss: 20045.144571\n",
      "Epoch 5, loss: 19953.352107\n",
      "Epoch 6, loss: 19864.835106\n",
      "Epoch 7, loss: 19796.798439\n",
      "Epoch 8, loss: 19736.513695\n",
      "Epoch 9, loss: 19681.322967\n",
      "Epoch 10, loss: 19636.364201\n",
      "Epoch 11, loss: 19591.771327\n",
      "Epoch 12, loss: 19556.301053\n",
      "Epoch 13, loss: 19523.377181\n",
      "Epoch 14, loss: 19490.129548\n",
      "Epoch 15, loss: 19469.686238\n",
      "Epoch 16, loss: 19440.618697\n",
      "Epoch 17, loss: 19414.885759\n",
      "Epoch 18, loss: 19398.508006\n",
      "Epoch 19, loss: 19371.013108\n",
      "Epoch 20, loss: 19351.989838\n",
      "Epoch 21, loss: 19338.058270\n",
      "Epoch 22, loss: 19314.783373\n",
      "Epoch 23, loss: 19300.193995\n",
      "Epoch 24, loss: 19286.862990\n",
      "Epoch 25, loss: 19270.526907\n",
      "Epoch 26, loss: 19255.918611\n",
      "Epoch 27, loss: 19240.856368\n",
      "Epoch 28, loss: 19225.050725\n",
      "Epoch 29, loss: 19219.272408\n",
      "Epoch 30, loss: 19200.177515\n",
      "Epoch 31, loss: 19193.318050\n",
      "Epoch 32, loss: 19178.765094\n",
      "Epoch 33, loss: 19170.898952\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34, loss: 19157.832216\n",
      "Epoch 35, loss: 19145.973308\n",
      "Epoch 36, loss: 19137.342690\n",
      "Epoch 37, loss: 19131.520695\n",
      "Epoch 38, loss: 19118.560758\n",
      "Epoch 39, loss: 19104.337862\n",
      "Epoch 40, loss: 19101.840275\n",
      "Epoch 41, loss: 19089.283360\n",
      "Epoch 42, loss: 19081.642290\n",
      "Epoch 43, loss: 19079.270659\n",
      "Epoch 44, loss: 19060.913334\n",
      "Epoch 45, loss: 19047.501851\n",
      "Epoch 46, loss: 19053.614548\n",
      "Epoch 47, loss: 19043.142942\n",
      "Epoch 48, loss: 19036.536610\n",
      "Epoch 49, loss: 19025.882709\n",
      "Epoch 50, loss: 19021.693876\n",
      "Epoch 51, loss: 19014.625430\n",
      "Epoch 52, loss: 19001.563310\n",
      "Epoch 53, loss: 19000.525963\n",
      "Epoch 54, loss: 18989.520030\n",
      "Epoch 55, loss: 18990.485188\n",
      "Epoch 56, loss: 18978.214053\n",
      "Epoch 57, loss: 18978.603949\n",
      "Epoch 58, loss: 18966.232996\n",
      "Epoch 59, loss: 18967.095228\n",
      "Epoch 60, loss: 18956.064283\n",
      "Epoch 61, loss: 18953.660502\n",
      "Epoch 62, loss: 18950.385949\n",
      "Epoch 63, loss: 18940.497491\n",
      "Epoch 64, loss: 18935.127625\n",
      "Epoch 65, loss: 18925.146242\n",
      "Epoch 66, loss: 18923.077658\n",
      "Epoch 67, loss: 18919.045811\n",
      "Epoch 68, loss: 18913.799487\n",
      "Epoch 69, loss: 18907.537192\n",
      "Epoch 70, loss: 18902.702531\n",
      "Epoch 71, loss: 18894.833300\n",
      "Epoch 72, loss: 18894.047167\n",
      "Epoch 73, loss: 18888.830598\n",
      "Epoch 74, loss: 18885.611533\n",
      "Epoch 75, loss: 18877.557469\n",
      "Epoch 76, loss: 18875.368754\n",
      "Epoch 77, loss: 18867.537998\n",
      "Epoch 78, loss: 18858.050539\n",
      "Epoch 79, loss: 18859.209344\n",
      "Epoch 80, loss: 18855.678912\n",
      "Epoch 81, loss: 18850.059779\n",
      "Epoch 82, loss: 18846.917783\n",
      "Epoch 83, loss: 18844.997875\n",
      "Epoch 84, loss: 18836.588162\n",
      "Epoch 85, loss: 18831.812527\n",
      "Epoch 86, loss: 18829.728592\n",
      "Epoch 87, loss: 18829.747027\n",
      "Epoch 88, loss: 18823.008665\n",
      "Epoch 89, loss: 18820.145970\n",
      "Epoch 90, loss: 18815.267657\n",
      "Epoch 91, loss: 18814.702559\n",
      "Epoch 92, loss: 18806.438726\n",
      "Epoch 93, loss: 18806.157781\n",
      "Epoch 94, loss: 18793.287689\n",
      "Epoch 95, loss: 18793.525659\n",
      "Epoch 96, loss: 18794.002026\n",
      "Epoch 97, loss: 18787.935227\n",
      "Epoch 98, loss: 18782.511942\n",
      "Epoch 99, loss: 18778.965191\n",
      "Epoch 100, loss: 18778.493742\n",
      "Epoch 101, loss: 18774.719545\n",
      "Epoch 102, loss: 18769.419140\n",
      "Epoch 103, loss: 18764.013444\n",
      "Epoch 104, loss: 18764.154322\n",
      "Epoch 105, loss: 18756.279688\n",
      "Epoch 106, loss: 18755.416544\n",
      "Epoch 107, loss: 18749.095238\n",
      "Epoch 108, loss: 18747.695023\n",
      "Epoch 109, loss: 18750.934746\n",
      "Epoch 110, loss: 18740.066644\n",
      "Epoch 111, loss: 18742.461740\n",
      "Epoch 112, loss: 18731.512601\n",
      "Epoch 113, loss: 18730.107132\n",
      "Epoch 114, loss: 18733.728179\n",
      "Epoch 115, loss: 18727.797618\n",
      "Epoch 116, loss: 18719.957710\n",
      "Epoch 117, loss: 18709.704336\n",
      "Epoch 118, loss: 18716.240406\n",
      "Epoch 119, loss: 18716.241010\n",
      "Epoch 120, loss: 18707.708890\n",
      "Epoch 121, loss: 18712.279676\n",
      "Epoch 122, loss: 18701.405593\n",
      "Epoch 123, loss: 18696.963572\n",
      "Epoch 124, loss: 18699.062898\n",
      "Epoch 125, loss: 18699.324587\n",
      "Epoch 126, loss: 18691.323756\n",
      "Epoch 127, loss: 18690.516864\n",
      "Epoch 128, loss: 18687.741760\n",
      "Epoch 129, loss: 18686.192280\n",
      "Epoch 130, loss: 18682.666519\n",
      "Epoch 131, loss: 18675.500848\n",
      "Epoch 132, loss: 18677.057464\n",
      "Epoch 133, loss: 18674.832905\n",
      "Epoch 134, loss: 18672.105478\n",
      "Epoch 135, loss: 18671.550294\n",
      "Epoch 136, loss: 18668.610121\n",
      "Epoch 137, loss: 18667.422090\n",
      "Epoch 138, loss: 18654.881251\n",
      "Epoch 139, loss: 18658.265997\n",
      "Epoch 140, loss: 18655.558219\n",
      "Epoch 141, loss: 18647.732406\n",
      "Epoch 142, loss: 18648.086590\n",
      "Epoch 143, loss: 18648.718764\n",
      "Epoch 144, loss: 18642.686829\n",
      "Epoch 145, loss: 18638.509980\n",
      "Epoch 146, loss: 18633.826382\n",
      "Epoch 147, loss: 18634.107692\n",
      "Epoch 148, loss: 18629.738248\n",
      "Epoch 149, loss: 18629.681882\n",
      "Epoch 150, loss: 18625.754106\n",
      "Epoch 151, loss: 18627.661937\n",
      "Epoch 152, loss: 18626.223954\n",
      "Epoch 153, loss: 18612.680014\n",
      "Epoch 154, loss: 18611.155777\n",
      "Epoch 155, loss: 18622.333930\n",
      "Epoch 156, loss: 18616.768895\n",
      "Epoch 157, loss: 18609.641708\n",
      "Epoch 158, loss: 18612.066244\n",
      "Epoch 159, loss: 18604.673248\n",
      "Epoch 160, loss: 18606.398303\n",
      "Epoch 161, loss: 18602.075052\n",
      "Epoch 162, loss: 18593.459001\n",
      "Epoch 163, loss: 18596.798909\n",
      "Epoch 164, loss: 18590.315178\n",
      "Epoch 165, loss: 18589.560712\n",
      "Epoch 166, loss: 18588.194665\n",
      "Epoch 167, loss: 18586.645493\n",
      "Epoch 168, loss: 18580.831026\n",
      "Epoch 169, loss: 18583.576785\n",
      "Epoch 170, loss: 18578.103455\n",
      "Epoch 171, loss: 18580.714815\n",
      "Epoch 172, loss: 18581.387936\n",
      "Epoch 173, loss: 18573.760786\n",
      "Epoch 174, loss: 18566.655026\n",
      "Epoch 175, loss: 18565.600039\n",
      "Epoch 176, loss: 18561.992941\n",
      "Epoch 177, loss: 18561.158589\n",
      "Epoch 178, loss: 18560.185932\n",
      "Epoch 179, loss: 18555.221982\n",
      "Epoch 180, loss: 18561.942566\n",
      "Epoch 181, loss: 18555.259385\n",
      "Epoch 182, loss: 18555.541193\n",
      "Epoch 183, loss: 18555.399481\n",
      "Epoch 184, loss: 18553.054642\n",
      "Epoch 185, loss: 18553.636166\n",
      "Epoch 186, loss: 18542.543058\n",
      "Epoch 187, loss: 18547.000249\n",
      "Epoch 188, loss: 18546.662088\n",
      "Epoch 189, loss: 18540.452040\n",
      "Epoch 190, loss: 18536.589920\n",
      "Epoch 191, loss: 18535.777461\n",
      "Epoch 192, loss: 18535.659418\n",
      "Epoch 193, loss: 18531.854585\n",
      "Epoch 194, loss: 18531.701427\n",
      "Epoch 195, loss: 18523.233179\n",
      "Epoch 196, loss: 18523.325928\n",
      "Epoch 197, loss: 18520.945875\n",
      "Epoch 198, loss: 18520.087046\n",
      "Epoch 199, loss: 18520.414662\n",
      "0.0001 1e-05 0.244\n",
      "Epoch 0, loss: 20645.107777\n",
      "Epoch 1, loss: 20445.152632\n",
      "Epoch 2, loss: 20294.288560\n",
      "Epoch 3, loss: 20156.622829\n",
      "Epoch 4, loss: 20042.501589\n",
      "Epoch 5, loss: 19950.576271\n",
      "Epoch 6, loss: 19866.479382\n",
      "Epoch 7, loss: 19797.535738\n",
      "Epoch 8, loss: 19738.237281\n",
      "Epoch 9, loss: 19678.152514\n",
      "Epoch 10, loss: 19640.784351\n",
      "Epoch 11, loss: 19595.284615\n",
      "Epoch 12, loss: 19560.177257\n",
      "Epoch 13, loss: 19519.674682\n",
      "Epoch 14, loss: 19496.269473\n",
      "Epoch 15, loss: 19460.613882\n",
      "Epoch 16, loss: 19442.804360\n",
      "Epoch 17, loss: 19417.239609\n",
      "Epoch 18, loss: 19392.597486\n",
      "Epoch 19, loss: 19368.311714\n",
      "Epoch 20, loss: 19353.687696\n",
      "Epoch 21, loss: 19334.251944\n",
      "Epoch 22, loss: 19315.173590\n",
      "Epoch 23, loss: 19304.366813\n",
      "Epoch 24, loss: 19281.988838\n",
      "Epoch 25, loss: 19271.344600\n",
      "Epoch 26, loss: 19252.650976\n",
      "Epoch 27, loss: 19244.497996\n",
      "Epoch 28, loss: 19230.093314\n",
      "Epoch 29, loss: 19217.765304\n",
      "Epoch 30, loss: 19208.383873\n",
      "Epoch 31, loss: 19190.050926\n",
      "Epoch 32, loss: 19180.915538\n",
      "Epoch 33, loss: 19172.326723\n",
      "Epoch 34, loss: 19158.021911\n",
      "Epoch 35, loss: 19148.651253\n",
      "Epoch 36, loss: 19136.514881\n",
      "Epoch 37, loss: 19131.985400\n",
      "Epoch 38, loss: 19121.246909\n",
      "Epoch 39, loss: 19108.514149\n",
      "Epoch 40, loss: 19098.546786\n",
      "Epoch 41, loss: 19092.198305\n",
      "Epoch 42, loss: 19078.895786\n",
      "Epoch 43, loss: 19072.850821\n",
      "Epoch 44, loss: 19069.407674\n",
      "Epoch 45, loss: 19054.991465\n",
      "Epoch 46, loss: 19052.468180\n",
      "Epoch 47, loss: 19051.618858\n",
      "Epoch 48, loss: 19036.713115\n",
      "Epoch 49, loss: 19025.998046\n",
      "Epoch 50, loss: 19014.032951\n",
      "Epoch 51, loss: 19014.423773\n",
      "Epoch 52, loss: 19007.202981\n",
      "Epoch 53, loss: 19007.004115\n",
      "Epoch 54, loss: 18989.757301\n",
      "Epoch 55, loss: 18990.029673\n",
      "Epoch 56, loss: 18978.317497\n",
      "Epoch 57, loss: 18979.713900\n",
      "Epoch 58, loss: 18962.658243\n",
      "Epoch 59, loss: 18965.911279\n",
      "Epoch 60, loss: 18957.839777\n",
      "Epoch 61, loss: 18955.519494\n",
      "Epoch 62, loss: 18945.318857\n",
      "Epoch 63, loss: 18941.318915\n",
      "Epoch 64, loss: 18926.495807\n",
      "Epoch 65, loss: 18933.610770\n",
      "Epoch 66, loss: 18926.285367\n",
      "Epoch 67, loss: 18913.760514\n",
      "Epoch 68, loss: 18912.614556\n",
      "Epoch 69, loss: 18905.305720\n",
      "Epoch 70, loss: 18907.661276\n",
      "Epoch 71, loss: 18893.658590\n",
      "Epoch 72, loss: 18891.193630\n",
      "Epoch 73, loss: 18890.734041\n",
      "Epoch 74, loss: 18881.178686\n",
      "Epoch 75, loss: 18875.109846\n",
      "Epoch 76, loss: 18870.769868\n",
      "Epoch 77, loss: 18864.466745\n",
      "Epoch 78, loss: 18865.215258\n",
      "Epoch 79, loss: 18860.205280\n",
      "Epoch 80, loss: 18860.023338\n",
      "Epoch 81, loss: 18854.609425\n",
      "Epoch 82, loss: 18848.262890\n",
      "Epoch 83, loss: 18841.875239\n",
      "Epoch 84, loss: 18843.522084\n",
      "Epoch 85, loss: 18835.615578\n",
      "Epoch 86, loss: 18826.438794\n",
      "Epoch 87, loss: 18828.553378\n",
      "Epoch 88, loss: 18818.454177\n",
      "Epoch 89, loss: 18816.403621\n",
      "Epoch 90, loss: 18820.237512\n",
      "Epoch 91, loss: 18810.467183\n",
      "Epoch 92, loss: 18810.523417\n",
      "Epoch 93, loss: 18801.029763\n",
      "Epoch 94, loss: 18803.118311\n",
      "Epoch 95, loss: 18793.896828\n",
      "Epoch 96, loss: 18795.109693\n",
      "Epoch 97, loss: 18792.381507\n",
      "Epoch 98, loss: 18783.991084\n",
      "Epoch 99, loss: 18778.171142\n",
      "Epoch 100, loss: 18775.031346\n",
      "Epoch 101, loss: 18771.066907\n",
      "Epoch 102, loss: 18771.685104\n",
      "Epoch 103, loss: 18767.867444\n",
      "Epoch 104, loss: 18769.504813\n",
      "Epoch 105, loss: 18762.265068\n",
      "Epoch 106, loss: 18754.580747\n",
      "Epoch 107, loss: 18755.363688\n",
      "Epoch 108, loss: 18749.800444\n",
      "Epoch 109, loss: 18746.360720\n",
      "Epoch 110, loss: 18742.235799\n",
      "Epoch 111, loss: 18736.122411\n",
      "Epoch 112, loss: 18741.025523\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 113, loss: 18735.333335\n",
      "Epoch 114, loss: 18728.466331\n",
      "Epoch 115, loss: 18727.596613\n",
      "Epoch 116, loss: 18722.702876\n",
      "Epoch 117, loss: 18719.336569\n",
      "Epoch 118, loss: 18719.060131\n",
      "Epoch 119, loss: 18709.590425\n",
      "Epoch 120, loss: 18707.059072\n",
      "Epoch 121, loss: 18703.911200\n",
      "Epoch 122, loss: 18706.042808\n",
      "Epoch 123, loss: 18696.082882\n",
      "Epoch 124, loss: 18697.197155\n",
      "Epoch 125, loss: 18695.055933\n",
      "Epoch 126, loss: 18692.919510\n",
      "Epoch 127, loss: 18682.749643\n",
      "Epoch 128, loss: 18688.106962\n",
      "Epoch 129, loss: 18686.075456\n",
      "Epoch 130, loss: 18674.707844\n",
      "Epoch 131, loss: 18680.766043\n",
      "Epoch 132, loss: 18674.458585\n",
      "Epoch 133, loss: 18672.767326\n",
      "Epoch 134, loss: 18675.325542\n",
      "Epoch 135, loss: 18663.778297\n",
      "Epoch 136, loss: 18660.621702\n",
      "Epoch 137, loss: 18665.340995\n",
      "Epoch 138, loss: 18656.758504\n",
      "Epoch 139, loss: 18652.691895\n",
      "Epoch 140, loss: 18652.116330\n",
      "Epoch 141, loss: 18652.870424\n",
      "Epoch 142, loss: 18648.602884\n",
      "Epoch 143, loss: 18646.348149\n",
      "Epoch 144, loss: 18639.737159\n",
      "Epoch 145, loss: 18640.491606\n",
      "Epoch 146, loss: 18633.701457\n",
      "Epoch 147, loss: 18635.315437\n",
      "Epoch 148, loss: 18636.367114\n",
      "Epoch 149, loss: 18633.931827\n",
      "Epoch 150, loss: 18626.100445\n",
      "Epoch 151, loss: 18627.781529\n",
      "Epoch 152, loss: 18622.467665\n",
      "Epoch 153, loss: 18616.685362\n",
      "Epoch 154, loss: 18620.035284\n",
      "Epoch 155, loss: 18617.731963\n",
      "Epoch 156, loss: 18610.157358\n",
      "Epoch 157, loss: 18608.973322\n",
      "Epoch 158, loss: 18604.227898\n",
      "Epoch 159, loss: 18601.353213\n",
      "Epoch 160, loss: 18605.889397\n",
      "Epoch 161, loss: 18600.664306\n",
      "Epoch 162, loss: 18597.500218\n",
      "Epoch 163, loss: 18595.612538\n",
      "Epoch 164, loss: 18595.778713\n",
      "Epoch 165, loss: 18592.159375\n",
      "Epoch 166, loss: 18584.669613\n",
      "Epoch 167, loss: 18593.342845\n",
      "Epoch 168, loss: 18582.183971\n",
      "Epoch 169, loss: 18577.448858\n",
      "Epoch 170, loss: 18581.658038\n",
      "Epoch 171, loss: 18575.686215\n",
      "Epoch 172, loss: 18569.361005\n",
      "Epoch 173, loss: 18575.972571\n",
      "Epoch 174, loss: 18574.195910\n",
      "Epoch 175, loss: 18567.193394\n",
      "Epoch 176, loss: 18564.438557\n",
      "Epoch 177, loss: 18563.925272\n",
      "Epoch 178, loss: 18560.849390\n",
      "Epoch 179, loss: 18557.146004\n",
      "Epoch 180, loss: 18554.620001\n",
      "Epoch 181, loss: 18558.942538\n",
      "Epoch 182, loss: 18552.063354\n",
      "Epoch 183, loss: 18547.446203\n",
      "Epoch 184, loss: 18545.289210\n",
      "Epoch 185, loss: 18550.564487\n",
      "Epoch 186, loss: 18545.800494\n",
      "Epoch 187, loss: 18542.574276\n",
      "Epoch 188, loss: 18538.591555\n",
      "Epoch 189, loss: 18541.994224\n",
      "Epoch 190, loss: 18533.679423\n",
      "Epoch 191, loss: 18538.972183\n",
      "Epoch 192, loss: 18533.732701\n",
      "Epoch 193, loss: 18533.941235\n",
      "Epoch 194, loss: 18525.560487\n",
      "Epoch 195, loss: 18521.257261\n",
      "Epoch 196, loss: 18522.524349\n",
      "Epoch 197, loss: 18520.725599\n",
      "Epoch 198, loss: 18516.660846\n",
      "Epoch 199, loss: 18522.101808\n",
      "0.0001 1e-06 0.254\n",
      "Epoch 0, loss: 20714.873841\n",
      "Epoch 1, loss: 20689.020148\n",
      "Epoch 2, loss: 20665.057664\n",
      "Epoch 3, loss: 20641.596220\n",
      "Epoch 4, loss: 20619.266578\n",
      "Epoch 5, loss: 20597.397008\n",
      "Epoch 6, loss: 20576.206215\n",
      "Epoch 7, loss: 20555.127659\n",
      "Epoch 8, loss: 20534.387274\n",
      "Epoch 9, loss: 20515.536912\n",
      "Epoch 10, loss: 20495.244532\n",
      "Epoch 11, loss: 20476.383552\n",
      "Epoch 12, loss: 20458.037458\n",
      "Epoch 13, loss: 20439.406561\n",
      "Epoch 14, loss: 20421.473677\n",
      "Epoch 15, loss: 20404.832584\n",
      "Epoch 16, loss: 20386.703944\n",
      "Epoch 17, loss: 20370.565075\n",
      "Epoch 18, loss: 20353.451777\n",
      "Epoch 19, loss: 20337.848377\n",
      "Epoch 20, loss: 20321.425953\n",
      "Epoch 21, loss: 20305.517451\n",
      "Epoch 22, loss: 20290.806791\n",
      "Epoch 23, loss: 20275.351545\n",
      "Epoch 24, loss: 20260.182957\n",
      "Epoch 25, loss: 20246.009149\n",
      "Epoch 26, loss: 20232.141110\n",
      "Epoch 27, loss: 20217.834529\n",
      "Epoch 28, loss: 20204.617693\n",
      "Epoch 29, loss: 20190.903941\n",
      "Epoch 30, loss: 20177.701724\n",
      "Epoch 31, loss: 20164.406781\n",
      "Epoch 32, loss: 20152.430403\n",
      "Epoch 33, loss: 20139.251023\n",
      "Epoch 34, loss: 20127.012848\n",
      "Epoch 35, loss: 20114.694230\n",
      "Epoch 36, loss: 20102.640071\n",
      "Epoch 37, loss: 20091.596694\n",
      "Epoch 38, loss: 20079.220442\n",
      "Epoch 39, loss: 20068.040549\n",
      "Epoch 40, loss: 20056.974972\n",
      "Epoch 41, loss: 20045.744660\n",
      "Epoch 42, loss: 20035.253625\n",
      "Epoch 43, loss: 20023.941792\n",
      "Epoch 44, loss: 20014.042749\n",
      "Epoch 45, loss: 20004.148617\n",
      "Epoch 46, loss: 19993.947175\n",
      "Epoch 47, loss: 19983.828324\n",
      "Epoch 48, loss: 19973.618418\n",
      "Epoch 49, loss: 19964.422369\n",
      "Epoch 50, loss: 19954.790747\n",
      "Epoch 51, loss: 19945.402343\n",
      "Epoch 52, loss: 19936.361962\n",
      "Epoch 53, loss: 19927.058304\n",
      "Epoch 54, loss: 19918.684519\n",
      "Epoch 55, loss: 19909.537507\n",
      "Epoch 56, loss: 19900.733422\n",
      "Epoch 57, loss: 19892.363859\n",
      "Epoch 58, loss: 19883.928444\n",
      "Epoch 59, loss: 19875.962311\n",
      "Epoch 60, loss: 19867.973133\n",
      "Epoch 61, loss: 19859.944571\n",
      "Epoch 62, loss: 19852.262987\n",
      "Epoch 63, loss: 19844.509441\n",
      "Epoch 64, loss: 19837.020949\n",
      "Epoch 65, loss: 19829.380193\n",
      "Epoch 66, loss: 19821.551709\n",
      "Epoch 67, loss: 19814.971764\n",
      "Epoch 68, loss: 19807.267702\n",
      "Epoch 69, loss: 19800.463147\n",
      "Epoch 70, loss: 19793.304030\n",
      "Epoch 71, loss: 19786.219624\n",
      "Epoch 72, loss: 19780.006679\n",
      "Epoch 73, loss: 19772.676801\n",
      "Epoch 74, loss: 19766.553205\n",
      "Epoch 75, loss: 19759.678823\n",
      "Epoch 76, loss: 19754.195064\n",
      "Epoch 77, loss: 19747.580549\n",
      "Epoch 78, loss: 19740.928577\n",
      "Epoch 79, loss: 19735.364327\n",
      "Epoch 80, loss: 19728.629125\n",
      "Epoch 81, loss: 19723.106834\n",
      "Epoch 82, loss: 19717.045149\n",
      "Epoch 83, loss: 19710.914292\n",
      "Epoch 84, loss: 19705.700854\n",
      "Epoch 85, loss: 19699.762593\n",
      "Epoch 86, loss: 19694.136516\n",
      "Epoch 87, loss: 19689.267400\n",
      "Epoch 88, loss: 19683.815772\n",
      "Epoch 89, loss: 19677.636531\n",
      "Epoch 90, loss: 19672.815515\n",
      "Epoch 91, loss: 19667.313817\n",
      "Epoch 92, loss: 19663.029739\n",
      "Epoch 93, loss: 19657.837917\n",
      "Epoch 94, loss: 19652.822646\n",
      "Epoch 95, loss: 19647.586923\n",
      "Epoch 96, loss: 19642.914989\n",
      "Epoch 97, loss: 19637.752031\n",
      "Epoch 98, loss: 19633.187865\n",
      "Epoch 99, loss: 19628.433757\n",
      "Epoch 100, loss: 19624.212812\n",
      "Epoch 101, loss: 19618.963714\n",
      "Epoch 102, loss: 19614.783320\n",
      "Epoch 103, loss: 19610.422214\n",
      "Epoch 104, loss: 19605.728706\n",
      "Epoch 105, loss: 19601.228072\n",
      "Epoch 106, loss: 19596.932536\n",
      "Epoch 107, loss: 19592.540635\n",
      "Epoch 108, loss: 19588.581121\n",
      "Epoch 109, loss: 19584.424707\n",
      "Epoch 110, loss: 19580.276751\n",
      "Epoch 111, loss: 19576.633256\n",
      "Epoch 112, loss: 19571.919578\n",
      "Epoch 113, loss: 19567.967881\n",
      "Epoch 114, loss: 19564.324022\n",
      "Epoch 115, loss: 19560.453736\n",
      "Epoch 116, loss: 19556.722233\n",
      "Epoch 117, loss: 19552.187191\n",
      "Epoch 118, loss: 19548.985657\n",
      "Epoch 119, loss: 19545.063712\n",
      "Epoch 120, loss: 19541.432782\n",
      "Epoch 121, loss: 19537.775844\n",
      "Epoch 122, loss: 19534.128732\n",
      "Epoch 123, loss: 19530.260835\n",
      "Epoch 124, loss: 19526.824321\n",
      "Epoch 125, loss: 19523.051979\n",
      "Epoch 126, loss: 19520.171047\n",
      "Epoch 127, loss: 19516.343922\n",
      "Epoch 128, loss: 19513.091851\n",
      "Epoch 129, loss: 19510.062623\n",
      "Epoch 130, loss: 19506.573181\n",
      "Epoch 131, loss: 19503.149344\n",
      "Epoch 132, loss: 19499.465985\n",
      "Epoch 133, loss: 19496.783675\n",
      "Epoch 134, loss: 19493.189156\n",
      "Epoch 135, loss: 19489.769144\n",
      "Epoch 136, loss: 19487.386560\n",
      "Epoch 137, loss: 19483.797703\n",
      "Epoch 138, loss: 19480.540192\n",
      "Epoch 139, loss: 19477.455342\n",
      "Epoch 140, loss: 19474.219598\n",
      "Epoch 141, loss: 19471.955577\n",
      "Epoch 142, loss: 19468.508301\n",
      "Epoch 143, loss: 19466.109755\n",
      "Epoch 144, loss: 19462.891202\n",
      "Epoch 145, loss: 19460.100274\n",
      "Epoch 146, loss: 19457.031582\n",
      "Epoch 147, loss: 19454.071522\n",
      "Epoch 148, loss: 19451.282021\n",
      "Epoch 149, loss: 19448.155586\n",
      "Epoch 150, loss: 19445.568563\n",
      "Epoch 151, loss: 19443.053594\n",
      "Epoch 152, loss: 19439.715194\n",
      "Epoch 153, loss: 19437.345835\n",
      "Epoch 154, loss: 19434.690592\n",
      "Epoch 155, loss: 19432.138954\n",
      "Epoch 156, loss: 19429.423943\n",
      "Epoch 157, loss: 19427.089759\n",
      "Epoch 158, loss: 19423.967488\n",
      "Epoch 159, loss: 19421.841382\n",
      "Epoch 160, loss: 19419.126826\n",
      "Epoch 161, loss: 19416.959009\n",
      "Epoch 162, loss: 19414.165328\n",
      "Epoch 163, loss: 19411.032404\n",
      "Epoch 164, loss: 19408.866619\n",
      "Epoch 165, loss: 19406.739714\n",
      "Epoch 166, loss: 19403.850698\n",
      "Epoch 167, loss: 19401.345685\n",
      "Epoch 168, loss: 19399.013453\n",
      "Epoch 169, loss: 19397.235234\n",
      "Epoch 170, loss: 19394.336694\n",
      "Epoch 171, loss: 19392.515901\n",
      "Epoch 172, loss: 19389.487861\n",
      "Epoch 173, loss: 19387.671351\n",
      "Epoch 174, loss: 19385.224066\n",
      "Epoch 175, loss: 19382.482948\n",
      "Epoch 176, loss: 19380.501767\n",
      "Epoch 177, loss: 19378.418425\n",
      "Epoch 178, loss: 19376.158556\n",
      "Epoch 179, loss: 19373.598404\n",
      "Epoch 180, loss: 19371.944774\n",
      "Epoch 181, loss: 19369.618708\n",
      "Epoch 182, loss: 19366.595288\n",
      "Epoch 183, loss: 19365.153952\n",
      "Epoch 184, loss: 19362.813354\n",
      "Epoch 185, loss: 19360.228290\n",
      "Epoch 186, loss: 19358.566618\n",
      "Epoch 187, loss: 19356.347915\n",
      "Epoch 188, loss: 19354.396171\n",
      "Epoch 189, loss: 19351.877720\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 190, loss: 19350.393411\n",
      "Epoch 191, loss: 19348.018223\n",
      "Epoch 192, loss: 19346.087247\n",
      "Epoch 193, loss: 19343.970942\n",
      "Epoch 194, loss: 19341.915445\n",
      "Epoch 195, loss: 19340.084258\n",
      "Epoch 196, loss: 19338.065561\n",
      "Epoch 197, loss: 19335.906796\n",
      "Epoch 198, loss: 19333.977538\n",
      "Epoch 199, loss: 19331.859299\n",
      "1e-05 0.0001 0.231\n",
      "Epoch 0, loss: 20714.435938\n",
      "Epoch 1, loss: 20688.456211\n",
      "Epoch 2, loss: 20664.011852\n",
      "Epoch 3, loss: 20640.910620\n",
      "Epoch 4, loss: 20618.412587\n",
      "Epoch 5, loss: 20597.096391\n",
      "Epoch 6, loss: 20575.023958\n",
      "Epoch 7, loss: 20554.934627\n",
      "Epoch 8, loss: 20533.985073\n",
      "Epoch 9, loss: 20514.517249\n",
      "Epoch 10, loss: 20494.722583\n",
      "Epoch 11, loss: 20475.808699\n",
      "Epoch 12, loss: 20456.834494\n",
      "Epoch 13, loss: 20438.615456\n",
      "Epoch 14, loss: 20420.519398\n",
      "Epoch 15, loss: 20403.267497\n",
      "Epoch 16, loss: 20386.330767\n",
      "Epoch 17, loss: 20369.642620\n",
      "Epoch 18, loss: 20353.060763\n",
      "Epoch 19, loss: 20336.260840\n",
      "Epoch 20, loss: 20320.648682\n",
      "Epoch 21, loss: 20305.190231\n",
      "Epoch 22, loss: 20289.719387\n",
      "Epoch 23, loss: 20274.444956\n",
      "Epoch 24, loss: 20260.051997\n",
      "Epoch 25, loss: 20245.628375\n",
      "Epoch 26, loss: 20231.275248\n",
      "Epoch 27, loss: 20217.071367\n",
      "Epoch 28, loss: 20203.408905\n",
      "Epoch 29, loss: 20189.999923\n",
      "Epoch 30, loss: 20176.690674\n",
      "Epoch 31, loss: 20163.851038\n",
      "Epoch 32, loss: 20151.185909\n",
      "Epoch 33, loss: 20138.318228\n",
      "Epoch 34, loss: 20126.075556\n",
      "Epoch 35, loss: 20113.510212\n",
      "Epoch 36, loss: 20101.559641\n",
      "Epoch 37, loss: 20090.033061\n",
      "Epoch 38, loss: 20078.708985\n",
      "Epoch 39, loss: 20067.099696\n",
      "Epoch 40, loss: 20055.966652\n",
      "Epoch 41, loss: 20044.891990\n",
      "Epoch 42, loss: 20034.338524\n",
      "Epoch 43, loss: 20023.475492\n",
      "Epoch 44, loss: 20013.202251\n",
      "Epoch 45, loss: 20002.999179\n",
      "Epoch 46, loss: 19992.441749\n",
      "Epoch 47, loss: 19983.034483\n",
      "Epoch 48, loss: 19972.499198\n",
      "Epoch 49, loss: 19963.570984\n",
      "Epoch 50, loss: 19953.575313\n",
      "Epoch 51, loss: 19944.771917\n",
      "Epoch 52, loss: 19935.587191\n",
      "Epoch 53, loss: 19926.586135\n",
      "Epoch 54, loss: 19917.239124\n",
      "Epoch 55, loss: 19908.856467\n",
      "Epoch 56, loss: 19899.957410\n",
      "Epoch 57, loss: 19891.304783\n",
      "Epoch 58, loss: 19883.553745\n",
      "Epoch 59, loss: 19875.290072\n",
      "Epoch 60, loss: 19866.839697\n",
      "Epoch 61, loss: 19858.685791\n",
      "Epoch 62, loss: 19851.619627\n",
      "Epoch 63, loss: 19843.287683\n",
      "Epoch 64, loss: 19835.732035\n",
      "Epoch 65, loss: 19827.988026\n",
      "Epoch 66, loss: 19820.780679\n",
      "Epoch 67, loss: 19813.598643\n",
      "Epoch 68, loss: 19805.936719\n",
      "Epoch 69, loss: 19799.260926\n",
      "Epoch 70, loss: 19792.406617\n",
      "Epoch 71, loss: 19785.243489\n",
      "Epoch 72, loss: 19778.358405\n",
      "Epoch 73, loss: 19771.871427\n",
      "Epoch 74, loss: 19765.544073\n",
      "Epoch 75, loss: 19759.089856\n",
      "Epoch 76, loss: 19753.205724\n",
      "Epoch 77, loss: 19746.348934\n",
      "Epoch 78, loss: 19740.439781\n",
      "Epoch 79, loss: 19734.576714\n",
      "Epoch 80, loss: 19727.965314\n",
      "Epoch 81, loss: 19721.669639\n",
      "Epoch 82, loss: 19715.775581\n",
      "Epoch 83, loss: 19710.178297\n",
      "Epoch 84, loss: 19704.732694\n",
      "Epoch 85, loss: 19699.065685\n",
      "Epoch 86, loss: 19693.027143\n",
      "Epoch 87, loss: 19687.388593\n",
      "Epoch 88, loss: 19682.777006\n",
      "Epoch 89, loss: 19676.911819\n",
      "Epoch 90, loss: 19671.589609\n",
      "Epoch 91, loss: 19666.936506\n",
      "Epoch 92, loss: 19661.742760\n",
      "Epoch 93, loss: 19656.291003\n",
      "Epoch 94, loss: 19651.216622\n",
      "Epoch 95, loss: 19646.161461\n",
      "Epoch 96, loss: 19641.469956\n",
      "Epoch 97, loss: 19636.528117\n",
      "Epoch 98, loss: 19631.852242\n",
      "Epoch 99, loss: 19627.452280\n",
      "Epoch 100, loss: 19622.959845\n",
      "Epoch 101, loss: 19617.615895\n",
      "Epoch 102, loss: 19613.497788\n",
      "Epoch 103, loss: 19609.124835\n",
      "Epoch 104, loss: 19604.429321\n",
      "Epoch 105, loss: 19600.344717\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-9867a9f15f98>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mreg_str\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreg_strengths\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mclassifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinear_classifer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinearSoftmaxClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreg_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_X\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmulticlass_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Courses/DL/dlcourse-master/assignments/assignment1_right/linear_classifer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, batch_size, learning_rate, reg, epochs)\u001b[0m\n\u001b[1;32m    185\u001b[0m                 \u001b[0mbatch_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m                 \u001b[0mloss_softmax\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdW_softmax\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinear_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m                 \u001b[0mloss_relur\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdW_regul\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ml2_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreg_strength\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mloss_softmax\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss_relur\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Courses/DL/dlcourse-master/assignments/assignment1_right/linear_classifer.py\u001b[0m in \u001b[0;36mlinear_softmax\u001b[0;34m(X, W, target_index)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;31m#     loss, dprediction = softmax_with_cross_entropy(predictions, target_index)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;31m#     dW = X.transpose().dot(dprediction)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoftmax_with_cross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0mdW\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 200\n",
    "batch_size = 300\n",
    "\n",
    "learning_rates = [1e-3, 1e-4, 1e-5]\n",
    "reg_strengths = [1e-4, 1e-5, 1e-6]\n",
    "\n",
    "best_classifier = None\n",
    "best_val_accuracy = None\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for reg_str in reg_strengths:\n",
    "        classifier = linear_classifer.LinearSoftmaxClassifier()\n",
    "        classifier.fit(train_X, train_y, epochs=num_epochs, learning_rate=lr, batch_size=batch_size, reg=reg_str)\n",
    "        pred = classifier.predict(val_X)\n",
    "        accuracy = multiclass_accuracy(pred, val_y)\n",
    "        if best_val_accuracy is None or best_val_accuracy < accuracy:\n",
    "            best_val_accuracy = accuracy\n",
    "            best_classifier = classifier\n",
    "        print(lr, reg_str, accuracy)\n",
    "            \n",
    "print('best validation accuracy achieved: %f' % best_val_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Какой же точности мы добились на тестовых данных?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear softmax classifier test set accuracy: 0.210000\n"
     ]
    }
   ],
   "source": [
    "test_pred = best_classifier.predict(test_X)\n",
    "test_accuracy = multiclass_accuracy(test_pred, test_y)\n",
    "print('Linear softmax classifier test set accuracy: %f' % (test_accuracy, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
