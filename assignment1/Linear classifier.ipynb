{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 1.2 - Линейный классификатор (Linear classifier)\n",
    "\n",
    "В этом задании мы реализуем другую модель машинного обучения - линейный классификатор. Линейный классификатор подбирает для каждого класса веса, на которые нужно умножить значение каждого признака и потом сложить вместе.\n",
    "Тот класс, у которого эта сумма больше, и является предсказанием модели.\n",
    "\n",
    "В этом задании вы:\n",
    "- потренируетесь считать градиенты различных многомерных функций\n",
    "- реализуете подсчет градиентов через линейную модель и функцию потерь softmax\n",
    "- реализуете процесс тренировки линейного классификатора\n",
    "- подберете параметры тренировки на практике\n",
    "\n",
    "На всякий случай, еще раз ссылка на туториал по numpy:  \n",
    "http://cs231n.github.io/python-numpy-tutorial/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import load_svhn, random_split_train_val\n",
    "from gradient_check import check_gradient\n",
    "from metrics import multiclass_accuracy \n",
    "import linear_classifer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Как всегда, первым делом загружаем данные\n",
    "\n",
    "Мы будем использовать все тот же SVHN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_linear_classifier(train_X, test_X):\n",
    "    train_flat = train_X.reshape(train_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    test_flat = test_X.reshape(test_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    \n",
    "    # Subtract mean\n",
    "    mean_image = np.mean(train_flat, axis = 0)\n",
    "    train_flat -= mean_image\n",
    "    test_flat -= mean_image\n",
    "    \n",
    "    # Add another channel with ones as a bias term\n",
    "    train_flat_with_ones = np.hstack([train_flat, np.ones((train_X.shape[0], 1))])\n",
    "    test_flat_with_ones = np.hstack([test_flat, np.ones((test_X.shape[0], 1))])    \n",
    "    return train_flat_with_ones, test_flat_with_ones\n",
    "    \n",
    "train_X, train_y, test_X, test_y = load_svhn(\"data\", max_train=10000, max_test=1000)    \n",
    "train_X, test_X = prepare_for_linear_classifier(train_X, test_X)\n",
    "# Split train into train and val\n",
    "train_X, train_y, val_X, val_y = random_split_train_val(train_X, train_y, num_val = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Играемся с градиентами!\n",
    "\n",
    "В этом курсе мы будем писать много функций, которые вычисляют градиенты аналитическим методом.\n",
    "\n",
    "Все функции, в которых мы будем вычислять градиенты, будут написаны по одной и той же схеме.  \n",
    "Они будут получать на вход точку, где нужно вычислить значение и градиент функции, а на выходе будут выдавать кортеж (tuple) из двух значений - собственно значения функции в этой точке (всегда одно число) и аналитического значения градиента в той же точке (той же размерности, что и вход).\n",
    "```\n",
    "def f(x):\n",
    "    \"\"\"\n",
    "    Computes function and analytic gradient at x\n",
    "    \n",
    "    x: np array of float, input to the function\n",
    "    \n",
    "    Returns:\n",
    "    value: float, value of the function \n",
    "    grad: np array of float, same shape as x\n",
    "    \"\"\"\n",
    "    ...\n",
    "    \n",
    "    return value, grad\n",
    "```\n",
    "\n",
    "Необходимым инструментом во время реализации кода, вычисляющего градиенты, является функция его проверки. Эта функция вычисляет градиент численным методом и сверяет результат с градиентом, вычисленным аналитическим методом.\n",
    "\n",
    "Мы начнем с того, чтобы реализовать вычисление численного градиента (numeric gradient) в функции `check_gradient` в `gradient_check.py`. Эта функция будет принимать на вход функции формата, заданного выше, использовать значение `value` для вычисления численного градиента и сравнит его с аналитическим - они должны сходиться.\n",
    "\n",
    "Напишите часть функции, которая вычисляет градиент с помощью численной производной для каждой координаты. Для вычисления производной используйте так называемую two-point formula (https://en.wikipedia.org/wiki/Numerical_differentiation):\n",
    "\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/22fc2c0a66c63560a349604f8b6b39221566236d)\n",
    "\n",
    "Все функции приведенные в следующей клетке должны проходить gradient check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradients are not different at (0,). Analytic: 6.00000, Numeric: 6.00000\n",
      "Gradient check passed!\n",
      "Gradients are not different at (0,). Analytic: 1.00000, Numeric: 1.00000\n",
      "Gradients are not different at (1,). Analytic: 1.00000, Numeric: 1.00000\n",
      "Gradient check passed!\n",
      "Gradients are not different at (0, 0). Analytic: 1.00000, Numeric: 1.00000\n",
      "Gradients are not different at (0, 1). Analytic: 1.00000, Numeric: 1.00000\n",
      "Gradients are not different at (1, 0). Analytic: 1.00000, Numeric: 1.00000\n",
      "Gradients are not different at (1, 1). Analytic: 1.00000, Numeric: 1.00000\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Implement check_gradient function in gradient_check.py\n",
    "# All the functions below should pass the gradient check\n",
    "\n",
    "def square(x):\n",
    "    return float(x*x), 2*x\n",
    "\n",
    "check_gradient(square, np.array([3.0]))\n",
    "\n",
    "def array_sum(x):\n",
    "    assert x.shape == (2,), x.shape\n",
    "    return np.sum(x), np.ones_like(x)\n",
    "\n",
    "check_gradient(array_sum, np.array([3.0, 2.0]))\n",
    "\n",
    "def array_2d_sum(x):\n",
    "    assert x.shape == (2,2)\n",
    "    return np.sum(x), np.ones_like(x)\n",
    "\n",
    "check_gradient(array_2d_sum, np.array([[3.0, 2.0], [1.0, 0.0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Начинаем писать свои функции, считающие аналитический градиент\n",
    "\n",
    "Теперь реализуем функцию softmax, которая получает на вход оценки для каждого класса и преобразует их в вероятности от 0 до 1:\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/e348290cf48ddbb6e9a6ef4e39363568b67c09d3)\n",
    "\n",
    "**Важно:** Практический аспект вычисления этой функции заключается в том, что в ней учавствует вычисление экспоненты от потенциально очень больших чисел - это может привести к очень большим значениям в числителе и знаменателе за пределами диапазона float.\n",
    "\n",
    "К счастью, у этой проблемы есть простое решение -- перед вычислением softmax вычесть из всех оценок максимальное значение среди всех оценок:\n",
    "```\n",
    "predictions -= np.max(predictions)\n",
    "```\n",
    "(подробнее здесь - http://cs231n.github.io/linear-classify/#softmax, секция `Practical issues: Numeric stability`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Implement softmax and cross-entropy for single sample\n",
    "probs = linear_classifer.softmax(np.array([-10, 0, 10]))\n",
    "\n",
    "# Make sure it works for big numbers too!\n",
    "probs = linear_classifer.softmax(np.array([1000, 0, 0]))\n",
    "assert np.isclose(probs[0], 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кроме этого, мы реализуем cross-entropy loss, которую мы будем использовать как функцию ошибки (error function).\n",
    "В общем виде cross-entropy определена следующим образом:\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/0cb6da032ab424eefdca0884cd4113fe578f4293)\n",
    "\n",
    "где x - все классы, p(x) - истинная вероятность принадлежности сэмпла классу x, а q(x) - вероятность принадлежности классу x, предсказанная моделью.  \n",
    "В нашем случае сэмпл принадлежит только одному классу, индекс которого передается функции. Для него p(x) равна 1, а для остальных классов - 0. \n",
    "\n",
    "Это позволяет реализовать функцию проще!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.006760443547122"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = linear_classifer.softmax(np.array([-5, 0, 5]))\n",
    "linear_classifer.cross_entropy_loss(probs, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После того как мы реализовали сами функции, мы можем реализовать градиент.\n",
    "\n",
    "Оказывается, что вычисление градиента становится гораздо проще, если объединить эти функции в одну, которая сначала вычисляет вероятности через softmax, а потом использует их для вычисления функции ошибки через cross-entropy loss.\n",
    "\n",
    "Эта функция `softmax_with_cross_entropy` будет возвращает и значение ошибки, и градиент по входным параметрам. Мы проверим корректность реализации с помощью `check_gradient`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradients are not different at (0,). Analytic: 0.57612, Numeric: 0.57612\n",
      "Gradients are not different at (1,). Analytic: -0.78806, Numeric: -0.78806\n",
      "Gradients are not different at (2,). Analytic: 0.21194, Numeric: 0.21194\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement combined function or softmax and cross entropy and produces gradient\n",
    "loss, grad = linear_classifer.softmax_with_cross_entropy(np.array([1, 0, 0]), 1)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, 1), np.array([1, 0, 0], np.float))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве метода тренировки мы будем использовать стохастический градиентный спуск (stochastic gradient descent или SGD), который работает с батчами сэмплов. \n",
    "\n",
    "Поэтому все наши фукнции будут получать не один пример, а батч, то есть входом будет не вектор из `num_classes` оценок, а матрица размерности `batch_size, num_classes`. Индекс примера в батче всегда будет первым измерением.\n",
    "\n",
    "Следующий шаг - переписать наши функции так, чтобы они поддерживали батчи.\n",
    "\n",
    "Финальное значение функции ошибки должно остаться числом, и оно равно среднему значению ошибки среди всех примеров в батче."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradients are not different at (0, 0). Analytic: 0.20603, Numeric: 0.20603\n",
      "Gradients are not different at (0, 1). Analytic: 0.56005, Numeric: 0.56005\n",
      "Gradients are not different at (0, 2). Analytic: -0.97212, Numeric: -0.97212\n",
      "Gradients are not different at (0, 3). Analytic: 0.20603, Numeric: 0.20603\n",
      "Gradient check passed!\n",
      "Gradients are not different at (0, 0). Analytic: 0.68145, Numeric: 0.68145\n",
      "Gradients are not different at (0, 1). Analytic: 0.03393, Numeric: 0.03393\n",
      "Gradients are not different at (0, 2). Analytic: 0.03393, Numeric: 0.03393\n",
      "Gradients are not different at (0, 3). Analytic: -0.74931, Numeric: -0.74931\n",
      "Gradients are not different at (1, 0). Analytic: 0.10923, Numeric: 0.10923\n",
      "Gradients are not different at (1, 1). Analytic: 0.29692, Numeric: 0.29692\n",
      "Gradients are not different at (1, 2). Analytic: 0.29692, Numeric: 0.29692\n",
      "Gradients are not different at (1, 3). Analytic: -0.70308, Numeric: -0.70308\n",
      "Gradients are not different at (2, 0). Analytic: 0.15216, Numeric: 0.15216\n",
      "Gradients are not different at (2, 1). Analytic: 0.41362, Numeric: 0.41362\n",
      "Gradients are not different at (2, 2). Analytic: -0.97941, Numeric: -0.97941\n",
      "Gradients are not different at (2, 3). Analytic: 0.41362, Numeric: 0.41362\n",
      "Gradient check passed!\n"
     ]
    }
   ],
   "source": [
    "# TODO Extend combined function so it can receive a 2d array with batch of samples\n",
    "np.random.seed(42)\n",
    "# Test batch_size = 1\n",
    "num_classes = 4\n",
    "batch_size = 1\n",
    "predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(np.float)\n",
    "target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(np.int)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, target_index), predictions)\n",
    "\n",
    "# Test batch_size = 3\n",
    "num_classes = 4\n",
    "batch_size = 3\n",
    "predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(np.float)\n",
    "target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(np.int)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, target_index), predictions)\n",
    "\n",
    "# Make sure maximum subtraction for numberic stability is done separately for every sample in the batch\n",
    "probs = linear_classifer.softmax(np.array([[20,0,0], [1000, 0, 0]]))\n",
    "assert np.all(np.isclose(probs[:, 0], 1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.eye(3)[[2, 1, 1, 0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Наконец, реализуем сам линейный классификатор!\n",
    "\n",
    "softmax и cross-entropy получают на вход оценки, которые выдает линейный классификатор.\n",
    "\n",
    "Он делает это очень просто: для каждого класса есть набор весов, на которые надо умножить пиксели картинки и сложить. Получившееся число и является оценкой класса, идущей на вход softmax.\n",
    "\n",
    "Таким образом, линейный классификатор можно представить как умножение вектора с пикселями на матрицу W размера `num_features, num_classes`. Такой подход легко расширяется на случай батча векторов с пикселями X размера `batch_size, num_features`:\n",
    "\n",
    "`predictions = X * W`, где `*` - матричное умножение.\n",
    "\n",
    "Реализуйте функцию подсчета линейного классификатора и градиентов по весам `linear_softmax` в файле `linear_classifer.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradients are not different at (0, 0). Analytic: -0.88080, Numeric: -0.88080\n",
      "Gradients are not different at (0, 1). Analytic: 0.88080, Numeric: 0.88080\n",
      "Gradients are not different at (1, 0). Analytic: -0.83337, Numeric: -0.83337\n",
      "Gradients are not different at (1, 1). Analytic: 0.83337, Numeric: 0.83337\n",
      "Gradients are not different at (2, 0). Analytic: 0.92822, Numeric: 0.92822\n",
      "Gradients are not different at (2, 1). Analytic: -0.92822, Numeric: -0.92822\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement linear_softmax function that uses softmax with cross-entropy for linear classifier\n",
    "batch_size = 2\n",
    "num_classes = 2\n",
    "num_features = 3\n",
    "np.random.seed(42)\n",
    "W = np.random.randint(-1, 3, size=(num_features, num_classes)).astype(np.float)\n",
    "X = np.random.randint(-1, 3, size=(batch_size, num_features)).astype(np.float)\n",
    "target_index = np.ones(batch_size, dtype=np.int)\n",
    "\n",
    "loss, dW = linear_classifer.linear_softmax(X, W, target_index)\n",
    "check_gradient(lambda w: linear_classifer.linear_softmax(X, w, target_index), W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### И теперь регуляризация\n",
    "\n",
    "Мы будем использовать L2 regularization для весов как часть общей функции ошибки.\n",
    "\n",
    "Напомним, L2 regularization определяется как\n",
    "\n",
    "l2_reg_loss = regularization_strength * sum<sub>ij</sub> W[i, j]<sup>2</sup>\n",
    "\n",
    "Реализуйте функцию для его вычисления и вычисления соотвествующих градиентов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradients are not different at (0, 0). Analytic: 0.02000, Numeric: 0.02000\n",
      "Gradients are not different at (0, 1). Analytic: 0.04000, Numeric: 0.04000\n",
      "Gradients are not different at (1, 0). Analytic: -0.02000, Numeric: -0.02000\n",
      "Gradients are not different at (1, 1). Analytic: 0.02000, Numeric: 0.02000\n",
      "Gradients are not different at (2, 0). Analytic: 0.02000, Numeric: 0.02000\n",
      "Gradients are not different at (2, 1). Analytic: 0.04000, Numeric: 0.04000\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement l2_regularization function that implements loss for L2 regularization\n",
    "linear_classifer.l2_regularization(W, 0.01)\n",
    "check_gradient(lambda w: linear_classifer.l2_regularization(w, 0.01), W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Тренировка!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Градиенты в порядке, реализуем процесс тренировки!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss: 25455.182912\n",
      "Epoch 1, loss: 26928.310630\n",
      "Epoch 2, loss: 26611.885157\n",
      "Epoch 3, loss: 26995.425987\n",
      "Epoch 4, loss: 27096.880776\n",
      "Epoch 5, loss: 26210.793847\n",
      "Epoch 6, loss: 26911.822848\n",
      "Epoch 7, loss: 25936.854204\n",
      "Epoch 8, loss: 24215.358505\n",
      "Epoch 9, loss: 26642.289629\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement LinearSoftmaxClassifier.fit function\n",
    "classifier = linear_classifer.LinearSoftmaxClassifier()\n",
    "loss_history = classifier.fit(train_X, train_y, epochs=10, learning_rate=1e-3, batch_size=300, reg=1e1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fb12ae51b90>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# let's look at the loss history!\n",
    "plt.plot(loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.15\n",
      "Epoch 0, loss: 25994.650562\n",
      "Epoch 1, loss: 26920.096406\n",
      "Epoch 2, loss: 25213.456542\n",
      "Epoch 3, loss: 28049.569628\n",
      "Epoch 4, loss: 26689.940869\n",
      "Epoch 5, loss: 25386.348590\n",
      "Epoch 6, loss: 25039.112087\n",
      "Epoch 7, loss: 27265.390094\n",
      "Epoch 8, loss: 27256.802332\n",
      "Epoch 9, loss: 28062.113944\n",
      "Epoch 10, loss: 26052.925354\n",
      "Epoch 11, loss: 26209.660907\n",
      "Epoch 12, loss: 27484.413573\n",
      "Epoch 13, loss: 27446.307671\n",
      "Epoch 14, loss: 25695.665638\n",
      "Epoch 15, loss: 26972.530221\n",
      "Epoch 16, loss: 25808.123091\n",
      "Epoch 17, loss: 25296.899747\n",
      "Epoch 18, loss: 27734.807150\n",
      "Epoch 19, loss: 27202.949737\n",
      "Epoch 20, loss: 25885.015066\n",
      "Epoch 21, loss: 26735.523605\n",
      "Epoch 22, loss: 26820.567161\n",
      "Epoch 23, loss: 25807.412469\n",
      "Epoch 24, loss: 26162.520955\n",
      "Epoch 25, loss: 26714.836530\n",
      "Epoch 26, loss: 27056.775451\n",
      "Epoch 27, loss: 25838.665197\n",
      "Epoch 28, loss: 27364.895207\n",
      "Epoch 29, loss: 25736.090354\n",
      "Epoch 30, loss: 27222.549582\n",
      "Epoch 31, loss: 26052.654146\n",
      "Epoch 32, loss: 26390.551944\n",
      "Epoch 33, loss: 28193.099629\n",
      "Epoch 34, loss: 25491.865036\n",
      "Epoch 35, loss: 27020.827638\n",
      "Epoch 36, loss: 26977.541775\n",
      "Epoch 37, loss: 26964.163076\n",
      "Epoch 38, loss: 27282.343140\n",
      "Epoch 39, loss: 26369.870117\n",
      "Epoch 40, loss: 25530.419421\n",
      "Epoch 41, loss: 27060.582040\n",
      "Epoch 42, loss: 26776.341415\n",
      "Epoch 43, loss: 25614.341673\n",
      "Epoch 44, loss: 27358.130328\n",
      "Epoch 45, loss: 25605.823761\n",
      "Epoch 46, loss: 27810.900971\n",
      "Epoch 47, loss: 26493.566082\n",
      "Epoch 48, loss: 26388.393620\n",
      "Epoch 49, loss: 25520.294039\n",
      "Epoch 50, loss: 27064.804272\n",
      "Epoch 51, loss: 26968.221314\n",
      "Epoch 52, loss: 26954.209908\n",
      "Epoch 53, loss: 24802.844362\n",
      "Epoch 54, loss: 26525.732444\n",
      "Epoch 55, loss: 28438.182437\n",
      "Epoch 56, loss: 26451.868339\n",
      "Epoch 57, loss: 26486.399173\n",
      "Epoch 58, loss: 26336.782827\n",
      "Epoch 59, loss: 27071.340968\n",
      "Epoch 60, loss: 26172.600321\n",
      "Epoch 61, loss: 26981.620996\n",
      "Epoch 62, loss: 26127.787349\n",
      "Epoch 63, loss: 27188.068511\n",
      "Epoch 64, loss: 27012.378910\n",
      "Epoch 65, loss: 28477.654860\n",
      "Epoch 66, loss: 26764.645559\n",
      "Epoch 67, loss: 27648.616371\n",
      "Epoch 68, loss: 26687.730673\n",
      "Epoch 69, loss: 27000.891124\n",
      "Epoch 70, loss: 28418.918113\n",
      "Epoch 71, loss: 25861.563560\n",
      "Epoch 72, loss: 27630.810500\n",
      "Epoch 73, loss: 26628.952418\n",
      "Epoch 74, loss: 25846.855812\n",
      "Epoch 75, loss: 27567.532691\n",
      "Epoch 76, loss: 25647.270618\n",
      "Epoch 77, loss: 28231.170722\n",
      "Epoch 78, loss: 25472.211083\n",
      "Epoch 79, loss: 26639.484144\n",
      "Epoch 80, loss: 27040.179769\n",
      "Epoch 81, loss: 27023.111153\n",
      "Epoch 82, loss: 26397.094174\n",
      "Epoch 83, loss: 26878.474041\n",
      "Epoch 84, loss: 26307.892750\n",
      "Epoch 85, loss: 26031.384616\n",
      "Epoch 86, loss: 27118.470799\n",
      "Epoch 87, loss: 26557.776621\n",
      "Epoch 88, loss: 25649.050311\n",
      "Epoch 89, loss: 28264.967182\n",
      "Epoch 90, loss: 26047.917391\n",
      "Epoch 91, loss: 27198.897003\n",
      "Epoch 92, loss: 26508.822110\n",
      "Epoch 93, loss: 26476.710301\n",
      "Epoch 94, loss: 26580.695294\n",
      "Epoch 95, loss: 27316.583802\n",
      "Epoch 96, loss: 27275.455629\n",
      "Epoch 97, loss: 28034.267089\n",
      "Epoch 98, loss: 25086.757943\n",
      "Epoch 99, loss: 27637.477183\n",
      "Accuracy after training for 100 epochs:  0.146\n"
     ]
    }
   ],
   "source": [
    "# Let's check how it performs on validation set\n",
    "pred = classifier.predict(val_X)\n",
    "accuracy = multiclass_accuracy(pred, val_y)\n",
    "print(\"Accuracy: \", accuracy)\n",
    "\n",
    "# Now, let's train more and see if it performs better\n",
    "loss_history = classifier.fit(train_X, train_y, epochs=100, learning_rate=1e-3, batch_size=300, reg=1e1)\n",
    "pred = classifier.predict(val_X)\n",
    "accuracy = multiclass_accuracy(pred, val_y)\n",
    "print(\"Accuracy after training for 100 epochs: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Как и раньше, используем кросс-валидацию для подбора гиперпараметтов.\n",
    "\n",
    "В этот раз, чтобы тренировка занимала разумное время, мы будем использовать только одно разделение на тренировочные (training) и проверочные (validation) данные.\n",
    "\n",
    "Теперь нам нужно подобрать не один, а два гиперпараметра! Не ограничивайте себя изначальными значениями в коде.  \n",
    "Добейтесь точности более чем **20%** на проверочных данных (validation data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss: 25759.592558\n",
      "Epoch 1, loss: 25460.946816\n",
      "Epoch 2, loss: 25151.515980\n",
      "Epoch 3, loss: 25739.526796\n",
      "Epoch 4, loss: 25623.892221\n",
      "Epoch 5, loss: 24492.844342\n",
      "Epoch 6, loss: 24865.140192\n",
      "Epoch 7, loss: 24484.315318\n",
      "Epoch 8, loss: 25725.197654\n",
      "Epoch 9, loss: 23592.384122\n",
      "Epoch 10, loss: 24061.580241\n",
      "Epoch 11, loss: 26129.167136\n",
      "Epoch 12, loss: 23797.954559\n",
      "Epoch 13, loss: 23698.613272\n",
      "Epoch 14, loss: 24952.172215\n",
      "Epoch 15, loss: 24722.113038\n",
      "Epoch 16, loss: 23558.893476\n",
      "Epoch 17, loss: 23934.593422\n",
      "Epoch 18, loss: 24645.247477\n",
      "Epoch 19, loss: 24968.534430\n",
      "Epoch 20, loss: 23108.452481\n",
      "Epoch 21, loss: 23279.041648\n",
      "Epoch 22, loss: 24038.211386\n",
      "Epoch 23, loss: 24041.039260\n",
      "Epoch 24, loss: 24339.125064\n",
      "Epoch 25, loss: 24752.907949\n",
      "Epoch 26, loss: 23670.110499\n",
      "Epoch 27, loss: 24364.829429\n",
      "Epoch 28, loss: 24665.140888\n",
      "Epoch 29, loss: 24334.846817\n",
      "Epoch 30, loss: 24086.028118\n",
      "Epoch 31, loss: 23117.325172\n",
      "Epoch 32, loss: 24218.457961\n",
      "Epoch 33, loss: 23122.599286\n",
      "Epoch 34, loss: 23754.669108\n",
      "Epoch 35, loss: 23861.991387\n",
      "Epoch 36, loss: 24259.116497\n",
      "Epoch 37, loss: 23567.098392\n",
      "Epoch 38, loss: 24497.221617\n",
      "Epoch 39, loss: 24346.689337\n",
      "Epoch 40, loss: 23272.153697\n",
      "Epoch 41, loss: 23828.587491\n",
      "Epoch 42, loss: 23955.513989\n",
      "Epoch 43, loss: 23271.407068\n",
      "Epoch 44, loss: 23797.635817\n",
      "Epoch 45, loss: 23436.620851\n",
      "Epoch 46, loss: 23810.138555\n",
      "Epoch 47, loss: 23562.291550\n",
      "Epoch 48, loss: 24503.162553\n",
      "Epoch 49, loss: 23540.983022\n",
      "Epoch 50, loss: 22671.272811\n",
      "Epoch 51, loss: 23364.521167\n",
      "Epoch 52, loss: 23110.950653\n",
      "Epoch 53, loss: 24268.286158\n",
      "Epoch 54, loss: 23001.078302\n",
      "Epoch 55, loss: 24398.584890\n",
      "Epoch 56, loss: 23377.321802\n",
      "Epoch 57, loss: 23494.349912\n",
      "Epoch 58, loss: 23233.217108\n",
      "Epoch 59, loss: 23455.651735\n",
      "Epoch 60, loss: 22143.896738\n",
      "Epoch 61, loss: 24088.656964\n",
      "Epoch 62, loss: 24013.892074\n",
      "Epoch 63, loss: 22551.754216\n",
      "Epoch 64, loss: 22879.207937\n",
      "Epoch 65, loss: 22946.187210\n",
      "Epoch 66, loss: 22713.097536\n",
      "Epoch 67, loss: 23231.819091\n",
      "Epoch 68, loss: 24241.221542\n",
      "Epoch 69, loss: 22367.526000\n",
      "Epoch 70, loss: 24028.368354\n",
      "Epoch 71, loss: 23833.014239\n",
      "Epoch 72, loss: 22079.091181\n",
      "Epoch 73, loss: 23717.546322\n",
      "Epoch 74, loss: 21858.703172\n",
      "Epoch 75, loss: 24321.826286\n",
      "Epoch 76, loss: 22980.944814\n",
      "Epoch 77, loss: 22340.115613\n",
      "Epoch 78, loss: 23007.771131\n",
      "Epoch 79, loss: 23524.826221\n",
      "Epoch 80, loss: 23123.914571\n",
      "Epoch 81, loss: 23305.432855\n",
      "Epoch 82, loss: 23640.191713\n",
      "Epoch 83, loss: 21943.167138\n",
      "Epoch 84, loss: 24397.011430\n",
      "Epoch 85, loss: 21340.448643\n",
      "Epoch 86, loss: 23408.104107\n",
      "Epoch 87, loss: 23002.101284\n",
      "Epoch 88, loss: 23122.723451\n",
      "Epoch 89, loss: 22121.621393\n",
      "Epoch 90, loss: 22641.418403\n",
      "Epoch 91, loss: 23578.326760\n",
      "Epoch 92, loss: 23956.040809\n",
      "Epoch 93, loss: 21808.390698\n",
      "Epoch 94, loss: 24122.535312\n",
      "Epoch 95, loss: 23642.368899\n",
      "Epoch 96, loss: 24329.691937\n",
      "Epoch 97, loss: 21301.541789\n",
      "Epoch 98, loss: 23541.969343\n",
      "Epoch 99, loss: 23833.593286\n",
      "Epoch 100, loss: 23466.222615\n",
      "Epoch 101, loss: 22299.771762\n",
      "Epoch 102, loss: 22315.347594\n",
      "Epoch 103, loss: 24053.444567\n",
      "Epoch 104, loss: 22997.408240\n",
      "Epoch 105, loss: 22906.887668\n",
      "Epoch 106, loss: 22343.120852\n",
      "Epoch 107, loss: 23201.429166\n",
      "Epoch 108, loss: 23431.979918\n",
      "Epoch 109, loss: 22795.900524\n",
      "Epoch 110, loss: 23124.426323\n",
      "Epoch 111, loss: 23355.829185\n",
      "Epoch 112, loss: 22841.851947\n",
      "Epoch 113, loss: 22656.845523\n",
      "Epoch 114, loss: 23211.519855\n",
      "Epoch 115, loss: 22500.151859\n",
      "Epoch 116, loss: 22937.337908\n",
      "Epoch 117, loss: 22028.447885\n",
      "Epoch 118, loss: 23420.786380\n",
      "Epoch 119, loss: 23370.247661\n",
      "Epoch 120, loss: 23123.201292\n",
      "Epoch 121, loss: 22169.206952\n",
      "Epoch 122, loss: 23707.655510\n",
      "Epoch 123, loss: 22540.937932\n",
      "Epoch 124, loss: 22405.383703\n",
      "Epoch 125, loss: 21911.637095\n",
      "Epoch 126, loss: 22544.659462\n",
      "Epoch 127, loss: 23482.139865\n",
      "Epoch 128, loss: 23437.898197\n",
      "Epoch 129, loss: 22025.904871\n",
      "Epoch 130, loss: 23550.313039\n",
      "Epoch 131, loss: 22060.378360\n",
      "Epoch 132, loss: 24278.358535\n",
      "Epoch 133, loss: 22618.011296\n",
      "Epoch 134, loss: 21582.559342\n",
      "Epoch 135, loss: 23751.877609\n",
      "Epoch 136, loss: 22759.166307\n",
      "Epoch 137, loss: 22059.361985\n",
      "Epoch 138, loss: 21362.682463\n",
      "Epoch 139, loss: 22075.672829\n",
      "Epoch 140, loss: 23620.805592\n",
      "Epoch 141, loss: 21852.267880\n",
      "Epoch 142, loss: 23458.661998\n",
      "Epoch 143, loss: 21486.522327\n",
      "Epoch 144, loss: 23070.602149\n",
      "Epoch 145, loss: 22409.462250\n",
      "Epoch 146, loss: 22764.829255\n",
      "Epoch 147, loss: 24792.198328\n",
      "Epoch 148, loss: 22284.462949\n",
      "Epoch 149, loss: 21776.215043\n",
      "Epoch 150, loss: 23092.124388\n",
      "Epoch 151, loss: 22768.607092\n",
      "Epoch 152, loss: 22129.768627\n",
      "Epoch 153, loss: 23101.524487\n",
      "Epoch 154, loss: 21851.076334\n",
      "Epoch 155, loss: 23112.780088\n",
      "Epoch 156, loss: 21380.543865\n",
      "Epoch 157, loss: 22284.567447\n",
      "Epoch 158, loss: 23327.153621\n",
      "Epoch 159, loss: 21796.850430\n",
      "Epoch 160, loss: 21853.720329\n",
      "Epoch 161, loss: 23321.428999\n",
      "Epoch 162, loss: 22584.371553\n",
      "Epoch 163, loss: 21147.262989\n",
      "Epoch 164, loss: 22240.168958\n",
      "Epoch 165, loss: 22962.472294\n",
      "Epoch 166, loss: 22146.523870\n",
      "Epoch 167, loss: 22514.380323\n",
      "Epoch 168, loss: 22415.201290\n",
      "Epoch 169, loss: 22999.765177\n",
      "Epoch 170, loss: 22038.965360\n",
      "Epoch 171, loss: 22862.465639\n",
      "Epoch 172, loss: 21692.188884\n",
      "Epoch 173, loss: 23616.673355\n",
      "Epoch 174, loss: 22404.505638\n",
      "Epoch 175, loss: 22550.719120\n",
      "Epoch 176, loss: 21301.055190\n",
      "Epoch 177, loss: 22370.606585\n",
      "Epoch 178, loss: 22220.772160\n",
      "Epoch 179, loss: 22872.275221\n",
      "Epoch 180, loss: 23202.379829\n",
      "Epoch 181, loss: 22150.877128\n",
      "Epoch 182, loss: 22080.300561\n",
      "Epoch 183, loss: 21487.675202\n",
      "Epoch 184, loss: 21995.895475\n",
      "Epoch 185, loss: 21522.605982\n",
      "Epoch 186, loss: 23289.960331\n",
      "Epoch 187, loss: 21269.338948\n",
      "Epoch 188, loss: 23703.638015\n",
      "Epoch 189, loss: 21828.580520\n",
      "Epoch 190, loss: 21308.116310\n",
      "Epoch 191, loss: 21488.819148\n",
      "Epoch 192, loss: 22480.956269\n",
      "Epoch 193, loss: 23049.070427\n",
      "Epoch 194, loss: 22570.273525\n",
      "Epoch 195, loss: 22726.955122\n",
      "Epoch 196, loss: 22439.241606\n",
      "Epoch 197, loss: 21678.319587\n",
      "Epoch 198, loss: 21435.622531\n",
      "Epoch 199, loss: 22661.261613\n",
      "0.001 0.0001 0.187\n",
      "Epoch 0, loss: 25461.515065\n",
      "Epoch 1, loss: 26584.857078\n",
      "Epoch 2, loss: 25826.549321\n",
      "Epoch 3, loss: 26218.576884\n",
      "Epoch 4, loss: 26595.993973\n",
      "Epoch 5, loss: 23740.460647\n",
      "Epoch 6, loss: 25240.034029\n",
      "Epoch 7, loss: 24229.306433\n",
      "Epoch 8, loss: 25126.515093\n",
      "Epoch 9, loss: 25721.594504\n",
      "Epoch 10, loss: 24530.571213\n",
      "Epoch 11, loss: 23922.127050\n",
      "Epoch 12, loss: 24913.597792\n",
      "Epoch 13, loss: 23888.322290\n",
      "Epoch 14, loss: 24671.915608\n",
      "Epoch 15, loss: 24309.222811\n",
      "Epoch 16, loss: 25001.785951\n",
      "Epoch 17, loss: 24241.513146\n",
      "Epoch 18, loss: 25653.162434\n",
      "Epoch 19, loss: 24148.001181\n",
      "Epoch 20, loss: 23946.475316\n",
      "Epoch 21, loss: 25126.706720\n",
      "Epoch 22, loss: 23765.146125\n",
      "Epoch 23, loss: 24198.145039\n",
      "Epoch 24, loss: 23423.971208\n",
      "Epoch 25, loss: 24279.631251\n",
      "Epoch 26, loss: 23006.084066\n",
      "Epoch 27, loss: 23492.871335\n",
      "Epoch 28, loss: 25946.961738\n",
      "Epoch 29, loss: 24128.470096\n",
      "Epoch 30, loss: 25127.840078\n",
      "Epoch 31, loss: 23703.617885\n",
      "Epoch 32, loss: 24447.348960\n",
      "Epoch 33, loss: 23473.013222\n",
      "Epoch 34, loss: 23600.101083\n",
      "Epoch 35, loss: 23336.791773\n",
      "Epoch 36, loss: 22777.742999\n",
      "Epoch 37, loss: 21719.069073\n",
      "Epoch 38, loss: 24877.467489\n",
      "Epoch 39, loss: 23781.120273\n",
      "Epoch 40, loss: 24499.745388\n",
      "Epoch 41, loss: 23994.115301\n",
      "Epoch 42, loss: 22877.241138\n",
      "Epoch 43, loss: 24283.858727\n",
      "Epoch 44, loss: 24237.860589\n",
      "Epoch 45, loss: 23547.760543\n",
      "Epoch 46, loss: 23843.522976\n",
      "Epoch 47, loss: 24469.892589\n",
      "Epoch 48, loss: 22357.518648\n",
      "Epoch 49, loss: 23526.448330\n",
      "Epoch 50, loss: 23289.235787\n",
      "Epoch 51, loss: 22819.601053\n",
      "Epoch 52, loss: 24226.141508\n",
      "Epoch 53, loss: 24497.421410\n",
      "Epoch 54, loss: 22477.979853\n",
      "Epoch 55, loss: 24005.851236\n",
      "Epoch 56, loss: 23877.483230\n",
      "Epoch 57, loss: 24488.672797\n",
      "Epoch 58, loss: 23627.656995\n",
      "Epoch 59, loss: 23175.784901\n",
      "Epoch 60, loss: 22672.670254\n",
      "Epoch 61, loss: 23166.092032\n",
      "Epoch 62, loss: 24104.808934\n",
      "Epoch 63, loss: 23183.398619\n",
      "Epoch 64, loss: 24488.389594\n",
      "Epoch 65, loss: 22156.912677\n",
      "Epoch 66, loss: 22321.758458\n",
      "Epoch 67, loss: 24695.714181\n",
      "Epoch 68, loss: 22149.353511\n",
      "Epoch 69, loss: 23969.245919\n",
      "Epoch 70, loss: 23228.615730\n",
      "Epoch 71, loss: 23618.181190\n",
      "Epoch 72, loss: 21660.888493\n",
      "Epoch 73, loss: 22707.503578\n",
      "Epoch 74, loss: 24288.464568\n",
      "Epoch 75, loss: 23279.296711\n",
      "Epoch 76, loss: 23648.869608\n",
      "Epoch 77, loss: 22888.956172\n",
      "Epoch 78, loss: 22839.554022\n",
      "Epoch 79, loss: 23335.634799\n",
      "Epoch 80, loss: 22553.929906\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81, loss: 23053.876223\n",
      "Epoch 82, loss: 23118.220127\n",
      "Epoch 83, loss: 22655.573412\n",
      "Epoch 84, loss: 24240.324176\n",
      "Epoch 85, loss: 23646.835150\n",
      "Epoch 86, loss: 21541.942275\n",
      "Epoch 87, loss: 24746.440206\n",
      "Epoch 88, loss: 22981.770711\n",
      "Epoch 89, loss: 21651.638458\n",
      "Epoch 90, loss: 23445.242805\n",
      "Epoch 91, loss: 21981.157493\n",
      "Epoch 92, loss: 24679.581035\n",
      "Epoch 93, loss: 22829.122291\n",
      "Epoch 94, loss: 22986.733934\n",
      "Epoch 95, loss: 24407.743759\n",
      "Epoch 96, loss: 22141.831393\n",
      "Epoch 97, loss: 22153.829933\n",
      "Epoch 98, loss: 24163.670127\n",
      "Epoch 99, loss: 22899.109047\n",
      "Epoch 100, loss: 23814.504837\n",
      "Epoch 101, loss: 22931.513681\n",
      "Epoch 102, loss: 21371.880859\n",
      "Epoch 103, loss: 22660.708277\n",
      "Epoch 104, loss: 22160.489656\n",
      "Epoch 105, loss: 22466.172849\n",
      "Epoch 106, loss: 22548.639147\n",
      "Epoch 107, loss: 22656.009650\n",
      "Epoch 108, loss: 22870.969768\n",
      "Epoch 109, loss: 22685.621847\n",
      "Epoch 110, loss: 22887.902765\n",
      "Epoch 111, loss: 23952.995662\n",
      "Epoch 112, loss: 21817.728167\n",
      "Epoch 113, loss: 23341.750652\n",
      "Epoch 114, loss: 20968.023149\n",
      "Epoch 115, loss: 23501.442216\n",
      "Epoch 116, loss: 23283.216729\n",
      "Epoch 117, loss: 23013.781297\n",
      "Epoch 118, loss: 22518.269996\n",
      "Epoch 119, loss: 22975.876856\n",
      "Epoch 120, loss: 22615.807517\n",
      "Epoch 121, loss: 23320.017203\n",
      "Epoch 122, loss: 22312.463536\n",
      "Epoch 123, loss: 22570.765239\n",
      "Epoch 124, loss: 21929.797995\n",
      "Epoch 125, loss: 22323.057487\n",
      "Epoch 126, loss: 21758.764336\n",
      "Epoch 127, loss: 22657.668840\n",
      "Epoch 128, loss: 23142.054814\n",
      "Epoch 129, loss: 22121.720060\n",
      "Epoch 130, loss: 21214.096378\n",
      "Epoch 131, loss: 23228.802623\n",
      "Epoch 132, loss: 23085.848672\n",
      "Epoch 133, loss: 22680.331451\n",
      "Epoch 134, loss: 22854.169567\n",
      "Epoch 135, loss: 21690.114644\n",
      "Epoch 136, loss: 23660.812821\n",
      "Epoch 137, loss: 21536.028773\n",
      "Epoch 138, loss: 23865.244259\n",
      "Epoch 139, loss: 24205.008902\n",
      "Epoch 140, loss: 21898.406805\n",
      "Epoch 141, loss: 22781.603775\n",
      "Epoch 142, loss: 21241.772731\n",
      "Epoch 143, loss: 23527.627185\n",
      "Epoch 144, loss: 22701.329120\n",
      "Epoch 145, loss: 22154.966519\n",
      "Epoch 146, loss: 22990.552005\n",
      "Epoch 147, loss: 20538.487182\n",
      "Epoch 148, loss: 23100.584235\n",
      "Epoch 149, loss: 21894.470775\n",
      "Epoch 150, loss: 21738.783417\n",
      "Epoch 151, loss: 22976.749975\n",
      "Epoch 152, loss: 23575.538101\n",
      "Epoch 153, loss: 22705.303313\n",
      "Epoch 154, loss: 21333.281035\n",
      "Epoch 155, loss: 23297.019397\n",
      "Epoch 156, loss: 22947.755111\n",
      "Epoch 157, loss: 22267.225918\n",
      "Epoch 158, loss: 22421.216490\n",
      "Epoch 159, loss: 23339.496404\n",
      "Epoch 160, loss: 21525.516420\n",
      "Epoch 161, loss: 22378.909737\n",
      "Epoch 162, loss: 23007.449873\n",
      "Epoch 163, loss: 22071.720441\n",
      "Epoch 164, loss: 22564.914865\n",
      "Epoch 165, loss: 23510.835692\n",
      "Epoch 166, loss: 22048.265537\n",
      "Epoch 167, loss: 23440.550807\n",
      "Epoch 168, loss: 21473.698120\n",
      "Epoch 169, loss: 21904.128626\n",
      "Epoch 170, loss: 23252.363345\n",
      "Epoch 171, loss: 21215.090794\n",
      "Epoch 172, loss: 21576.231678\n",
      "Epoch 173, loss: 23576.317208\n",
      "Epoch 174, loss: 22940.361294\n",
      "Epoch 175, loss: 22173.505695\n",
      "Epoch 176, loss: 23608.237513\n",
      "Epoch 177, loss: 22136.154941\n",
      "Epoch 178, loss: 21849.995285\n",
      "Epoch 179, loss: 21786.853604\n",
      "Epoch 180, loss: 21500.600893\n",
      "Epoch 181, loss: 22671.720806\n",
      "Epoch 182, loss: 20833.004733\n",
      "Epoch 183, loss: 21091.679035\n",
      "Epoch 184, loss: 21953.126286\n",
      "Epoch 185, loss: 23500.642178\n",
      "Epoch 186, loss: 21042.602931\n",
      "Epoch 187, loss: 22833.959841\n",
      "Epoch 188, loss: 22464.550371\n",
      "Epoch 189, loss: 22002.609232\n",
      "Epoch 190, loss: 19832.169774\n",
      "Epoch 191, loss: 23050.243417\n",
      "Epoch 192, loss: 22585.204295\n",
      "Epoch 193, loss: 22943.656164\n",
      "Epoch 194, loss: 22299.169893\n",
      "Epoch 195, loss: 22693.802124\n",
      "Epoch 196, loss: 22085.783172\n",
      "Epoch 197, loss: 22601.424334\n",
      "Epoch 198, loss: 22344.172448\n",
      "Epoch 199, loss: 21758.125044\n",
      "0.001 1e-05 0.196\n",
      "Epoch 0, loss: 25980.196442\n",
      "Epoch 1, loss: 25349.962750\n",
      "Epoch 2, loss: 24862.114238\n",
      "Epoch 3, loss: 25986.330987\n",
      "Epoch 4, loss: 24776.285261\n",
      "Epoch 5, loss: 26357.458844\n",
      "Epoch 6, loss: 25935.489241\n",
      "Epoch 7, loss: 24975.519630\n",
      "Epoch 8, loss: 24653.281656\n",
      "Epoch 9, loss: 24770.224746\n",
      "Epoch 10, loss: 25559.717162\n",
      "Epoch 11, loss: 23562.908949\n",
      "Epoch 12, loss: 24865.837187\n",
      "Epoch 13, loss: 24572.421174\n",
      "Epoch 14, loss: 23655.060675\n",
      "Epoch 15, loss: 24276.756977\n",
      "Epoch 16, loss: 22492.720070\n",
      "Epoch 17, loss: 26080.478474\n",
      "Epoch 18, loss: 23652.761179\n",
      "Epoch 19, loss: 22313.253986\n",
      "Epoch 20, loss: 25725.107396\n",
      "Epoch 21, loss: 24246.929446\n",
      "Epoch 22, loss: 23739.808192\n",
      "Epoch 23, loss: 26130.308653\n",
      "Epoch 24, loss: 24064.273214\n",
      "Epoch 25, loss: 24160.944649\n",
      "Epoch 26, loss: 26003.591575\n",
      "Epoch 27, loss: 23844.326349\n",
      "Epoch 28, loss: 24408.718510\n",
      "Epoch 29, loss: 22857.205594\n",
      "Epoch 30, loss: 24359.741819\n",
      "Epoch 31, loss: 23214.013600\n",
      "Epoch 32, loss: 23453.335450\n",
      "Epoch 33, loss: 24866.388983\n",
      "Epoch 34, loss: 23345.765076\n",
      "Epoch 35, loss: 23629.371116\n",
      "Epoch 36, loss: 24474.082564\n",
      "Epoch 37, loss: 24168.006970\n",
      "Epoch 38, loss: 23321.295268\n",
      "Epoch 39, loss: 24566.204065\n",
      "Epoch 40, loss: 23488.408618\n",
      "Epoch 41, loss: 23129.834558\n",
      "Epoch 42, loss: 23685.159794\n",
      "Epoch 43, loss: 25232.505423\n",
      "Epoch 44, loss: 23270.905512\n",
      "Epoch 45, loss: 24941.202371\n",
      "Epoch 46, loss: 22536.517948\n",
      "Epoch 47, loss: 24621.809858\n",
      "Epoch 48, loss: 24493.664365\n",
      "Epoch 49, loss: 21664.655505\n",
      "Epoch 50, loss: 23902.591054\n",
      "Epoch 51, loss: 23192.183956\n",
      "Epoch 52, loss: 24147.631544\n",
      "Epoch 53, loss: 24093.295232\n",
      "Epoch 54, loss: 22565.250202\n",
      "Epoch 55, loss: 23841.286281\n",
      "Epoch 56, loss: 24299.696354\n",
      "Epoch 57, loss: 24092.708550\n",
      "Epoch 58, loss: 24635.954904\n",
      "Epoch 59, loss: 24638.380998\n",
      "Epoch 60, loss: 22212.127814\n",
      "Epoch 61, loss: 23370.949867\n",
      "Epoch 62, loss: 22855.527265\n",
      "Epoch 63, loss: 23466.001042\n",
      "Epoch 64, loss: 23869.164651\n",
      "Epoch 65, loss: 23605.325322\n",
      "Epoch 66, loss: 23176.133414\n",
      "Epoch 67, loss: 23133.080317\n",
      "Epoch 68, loss: 23142.131848\n",
      "Epoch 69, loss: 24466.959227\n",
      "Epoch 70, loss: 22984.177498\n",
      "Epoch 71, loss: 21759.959082\n",
      "Epoch 72, loss: 23406.131997\n",
      "Epoch 73, loss: 22634.818796\n",
      "Epoch 74, loss: 23566.340074\n",
      "Epoch 75, loss: 23075.355190\n",
      "Epoch 76, loss: 24020.861950\n",
      "Epoch 77, loss: 23846.327330\n",
      "Epoch 78, loss: 22169.537929\n",
      "Epoch 79, loss: 22881.721351\n",
      "Epoch 80, loss: 23432.059245\n",
      "Epoch 81, loss: 22753.644908\n",
      "Epoch 82, loss: 23909.272905\n",
      "Epoch 83, loss: 23612.407354\n",
      "Epoch 84, loss: 22632.245654\n",
      "Epoch 85, loss: 21769.400278\n",
      "Epoch 86, loss: 22243.849214\n",
      "Epoch 87, loss: 22650.926158\n",
      "Epoch 88, loss: 24150.342294\n",
      "Epoch 89, loss: 23195.839885\n",
      "Epoch 90, loss: 23223.222734\n",
      "Epoch 91, loss: 22825.730074\n",
      "Epoch 92, loss: 22245.334143\n",
      "Epoch 93, loss: 21442.025063\n",
      "Epoch 94, loss: 24077.267670\n",
      "Epoch 95, loss: 22485.123807\n",
      "Epoch 96, loss: 23341.949178\n",
      "Epoch 97, loss: 22843.268594\n",
      "Epoch 98, loss: 23526.246870\n",
      "Epoch 99, loss: 22136.324835\n",
      "Epoch 100, loss: 22546.324424\n",
      "Epoch 101, loss: 23483.951936\n",
      "Epoch 102, loss: 23274.261911\n",
      "Epoch 103, loss: 23239.909836\n",
      "Epoch 104, loss: 22951.424123\n",
      "Epoch 105, loss: 23350.508464\n",
      "Epoch 106, loss: 22519.056549\n",
      "Epoch 107, loss: 22896.692445\n",
      "Epoch 108, loss: 22211.075736\n",
      "Epoch 109, loss: 22928.883468\n",
      "Epoch 110, loss: 21931.918936\n",
      "Epoch 111, loss: 22644.523105\n",
      "Epoch 112, loss: 21652.755159\n",
      "Epoch 113, loss: 22514.334109\n",
      "Epoch 114, loss: 23640.108993\n",
      "Epoch 115, loss: 23117.866997\n",
      "Epoch 116, loss: 22899.486956\n",
      "Epoch 117, loss: 23641.469190\n",
      "Epoch 118, loss: 22139.396853\n",
      "Epoch 119, loss: 23578.891234\n",
      "Epoch 120, loss: 22292.174955\n",
      "Epoch 121, loss: 22022.524996\n",
      "Epoch 122, loss: 21803.284172\n",
      "Epoch 123, loss: 22456.169121\n",
      "Epoch 124, loss: 24040.305945\n",
      "Epoch 125, loss: 22392.108749\n",
      "Epoch 126, loss: 21931.491330\n",
      "Epoch 127, loss: 21757.680409\n",
      "Epoch 128, loss: 21408.408583\n",
      "Epoch 129, loss: 24434.525462\n",
      "Epoch 130, loss: 22515.798511\n",
      "Epoch 131, loss: 23050.323615\n",
      "Epoch 132, loss: 22340.424955\n",
      "Epoch 133, loss: 22580.055593\n",
      "Epoch 134, loss: 22353.346516\n",
      "Epoch 135, loss: 22529.545042\n",
      "Epoch 136, loss: 23647.027355\n",
      "Epoch 137, loss: 21457.181809\n",
      "Epoch 138, loss: 22713.296907\n",
      "Epoch 139, loss: 22963.613656\n",
      "Epoch 140, loss: 22277.022311\n",
      "Epoch 141, loss: 23141.099383\n",
      "Epoch 142, loss: 22073.609677\n",
      "Epoch 143, loss: 23230.976981\n",
      "Epoch 144, loss: 21763.973489\n",
      "Epoch 145, loss: 22365.087375\n",
      "Epoch 146, loss: 22481.451885\n",
      "Epoch 147, loss: 21729.281932\n",
      "Epoch 148, loss: 22312.517604\n",
      "Epoch 149, loss: 22129.147344\n",
      "Epoch 150, loss: 22977.834048\n",
      "Epoch 151, loss: 23311.920488\n",
      "Epoch 152, loss: 22231.942144\n",
      "Epoch 153, loss: 21849.718514\n",
      "Epoch 154, loss: 21993.224211\n",
      "Epoch 155, loss: 22474.742118\n",
      "Epoch 156, loss: 23605.137167\n",
      "Epoch 157, loss: 22496.589443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 158, loss: 24104.405693\n",
      "Epoch 159, loss: 21619.734906\n",
      "Epoch 160, loss: 22499.256231\n",
      "Epoch 161, loss: 22086.868009\n",
      "Epoch 162, loss: 23190.440741\n",
      "Epoch 163, loss: 21912.058791\n",
      "Epoch 164, loss: 20902.965618\n",
      "Epoch 165, loss: 22145.624591\n",
      "Epoch 166, loss: 22468.280550\n",
      "Epoch 167, loss: 20719.769414\n",
      "Epoch 168, loss: 23469.007567\n",
      "Epoch 169, loss: 22380.947533\n",
      "Epoch 170, loss: 22110.142148\n",
      "Epoch 171, loss: 22873.422774\n",
      "Epoch 172, loss: 22518.132779\n",
      "Epoch 173, loss: 23099.964579\n",
      "Epoch 174, loss: 20890.331637\n",
      "Epoch 175, loss: 22897.406328\n",
      "Epoch 176, loss: 22215.706633\n",
      "Epoch 177, loss: 22042.967517\n",
      "Epoch 178, loss: 22872.129260\n",
      "Epoch 179, loss: 23006.434369\n",
      "Epoch 180, loss: 24316.986264\n",
      "Epoch 181, loss: 21688.282503\n",
      "Epoch 182, loss: 22212.710814\n",
      "Epoch 183, loss: 22171.073199\n",
      "Epoch 184, loss: 21834.374208\n",
      "Epoch 185, loss: 22176.094428\n",
      "Epoch 186, loss: 23349.114027\n",
      "Epoch 187, loss: 20983.117681\n",
      "Epoch 188, loss: 23569.097111\n",
      "Epoch 189, loss: 23194.141231\n",
      "Epoch 190, loss: 21353.493209\n",
      "Epoch 191, loss: 20385.290974\n",
      "Epoch 192, loss: 22483.409393\n",
      "Epoch 193, loss: 21806.815987\n",
      "Epoch 194, loss: 22248.120675\n",
      "Epoch 195, loss: 22406.847228\n",
      "Epoch 196, loss: 22298.364113\n",
      "Epoch 197, loss: 22982.832380\n",
      "Epoch 198, loss: 21276.225236\n",
      "Epoch 199, loss: 24298.914879\n",
      "0.001 1e-06 0.204\n",
      "Epoch 0, loss: 20645.004856\n",
      "Epoch 1, loss: 20450.317094\n",
      "Epoch 2, loss: 20295.852344\n",
      "Epoch 3, loss: 20155.793417\n",
      "Epoch 4, loss: 20048.208574\n",
      "Epoch 5, loss: 19950.996634\n",
      "Epoch 6, loss: 19861.099088\n",
      "Epoch 7, loss: 19794.952912\n",
      "Epoch 8, loss: 19740.489225\n",
      "Epoch 9, loss: 19679.471883\n",
      "Epoch 10, loss: 19638.835217\n",
      "Epoch 11, loss: 19594.457752\n",
      "Epoch 12, loss: 19558.079324\n",
      "Epoch 13, loss: 19526.520838\n",
      "Epoch 14, loss: 19497.435868\n",
      "Epoch 15, loss: 19468.440180\n",
      "Epoch 16, loss: 19443.446215\n",
      "Epoch 17, loss: 19421.259732\n",
      "Epoch 18, loss: 19393.851926\n",
      "Epoch 19, loss: 19369.977890\n",
      "Epoch 20, loss: 19356.615458\n",
      "Epoch 21, loss: 19334.853026\n",
      "Epoch 22, loss: 19318.543697\n",
      "Epoch 23, loss: 19301.870414\n",
      "Epoch 24, loss: 19289.883759\n",
      "Epoch 25, loss: 19271.646443\n",
      "Epoch 26, loss: 19250.770903\n",
      "Epoch 27, loss: 19244.195725\n",
      "Epoch 28, loss: 19231.975842\n",
      "Epoch 29, loss: 19218.941935\n",
      "Epoch 30, loss: 19198.948621\n",
      "Epoch 31, loss: 19192.770445\n",
      "Epoch 32, loss: 19178.843515\n",
      "Epoch 33, loss: 19170.701557\n",
      "Epoch 34, loss: 19159.909155\n",
      "Epoch 35, loss: 19141.869529\n",
      "Epoch 36, loss: 19138.239068\n",
      "Epoch 37, loss: 19137.040521\n",
      "Epoch 38, loss: 19121.955119\n",
      "Epoch 39, loss: 19113.488510\n",
      "Epoch 40, loss: 19098.802788\n",
      "Epoch 41, loss: 19094.863130\n",
      "Epoch 42, loss: 19088.223942\n",
      "Epoch 43, loss: 19077.050168\n",
      "Epoch 44, loss: 19060.030360\n",
      "Epoch 45, loss: 19060.637393\n",
      "Epoch 46, loss: 19052.958062\n",
      "Epoch 47, loss: 19042.003542\n",
      "Epoch 48, loss: 19039.569024\n",
      "Epoch 49, loss: 19031.918527\n",
      "Epoch 50, loss: 19022.136880\n",
      "Epoch 51, loss: 19011.525061\n",
      "Epoch 52, loss: 19012.877182\n",
      "Epoch 53, loss: 19007.481618\n",
      "Epoch 54, loss: 18994.099396\n",
      "Epoch 55, loss: 18991.404502\n",
      "Epoch 56, loss: 18986.550207\n",
      "Epoch 57, loss: 18981.619975\n",
      "Epoch 58, loss: 18971.310543\n",
      "Epoch 59, loss: 18966.792756\n",
      "Epoch 60, loss: 18963.158226\n",
      "Epoch 61, loss: 18951.121580\n",
      "Epoch 62, loss: 18949.406105\n",
      "Epoch 63, loss: 18934.703540\n",
      "Epoch 64, loss: 18933.517572\n",
      "Epoch 65, loss: 18928.040687\n",
      "Epoch 66, loss: 18924.572120\n",
      "Epoch 67, loss: 18919.122881\n",
      "Epoch 68, loss: 18910.974690\n",
      "Epoch 69, loss: 18912.610761\n",
      "Epoch 70, loss: 18905.433501\n",
      "Epoch 71, loss: 18895.008354\n",
      "Epoch 72, loss: 18894.257975\n",
      "Epoch 73, loss: 18884.722705\n",
      "Epoch 74, loss: 18882.506871\n",
      "Epoch 75, loss: 18880.015161\n",
      "Epoch 76, loss: 18876.137784\n",
      "Epoch 77, loss: 18870.919727\n",
      "Epoch 78, loss: 18869.658321\n",
      "Epoch 79, loss: 18860.224122\n",
      "Epoch 80, loss: 18849.597443\n",
      "Epoch 81, loss: 18849.718811\n",
      "Epoch 82, loss: 18846.921496\n",
      "Epoch 83, loss: 18836.747043\n",
      "Epoch 84, loss: 18841.702183\n",
      "Epoch 85, loss: 18834.206809\n",
      "Epoch 86, loss: 18830.436870\n",
      "Epoch 87, loss: 18824.387204\n",
      "Epoch 88, loss: 18824.192837\n",
      "Epoch 89, loss: 18819.706481\n",
      "Epoch 90, loss: 18813.910260\n",
      "Epoch 91, loss: 18810.881891\n",
      "Epoch 92, loss: 18808.134421\n",
      "Epoch 93, loss: 18802.951137\n",
      "Epoch 94, loss: 18792.143728\n",
      "Epoch 95, loss: 18799.930871\n",
      "Epoch 96, loss: 18789.936370\n",
      "Epoch 97, loss: 18785.723500\n",
      "Epoch 98, loss: 18784.880140\n",
      "Epoch 99, loss: 18778.589214\n",
      "Epoch 100, loss: 18779.179356\n",
      "Epoch 101, loss: 18774.191526\n",
      "Epoch 102, loss: 18764.435091\n",
      "Epoch 103, loss: 18771.086158\n",
      "Epoch 104, loss: 18761.063789\n",
      "Epoch 105, loss: 18754.804688\n",
      "Epoch 106, loss: 18756.300043\n",
      "Epoch 107, loss: 18753.082354\n",
      "Epoch 108, loss: 18745.319394\n",
      "Epoch 109, loss: 18745.652825\n",
      "Epoch 110, loss: 18741.058597\n",
      "Epoch 111, loss: 18741.501742\n",
      "Epoch 112, loss: 18734.174843\n",
      "Epoch 113, loss: 18734.247879\n",
      "Epoch 114, loss: 18733.073501\n",
      "Epoch 115, loss: 18726.873046\n",
      "Epoch 116, loss: 18724.300099\n",
      "Epoch 117, loss: 18724.248478\n",
      "Epoch 118, loss: 18712.793150\n",
      "Epoch 119, loss: 18706.345343\n",
      "Epoch 120, loss: 18709.101805\n",
      "Epoch 121, loss: 18709.344050\n",
      "Epoch 122, loss: 18703.500065\n",
      "Epoch 123, loss: 18701.143961\n",
      "Epoch 124, loss: 18695.334493\n",
      "Epoch 125, loss: 18700.232866\n",
      "Epoch 126, loss: 18689.543553\n",
      "Epoch 127, loss: 18687.898734\n",
      "Epoch 128, loss: 18686.456053\n",
      "Epoch 129, loss: 18683.574314\n",
      "Epoch 130, loss: 18680.249600\n",
      "Epoch 131, loss: 18677.569247\n",
      "Epoch 132, loss: 18677.348047\n",
      "Epoch 133, loss: 18676.893717\n",
      "Epoch 134, loss: 18675.003413\n",
      "Epoch 135, loss: 18665.985404\n",
      "Epoch 136, loss: 18662.808389\n",
      "Epoch 137, loss: 18660.659824\n",
      "Epoch 138, loss: 18658.665401\n",
      "Epoch 139, loss: 18656.800062\n",
      "Epoch 140, loss: 18650.687957\n",
      "Epoch 141, loss: 18650.040120\n",
      "Epoch 142, loss: 18647.086232\n",
      "Epoch 143, loss: 18643.283831\n",
      "Epoch 144, loss: 18645.543735\n",
      "Epoch 145, loss: 18642.114413\n",
      "Epoch 146, loss: 18637.248162\n",
      "Epoch 147, loss: 18633.643516\n",
      "Epoch 148, loss: 18632.437505\n",
      "Epoch 149, loss: 18631.849348\n",
      "Epoch 150, loss: 18630.458756\n",
      "Epoch 151, loss: 18624.360957\n",
      "Epoch 152, loss: 18628.075667\n",
      "Epoch 153, loss: 18622.450967\n",
      "Epoch 154, loss: 18618.782205\n",
      "Epoch 155, loss: 18608.730862\n",
      "Epoch 156, loss: 18619.690752\n",
      "Epoch 157, loss: 18610.542806\n",
      "Epoch 158, loss: 18611.910274\n",
      "Epoch 159, loss: 18612.386125\n",
      "Epoch 160, loss: 18604.911693\n",
      "Epoch 161, loss: 18597.896603\n",
      "Epoch 162, loss: 18598.367837\n",
      "Epoch 163, loss: 18590.995726\n",
      "Epoch 164, loss: 18594.815954\n",
      "Epoch 165, loss: 18587.862318\n",
      "Epoch 166, loss: 18596.920242\n",
      "Epoch 167, loss: 18587.882019\n",
      "Epoch 168, loss: 18590.331163\n",
      "Epoch 169, loss: 18581.107040\n",
      "Epoch 170, loss: 18579.576585\n",
      "Epoch 171, loss: 18579.927150\n",
      "Epoch 172, loss: 18578.672491\n",
      "Epoch 173, loss: 18577.969478\n",
      "Epoch 174, loss: 18572.936555\n",
      "Epoch 175, loss: 18566.369696\n",
      "Epoch 176, loss: 18568.298668\n",
      "Epoch 177, loss: 18565.955884\n",
      "Epoch 178, loss: 18563.284628\n",
      "Epoch 179, loss: 18563.253586\n",
      "Epoch 180, loss: 18562.093866\n",
      "Epoch 181, loss: 18556.496199\n",
      "Epoch 182, loss: 18549.647798\n",
      "Epoch 183, loss: 18557.996691\n",
      "Epoch 184, loss: 18548.103121\n",
      "Epoch 185, loss: 18542.434900\n",
      "Epoch 186, loss: 18548.225877\n",
      "Epoch 187, loss: 18544.369600\n",
      "Epoch 188, loss: 18538.211982\n",
      "Epoch 189, loss: 18538.451417\n",
      "Epoch 190, loss: 18538.655163\n",
      "Epoch 191, loss: 18531.684100\n",
      "Epoch 192, loss: 18533.230486\n",
      "Epoch 193, loss: 18527.070664\n",
      "Epoch 194, loss: 18529.760051\n",
      "Epoch 195, loss: 18524.513870\n",
      "Epoch 196, loss: 18526.536026\n",
      "Epoch 197, loss: 18518.146085\n",
      "Epoch 198, loss: 18518.276596\n",
      "Epoch 199, loss: 18518.336962\n",
      "0.0001 0.0001 0.247\n",
      "Epoch 0, loss: 20650.416608\n",
      "Epoch 1, loss: 20448.145943\n",
      "Epoch 2, loss: 20287.750035\n",
      "Epoch 3, loss: 20151.277240\n",
      "Epoch 4, loss: 20045.144571\n",
      "Epoch 5, loss: 19953.352107\n",
      "Epoch 6, loss: 19864.835106\n",
      "Epoch 7, loss: 19796.798439\n",
      "Epoch 8, loss: 19736.513695\n",
      "Epoch 9, loss: 19681.322967\n",
      "Epoch 10, loss: 19636.364201\n",
      "Epoch 11, loss: 19591.771327\n",
      "Epoch 12, loss: 19556.301053\n",
      "Epoch 13, loss: 19523.377181\n",
      "Epoch 14, loss: 19490.129548\n",
      "Epoch 15, loss: 19469.686238\n",
      "Epoch 16, loss: 19440.618697\n",
      "Epoch 17, loss: 19414.885759\n",
      "Epoch 18, loss: 19398.508006\n",
      "Epoch 19, loss: 19371.013108\n",
      "Epoch 20, loss: 19351.989838\n",
      "Epoch 21, loss: 19338.058270\n",
      "Epoch 22, loss: 19314.783373\n",
      "Epoch 23, loss: 19300.193995\n",
      "Epoch 24, loss: 19286.862990\n",
      "Epoch 25, loss: 19270.526907\n",
      "Epoch 26, loss: 19255.918611\n",
      "Epoch 27, loss: 19240.856368\n",
      "Epoch 28, loss: 19225.050725\n",
      "Epoch 29, loss: 19219.272408\n",
      "Epoch 30, loss: 19200.177515\n",
      "Epoch 31, loss: 19193.318050\n",
      "Epoch 32, loss: 19178.765094\n",
      "Epoch 33, loss: 19170.898952\n",
      "Epoch 34, loss: 19157.832216\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35, loss: 19145.973308\n",
      "Epoch 36, loss: 19137.342690\n",
      "Epoch 37, loss: 19131.520695\n",
      "Epoch 38, loss: 19118.560758\n",
      "Epoch 39, loss: 19104.337862\n",
      "Epoch 40, loss: 19101.840275\n",
      "Epoch 41, loss: 19089.283360\n",
      "Epoch 42, loss: 19081.642290\n",
      "Epoch 43, loss: 19079.270659\n",
      "Epoch 44, loss: 19060.913334\n",
      "Epoch 45, loss: 19047.501851\n",
      "Epoch 46, loss: 19053.614548\n",
      "Epoch 47, loss: 19043.142942\n",
      "Epoch 48, loss: 19036.536610\n",
      "Epoch 49, loss: 19025.882709\n",
      "Epoch 50, loss: 19021.693876\n",
      "Epoch 51, loss: 19014.625430\n",
      "Epoch 52, loss: 19001.563310\n",
      "Epoch 53, loss: 19000.525963\n",
      "Epoch 54, loss: 18989.520030\n",
      "Epoch 55, loss: 18990.485188\n",
      "Epoch 56, loss: 18978.214053\n",
      "Epoch 57, loss: 18978.603949\n",
      "Epoch 58, loss: 18966.232996\n",
      "Epoch 59, loss: 18967.095228\n",
      "Epoch 60, loss: 18956.064283\n",
      "Epoch 61, loss: 18953.660502\n",
      "Epoch 62, loss: 18950.385949\n",
      "Epoch 63, loss: 18940.497491\n",
      "Epoch 64, loss: 18935.127625\n",
      "Epoch 65, loss: 18925.146242\n",
      "Epoch 66, loss: 18923.077658\n",
      "Epoch 67, loss: 18919.045811\n",
      "Epoch 68, loss: 18913.799487\n",
      "Epoch 69, loss: 18907.537192\n",
      "Epoch 70, loss: 18902.702531\n",
      "Epoch 71, loss: 18894.833300\n",
      "Epoch 72, loss: 18894.047167\n",
      "Epoch 73, loss: 18888.830598\n",
      "Epoch 74, loss: 18885.611533\n",
      "Epoch 75, loss: 18877.557469\n",
      "Epoch 76, loss: 18875.368754\n",
      "Epoch 77, loss: 18867.537998\n",
      "Epoch 78, loss: 18858.050539\n",
      "Epoch 79, loss: 18859.209344\n",
      "Epoch 80, loss: 18855.678912\n",
      "Epoch 81, loss: 18850.059779\n",
      "Epoch 82, loss: 18846.917783\n",
      "Epoch 83, loss: 18844.997875\n",
      "Epoch 84, loss: 18836.588162\n",
      "Epoch 85, loss: 18831.812527\n",
      "Epoch 86, loss: 18829.728592\n",
      "Epoch 87, loss: 18829.747027\n",
      "Epoch 88, loss: 18823.008665\n",
      "Epoch 89, loss: 18820.145970\n",
      "Epoch 90, loss: 18815.267657\n",
      "Epoch 91, loss: 18814.702559\n",
      "Epoch 92, loss: 18806.438726\n",
      "Epoch 93, loss: 18806.157781\n",
      "Epoch 94, loss: 18793.287689\n",
      "Epoch 95, loss: 18793.525659\n",
      "Epoch 96, loss: 18794.002026\n",
      "Epoch 97, loss: 18787.935227\n",
      "Epoch 98, loss: 18782.511942\n",
      "Epoch 99, loss: 18778.965191\n",
      "Epoch 100, loss: 18778.493742\n",
      "Epoch 101, loss: 18774.719545\n",
      "Epoch 102, loss: 18769.419140\n",
      "Epoch 103, loss: 18764.013444\n",
      "Epoch 104, loss: 18764.154322\n",
      "Epoch 105, loss: 18756.279688\n",
      "Epoch 106, loss: 18755.416544\n",
      "Epoch 107, loss: 18749.095238\n",
      "Epoch 108, loss: 18747.695023\n",
      "Epoch 109, loss: 18750.934746\n",
      "Epoch 110, loss: 18740.066644\n",
      "Epoch 111, loss: 18742.461740\n",
      "Epoch 112, loss: 18731.512601\n",
      "Epoch 113, loss: 18730.107132\n",
      "Epoch 114, loss: 18733.728179\n",
      "Epoch 115, loss: 18727.797618\n",
      "Epoch 116, loss: 18719.957710\n",
      "Epoch 117, loss: 18709.704336\n",
      "Epoch 118, loss: 18716.240406\n",
      "Epoch 119, loss: 18716.241010\n",
      "Epoch 120, loss: 18707.708890\n",
      "Epoch 121, loss: 18712.279676\n",
      "Epoch 122, loss: 18701.405593\n",
      "Epoch 123, loss: 18696.963572\n",
      "Epoch 124, loss: 18699.062898\n",
      "Epoch 125, loss: 18699.324587\n",
      "Epoch 126, loss: 18691.323756\n",
      "Epoch 127, loss: 18690.516864\n",
      "Epoch 128, loss: 18687.741760\n",
      "Epoch 129, loss: 18686.192280\n",
      "Epoch 130, loss: 18682.666519\n",
      "Epoch 131, loss: 18675.500848\n",
      "Epoch 132, loss: 18677.057464\n",
      "Epoch 133, loss: 18674.832905\n",
      "Epoch 134, loss: 18672.105478\n",
      "Epoch 135, loss: 18671.550294\n",
      "Epoch 136, loss: 18668.610121\n",
      "Epoch 137, loss: 18667.422090\n",
      "Epoch 138, loss: 18654.881251\n",
      "Epoch 139, loss: 18658.265997\n",
      "Epoch 140, loss: 18655.558219\n",
      "Epoch 141, loss: 18647.732406\n",
      "Epoch 142, loss: 18648.086590\n",
      "Epoch 143, loss: 18648.718764\n",
      "Epoch 144, loss: 18642.686829\n",
      "Epoch 145, loss: 18638.509980\n",
      "Epoch 146, loss: 18633.826382\n",
      "Epoch 147, loss: 18634.107692\n",
      "Epoch 148, loss: 18629.738248\n",
      "Epoch 149, loss: 18629.681882\n",
      "Epoch 150, loss: 18625.754106\n",
      "Epoch 151, loss: 18627.661937\n",
      "Epoch 152, loss: 18626.223954\n",
      "Epoch 153, loss: 18612.680014\n",
      "Epoch 154, loss: 18611.155777\n",
      "Epoch 155, loss: 18622.333930\n",
      "Epoch 156, loss: 18616.768895\n",
      "Epoch 157, loss: 18609.641708\n",
      "Epoch 158, loss: 18612.066244\n",
      "Epoch 159, loss: 18604.673248\n",
      "Epoch 160, loss: 18606.398303\n",
      "Epoch 161, loss: 18602.075052\n",
      "Epoch 162, loss: 18593.459001\n",
      "Epoch 163, loss: 18596.798909\n",
      "Epoch 164, loss: 18590.315178\n",
      "Epoch 165, loss: 18589.560712\n",
      "Epoch 166, loss: 18588.194665\n",
      "Epoch 167, loss: 18586.645493\n",
      "Epoch 168, loss: 18580.831026\n",
      "Epoch 169, loss: 18583.576785\n",
      "Epoch 170, loss: 18578.103455\n",
      "Epoch 171, loss: 18580.714815\n",
      "Epoch 172, loss: 18581.387936\n",
      "Epoch 173, loss: 18573.760786\n",
      "Epoch 174, loss: 18566.655026\n",
      "Epoch 175, loss: 18565.600039\n",
      "Epoch 176, loss: 18561.992941\n",
      "Epoch 177, loss: 18561.158589\n",
      "Epoch 178, loss: 18560.185932\n",
      "Epoch 179, loss: 18555.221982\n",
      "Epoch 180, loss: 18561.942566\n",
      "Epoch 181, loss: 18555.259385\n",
      "Epoch 182, loss: 18555.541193\n",
      "Epoch 183, loss: 18555.399481\n",
      "Epoch 184, loss: 18553.054642\n",
      "Epoch 185, loss: 18553.636166\n",
      "Epoch 186, loss: 18542.543058\n",
      "Epoch 187, loss: 18547.000249\n",
      "Epoch 188, loss: 18546.662088\n",
      "Epoch 189, loss: 18540.452040\n",
      "Epoch 190, loss: 18536.589920\n",
      "Epoch 191, loss: 18535.777461\n",
      "Epoch 192, loss: 18535.659418\n",
      "Epoch 193, loss: 18531.854585\n",
      "Epoch 194, loss: 18531.701427\n",
      "Epoch 195, loss: 18523.233179\n",
      "Epoch 196, loss: 18523.325928\n",
      "Epoch 197, loss: 18520.945875\n",
      "Epoch 198, loss: 18520.087046\n",
      "Epoch 199, loss: 18520.414662\n",
      "0.0001 1e-05 0.244\n",
      "Epoch 0, loss: 20645.107777\n",
      "Epoch 1, loss: 20445.152632\n",
      "Epoch 2, loss: 20294.288560\n",
      "Epoch 3, loss: 20156.622829\n",
      "Epoch 4, loss: 20042.501589\n",
      "Epoch 5, loss: 19950.576271\n",
      "Epoch 6, loss: 19866.479382\n",
      "Epoch 7, loss: 19797.535738\n",
      "Epoch 8, loss: 19738.237281\n",
      "Epoch 9, loss: 19678.152514\n",
      "Epoch 10, loss: 19640.784351\n",
      "Epoch 11, loss: 19595.284615\n",
      "Epoch 12, loss: 19560.177257\n",
      "Epoch 13, loss: 19519.674682\n",
      "Epoch 14, loss: 19496.269473\n",
      "Epoch 15, loss: 19460.613882\n",
      "Epoch 16, loss: 19442.804360\n",
      "Epoch 17, loss: 19417.239609\n",
      "Epoch 18, loss: 19392.597486\n",
      "Epoch 19, loss: 19368.311714\n",
      "Epoch 20, loss: 19353.687696\n",
      "Epoch 21, loss: 19334.251944\n",
      "Epoch 22, loss: 19315.173590\n",
      "Epoch 23, loss: 19304.366813\n",
      "Epoch 24, loss: 19281.988838\n",
      "Epoch 25, loss: 19271.344600\n",
      "Epoch 26, loss: 19252.650976\n",
      "Epoch 27, loss: 19244.497996\n",
      "Epoch 28, loss: 19230.093314\n",
      "Epoch 29, loss: 19217.765304\n",
      "Epoch 30, loss: 19208.383873\n",
      "Epoch 31, loss: 19190.050926\n",
      "Epoch 32, loss: 19180.915538\n",
      "Epoch 33, loss: 19172.326723\n",
      "Epoch 34, loss: 19158.021911\n",
      "Epoch 35, loss: 19148.651253\n",
      "Epoch 36, loss: 19136.514881\n",
      "Epoch 37, loss: 19131.985400\n",
      "Epoch 38, loss: 19121.246909\n",
      "Epoch 39, loss: 19108.514149\n",
      "Epoch 40, loss: 19098.546786\n",
      "Epoch 41, loss: 19092.198305\n",
      "Epoch 42, loss: 19078.895786\n",
      "Epoch 43, loss: 19072.850821\n",
      "Epoch 44, loss: 19069.407674\n",
      "Epoch 45, loss: 19054.991465\n",
      "Epoch 46, loss: 19052.468180\n",
      "Epoch 47, loss: 19051.618858\n",
      "Epoch 48, loss: 19036.713115\n",
      "Epoch 49, loss: 19025.998046\n",
      "Epoch 50, loss: 19014.032951\n",
      "Epoch 51, loss: 19014.423773\n",
      "Epoch 52, loss: 19007.202981\n",
      "Epoch 53, loss: 19007.004115\n",
      "Epoch 54, loss: 18989.757301\n",
      "Epoch 55, loss: 18990.029673\n",
      "Epoch 56, loss: 18978.317497\n",
      "Epoch 57, loss: 18979.713900\n",
      "Epoch 58, loss: 18962.658243\n",
      "Epoch 59, loss: 18965.911279\n",
      "Epoch 60, loss: 18957.839777\n",
      "Epoch 61, loss: 18955.519494\n",
      "Epoch 62, loss: 18945.318857\n",
      "Epoch 63, loss: 18941.318915\n",
      "Epoch 64, loss: 18926.495807\n",
      "Epoch 65, loss: 18933.610770\n",
      "Epoch 66, loss: 18926.285367\n",
      "Epoch 67, loss: 18913.760514\n",
      "Epoch 68, loss: 18912.614556\n",
      "Epoch 69, loss: 18905.305720\n",
      "Epoch 70, loss: 18907.661276\n",
      "Epoch 71, loss: 18893.658590\n",
      "Epoch 72, loss: 18891.193630\n",
      "Epoch 73, loss: 18890.734041\n",
      "Epoch 74, loss: 18881.178686\n",
      "Epoch 75, loss: 18875.109846\n",
      "Epoch 76, loss: 18870.769868\n",
      "Epoch 77, loss: 18864.466745\n",
      "Epoch 78, loss: 18865.215258\n",
      "Epoch 79, loss: 18860.205280\n",
      "Epoch 80, loss: 18860.023338\n",
      "Epoch 81, loss: 18854.609425\n",
      "Epoch 82, loss: 18848.262890\n",
      "Epoch 83, loss: 18841.875239\n",
      "Epoch 84, loss: 18843.522084\n",
      "Epoch 85, loss: 18835.615578\n",
      "Epoch 86, loss: 18826.438794\n",
      "Epoch 87, loss: 18828.553378\n",
      "Epoch 88, loss: 18818.454177\n",
      "Epoch 89, loss: 18816.403621\n",
      "Epoch 90, loss: 18820.237512\n",
      "Epoch 91, loss: 18810.467183\n",
      "Epoch 92, loss: 18810.523417\n",
      "Epoch 93, loss: 18801.029763\n",
      "Epoch 94, loss: 18803.118311\n",
      "Epoch 95, loss: 18793.896828\n",
      "Epoch 96, loss: 18795.109693\n",
      "Epoch 97, loss: 18792.381507\n",
      "Epoch 98, loss: 18783.991084\n",
      "Epoch 99, loss: 18778.171142\n",
      "Epoch 100, loss: 18775.031346\n",
      "Epoch 101, loss: 18771.066907\n",
      "Epoch 102, loss: 18771.685104\n",
      "Epoch 103, loss: 18767.867444\n",
      "Epoch 104, loss: 18769.504813\n",
      "Epoch 105, loss: 18762.265068\n",
      "Epoch 106, loss: 18754.580747\n",
      "Epoch 107, loss: 18755.363688\n",
      "Epoch 108, loss: 18749.800444\n",
      "Epoch 109, loss: 18746.360720\n",
      "Epoch 110, loss: 18742.235799\n",
      "Epoch 111, loss: 18736.122411\n",
      "Epoch 112, loss: 18741.025523\n",
      "Epoch 113, loss: 18735.333335\n",
      "Epoch 114, loss: 18728.466331\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 115, loss: 18727.596613\n",
      "Epoch 116, loss: 18722.702876\n",
      "Epoch 117, loss: 18719.336569\n",
      "Epoch 118, loss: 18719.060131\n",
      "Epoch 119, loss: 18709.590425\n",
      "Epoch 120, loss: 18707.059072\n",
      "Epoch 121, loss: 18703.911200\n",
      "Epoch 122, loss: 18706.042808\n",
      "Epoch 123, loss: 18696.082882\n",
      "Epoch 124, loss: 18697.197155\n",
      "Epoch 125, loss: 18695.055933\n",
      "Epoch 126, loss: 18692.919510\n",
      "Epoch 127, loss: 18682.749643\n",
      "Epoch 128, loss: 18688.106962\n",
      "Epoch 129, loss: 18686.075456\n",
      "Epoch 130, loss: 18674.707844\n",
      "Epoch 131, loss: 18680.766043\n",
      "Epoch 132, loss: 18674.458585\n",
      "Epoch 133, loss: 18672.767326\n",
      "Epoch 134, loss: 18675.325542\n",
      "Epoch 135, loss: 18663.778297\n",
      "Epoch 136, loss: 18660.621702\n",
      "Epoch 137, loss: 18665.340995\n",
      "Epoch 138, loss: 18656.758504\n",
      "Epoch 139, loss: 18652.691895\n",
      "Epoch 140, loss: 18652.116330\n",
      "Epoch 141, loss: 18652.870424\n",
      "Epoch 142, loss: 18648.602884\n",
      "Epoch 143, loss: 18646.348149\n",
      "Epoch 144, loss: 18639.737159\n",
      "Epoch 145, loss: 18640.491606\n",
      "Epoch 146, loss: 18633.701457\n",
      "Epoch 147, loss: 18635.315437\n",
      "Epoch 148, loss: 18636.367114\n",
      "Epoch 149, loss: 18633.931827\n",
      "Epoch 150, loss: 18626.100445\n",
      "Epoch 151, loss: 18627.781529\n",
      "Epoch 152, loss: 18622.467665\n",
      "Epoch 153, loss: 18616.685362\n",
      "Epoch 154, loss: 18620.035284\n",
      "Epoch 155, loss: 18617.731963\n",
      "Epoch 156, loss: 18610.157358\n",
      "Epoch 157, loss: 18608.973322\n",
      "Epoch 158, loss: 18604.227898\n",
      "Epoch 159, loss: 18601.353213\n",
      "Epoch 160, loss: 18605.889397\n",
      "Epoch 161, loss: 18600.664306\n",
      "Epoch 162, loss: 18597.500218\n",
      "Epoch 163, loss: 18595.612538\n",
      "Epoch 164, loss: 18595.778713\n",
      "Epoch 165, loss: 18592.159375\n",
      "Epoch 166, loss: 18584.669613\n",
      "Epoch 167, loss: 18593.342845\n",
      "Epoch 168, loss: 18582.183971\n",
      "Epoch 169, loss: 18577.448858\n",
      "Epoch 170, loss: 18581.658038\n",
      "Epoch 171, loss: 18575.686215\n",
      "Epoch 172, loss: 18569.361005\n",
      "Epoch 173, loss: 18575.972571\n",
      "Epoch 174, loss: 18574.195910\n",
      "Epoch 175, loss: 18567.193394\n",
      "Epoch 176, loss: 18564.438557\n",
      "Epoch 177, loss: 18563.925272\n",
      "Epoch 178, loss: 18560.849390\n",
      "Epoch 179, loss: 18557.146004\n",
      "Epoch 180, loss: 18554.620001\n",
      "Epoch 181, loss: 18558.942538\n",
      "Epoch 182, loss: 18552.063354\n",
      "Epoch 183, loss: 18547.446203\n",
      "Epoch 184, loss: 18545.289210\n",
      "Epoch 185, loss: 18550.564487\n",
      "Epoch 186, loss: 18545.800494\n",
      "Epoch 187, loss: 18542.574276\n",
      "Epoch 188, loss: 18538.591555\n",
      "Epoch 189, loss: 18541.994224\n",
      "Epoch 190, loss: 18533.679423\n",
      "Epoch 191, loss: 18538.972183\n",
      "Epoch 192, loss: 18533.732701\n",
      "Epoch 193, loss: 18533.941235\n",
      "Epoch 194, loss: 18525.560487\n",
      "Epoch 195, loss: 18521.257261\n",
      "Epoch 196, loss: 18522.524349\n",
      "Epoch 197, loss: 18520.725599\n",
      "Epoch 198, loss: 18516.660846\n",
      "Epoch 199, loss: 18522.101808\n",
      "0.0001 1e-06 0.254\n",
      "Epoch 0, loss: 20714.873841\n",
      "Epoch 1, loss: 20689.020148\n",
      "Epoch 2, loss: 20665.057664\n",
      "Epoch 3, loss: 20641.596220\n",
      "Epoch 4, loss: 20619.266578\n",
      "Epoch 5, loss: 20597.397008\n",
      "Epoch 6, loss: 20576.206215\n",
      "Epoch 7, loss: 20555.127659\n",
      "Epoch 8, loss: 20534.387274\n",
      "Epoch 9, loss: 20515.536912\n",
      "Epoch 10, loss: 20495.244532\n",
      "Epoch 11, loss: 20476.383552\n",
      "Epoch 12, loss: 20458.037458\n",
      "Epoch 13, loss: 20439.406561\n",
      "Epoch 14, loss: 20421.473677\n",
      "Epoch 15, loss: 20404.832584\n",
      "Epoch 16, loss: 20386.703944\n",
      "Epoch 17, loss: 20370.565075\n",
      "Epoch 18, loss: 20353.451777\n",
      "Epoch 19, loss: 20337.848377\n",
      "Epoch 20, loss: 20321.425953\n",
      "Epoch 21, loss: 20305.517451\n",
      "Epoch 22, loss: 20290.806791\n",
      "Epoch 23, loss: 20275.351545\n",
      "Epoch 24, loss: 20260.182957\n",
      "Epoch 25, loss: 20246.009149\n",
      "Epoch 26, loss: 20232.141110\n",
      "Epoch 27, loss: 20217.834529\n",
      "Epoch 28, loss: 20204.617693\n",
      "Epoch 29, loss: 20190.903941\n",
      "Epoch 30, loss: 20177.701724\n",
      "Epoch 31, loss: 20164.406781\n",
      "Epoch 32, loss: 20152.430403\n",
      "Epoch 33, loss: 20139.251023\n",
      "Epoch 34, loss: 20127.012848\n",
      "Epoch 35, loss: 20114.694230\n",
      "Epoch 36, loss: 20102.640071\n",
      "Epoch 37, loss: 20091.596694\n",
      "Epoch 38, loss: 20079.220442\n",
      "Epoch 39, loss: 20068.040549\n",
      "Epoch 40, loss: 20056.974972\n",
      "Epoch 41, loss: 20045.744660\n",
      "Epoch 42, loss: 20035.253625\n",
      "Epoch 43, loss: 20023.941792\n",
      "Epoch 44, loss: 20014.042749\n",
      "Epoch 45, loss: 20004.148617\n",
      "Epoch 46, loss: 19993.947175\n",
      "Epoch 47, loss: 19983.828324\n",
      "Epoch 48, loss: 19973.618418\n",
      "Epoch 49, loss: 19964.422369\n",
      "Epoch 50, loss: 19954.790747\n",
      "Epoch 51, loss: 19945.402343\n",
      "Epoch 52, loss: 19936.361962\n",
      "Epoch 53, loss: 19927.058304\n",
      "Epoch 54, loss: 19918.684519\n",
      "Epoch 55, loss: 19909.537507\n",
      "Epoch 56, loss: 19900.733422\n",
      "Epoch 57, loss: 19892.363859\n",
      "Epoch 58, loss: 19883.928444\n",
      "Epoch 59, loss: 19875.962311\n",
      "Epoch 60, loss: 19867.973133\n",
      "Epoch 61, loss: 19859.944571\n",
      "Epoch 62, loss: 19852.262987\n",
      "Epoch 63, loss: 19844.509441\n",
      "Epoch 64, loss: 19837.020949\n",
      "Epoch 65, loss: 19829.380193\n",
      "Epoch 66, loss: 19821.551709\n",
      "Epoch 67, loss: 19814.971764\n",
      "Epoch 68, loss: 19807.267702\n",
      "Epoch 69, loss: 19800.463147\n",
      "Epoch 70, loss: 19793.304030\n",
      "Epoch 71, loss: 19786.219624\n",
      "Epoch 72, loss: 19780.006679\n",
      "Epoch 73, loss: 19772.676801\n",
      "Epoch 74, loss: 19766.553205\n",
      "Epoch 75, loss: 19759.678823\n",
      "Epoch 76, loss: 19754.195064\n",
      "Epoch 77, loss: 19747.580549\n",
      "Epoch 78, loss: 19740.928577\n",
      "Epoch 79, loss: 19735.364327\n",
      "Epoch 80, loss: 19728.629125\n",
      "Epoch 81, loss: 19723.106834\n",
      "Epoch 82, loss: 19717.045149\n",
      "Epoch 83, loss: 19710.914292\n",
      "Epoch 84, loss: 19705.700854\n",
      "Epoch 85, loss: 19699.762593\n",
      "Epoch 86, loss: 19694.136516\n",
      "Epoch 87, loss: 19689.267400\n",
      "Epoch 88, loss: 19683.815772\n",
      "Epoch 89, loss: 19677.636531\n",
      "Epoch 90, loss: 19672.815515\n",
      "Epoch 91, loss: 19667.313817\n",
      "Epoch 92, loss: 19663.029739\n",
      "Epoch 93, loss: 19657.837917\n",
      "Epoch 94, loss: 19652.822646\n",
      "Epoch 95, loss: 19647.586923\n",
      "Epoch 96, loss: 19642.914989\n",
      "Epoch 97, loss: 19637.752031\n",
      "Epoch 98, loss: 19633.187865\n",
      "Epoch 99, loss: 19628.433757\n",
      "Epoch 100, loss: 19624.212812\n",
      "Epoch 101, loss: 19618.963714\n",
      "Epoch 102, loss: 19614.783320\n",
      "Epoch 103, loss: 19610.422214\n",
      "Epoch 104, loss: 19605.728706\n",
      "Epoch 105, loss: 19601.228072\n",
      "Epoch 106, loss: 19596.932536\n",
      "Epoch 107, loss: 19592.540635\n",
      "Epoch 108, loss: 19588.581121\n",
      "Epoch 109, loss: 19584.424707\n",
      "Epoch 110, loss: 19580.276751\n",
      "Epoch 111, loss: 19576.633256\n",
      "Epoch 112, loss: 19571.919578\n",
      "Epoch 113, loss: 19567.967881\n",
      "Epoch 114, loss: 19564.324022\n",
      "Epoch 115, loss: 19560.453736\n",
      "Epoch 116, loss: 19556.722233\n",
      "Epoch 117, loss: 19552.187191\n",
      "Epoch 118, loss: 19548.985657\n",
      "Epoch 119, loss: 19545.063712\n",
      "Epoch 120, loss: 19541.432782\n",
      "Epoch 121, loss: 19537.775844\n",
      "Epoch 122, loss: 19534.128732\n",
      "Epoch 123, loss: 19530.260835\n",
      "Epoch 124, loss: 19526.824321\n",
      "Epoch 125, loss: 19523.051979\n",
      "Epoch 126, loss: 19520.171047\n",
      "Epoch 127, loss: 19516.343922\n",
      "Epoch 128, loss: 19513.091851\n",
      "Epoch 129, loss: 19510.062623\n",
      "Epoch 130, loss: 19506.573181\n",
      "Epoch 131, loss: 19503.149344\n",
      "Epoch 132, loss: 19499.465985\n",
      "Epoch 133, loss: 19496.783675\n",
      "Epoch 134, loss: 19493.189156\n",
      "Epoch 135, loss: 19489.769144\n",
      "Epoch 136, loss: 19487.386560\n",
      "Epoch 137, loss: 19483.797703\n",
      "Epoch 138, loss: 19480.540192\n",
      "Epoch 139, loss: 19477.455342\n",
      "Epoch 140, loss: 19474.219598\n",
      "Epoch 141, loss: 19471.955577\n",
      "Epoch 142, loss: 19468.508301\n",
      "Epoch 143, loss: 19466.109755\n",
      "Epoch 144, loss: 19462.891202\n",
      "Epoch 145, loss: 19460.100274\n",
      "Epoch 146, loss: 19457.031582\n",
      "Epoch 147, loss: 19454.071522\n",
      "Epoch 148, loss: 19451.282021\n",
      "Epoch 149, loss: 19448.155586\n",
      "Epoch 150, loss: 19445.568563\n",
      "Epoch 151, loss: 19443.053594\n",
      "Epoch 152, loss: 19439.715194\n",
      "Epoch 153, loss: 19437.345835\n",
      "Epoch 154, loss: 19434.690592\n",
      "Epoch 155, loss: 19432.138954\n",
      "Epoch 156, loss: 19429.423943\n",
      "Epoch 157, loss: 19427.089759\n",
      "Epoch 158, loss: 19423.967488\n",
      "Epoch 159, loss: 19421.841382\n",
      "Epoch 160, loss: 19419.126826\n",
      "Epoch 161, loss: 19416.959009\n",
      "Epoch 162, loss: 19414.165328\n",
      "Epoch 163, loss: 19411.032404\n",
      "Epoch 164, loss: 19408.866619\n",
      "Epoch 165, loss: 19406.739714\n",
      "Epoch 166, loss: 19403.850698\n",
      "Epoch 167, loss: 19401.345685\n",
      "Epoch 168, loss: 19399.013453\n",
      "Epoch 169, loss: 19397.235234\n",
      "Epoch 170, loss: 19394.336694\n",
      "Epoch 171, loss: 19392.515901\n",
      "Epoch 172, loss: 19389.487861\n",
      "Epoch 173, loss: 19387.671351\n",
      "Epoch 174, loss: 19385.224066\n",
      "Epoch 175, loss: 19382.482948\n",
      "Epoch 176, loss: 19380.501767\n",
      "Epoch 177, loss: 19378.418425\n",
      "Epoch 178, loss: 19376.158556\n",
      "Epoch 179, loss: 19373.598404\n",
      "Epoch 180, loss: 19371.944774\n",
      "Epoch 181, loss: 19369.618708\n",
      "Epoch 182, loss: 19366.595288\n",
      "Epoch 183, loss: 19365.153952\n",
      "Epoch 184, loss: 19362.813354\n",
      "Epoch 185, loss: 19360.228290\n",
      "Epoch 186, loss: 19358.566618\n",
      "Epoch 187, loss: 19356.347915\n",
      "Epoch 188, loss: 19354.396171\n",
      "Epoch 189, loss: 19351.877720\n",
      "Epoch 190, loss: 19350.393411\n",
      "Epoch 191, loss: 19348.018223\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 192, loss: 19346.087247\n",
      "Epoch 193, loss: 19343.970942\n",
      "Epoch 194, loss: 19341.915445\n",
      "Epoch 195, loss: 19340.084258\n",
      "Epoch 196, loss: 19338.065561\n",
      "Epoch 197, loss: 19335.906796\n",
      "Epoch 198, loss: 19333.977538\n",
      "Epoch 199, loss: 19331.859299\n",
      "1e-05 0.0001 0.231\n",
      "Epoch 0, loss: 20714.435938\n",
      "Epoch 1, loss: 20688.456211\n",
      "Epoch 2, loss: 20664.011852\n",
      "Epoch 3, loss: 20640.910620\n",
      "Epoch 4, loss: 20618.412587\n",
      "Epoch 5, loss: 20597.096391\n",
      "Epoch 6, loss: 20575.023958\n",
      "Epoch 7, loss: 20554.934627\n",
      "Epoch 8, loss: 20533.985073\n",
      "Epoch 9, loss: 20514.517249\n",
      "Epoch 10, loss: 20494.722583\n",
      "Epoch 11, loss: 20475.808699\n",
      "Epoch 12, loss: 20456.834494\n",
      "Epoch 13, loss: 20438.615456\n",
      "Epoch 14, loss: 20420.519398\n",
      "Epoch 15, loss: 20403.267497\n",
      "Epoch 16, loss: 20386.330767\n",
      "Epoch 17, loss: 20369.642620\n",
      "Epoch 18, loss: 20353.060763\n",
      "Epoch 19, loss: 20336.260840\n",
      "Epoch 20, loss: 20320.648682\n",
      "Epoch 21, loss: 20305.190231\n",
      "Epoch 22, loss: 20289.719387\n",
      "Epoch 23, loss: 20274.444956\n",
      "Epoch 24, loss: 20260.051997\n",
      "Epoch 25, loss: 20245.628375\n",
      "Epoch 26, loss: 20231.275248\n",
      "Epoch 27, loss: 20217.071367\n",
      "Epoch 28, loss: 20203.408905\n",
      "Epoch 29, loss: 20189.999923\n",
      "Epoch 30, loss: 20176.690674\n",
      "Epoch 31, loss: 20163.851038\n",
      "Epoch 32, loss: 20151.185909\n",
      "Epoch 33, loss: 20138.318228\n",
      "Epoch 34, loss: 20126.075556\n",
      "Epoch 35, loss: 20113.510212\n",
      "Epoch 36, loss: 20101.559641\n",
      "Epoch 37, loss: 20090.033061\n",
      "Epoch 38, loss: 20078.708985\n",
      "Epoch 39, loss: 20067.099696\n",
      "Epoch 40, loss: 20055.966652\n",
      "Epoch 41, loss: 20044.891990\n",
      "Epoch 42, loss: 20034.338524\n",
      "Epoch 43, loss: 20023.475492\n",
      "Epoch 44, loss: 20013.202251\n",
      "Epoch 45, loss: 20002.999179\n",
      "Epoch 46, loss: 19992.441749\n",
      "Epoch 47, loss: 19983.034483\n",
      "Epoch 48, loss: 19972.499198\n",
      "Epoch 49, loss: 19963.570984\n",
      "Epoch 50, loss: 19953.575313\n",
      "Epoch 51, loss: 19944.771917\n",
      "Epoch 52, loss: 19935.587191\n",
      "Epoch 53, loss: 19926.586135\n",
      "Epoch 54, loss: 19917.239124\n",
      "Epoch 55, loss: 19908.856467\n",
      "Epoch 56, loss: 19899.957410\n",
      "Epoch 57, loss: 19891.304783\n",
      "Epoch 58, loss: 19883.553745\n",
      "Epoch 59, loss: 19875.290072\n",
      "Epoch 60, loss: 19866.839697\n",
      "Epoch 61, loss: 19858.685791\n",
      "Epoch 62, loss: 19851.619627\n",
      "Epoch 63, loss: 19843.287683\n",
      "Epoch 64, loss: 19835.732035\n",
      "Epoch 65, loss: 19827.988026\n",
      "Epoch 66, loss: 19820.780679\n",
      "Epoch 67, loss: 19813.598643\n",
      "Epoch 68, loss: 19805.936719\n",
      "Epoch 69, loss: 19799.260926\n",
      "Epoch 70, loss: 19792.406617\n",
      "Epoch 71, loss: 19785.243489\n",
      "Epoch 72, loss: 19778.358405\n",
      "Epoch 73, loss: 19771.871427\n",
      "Epoch 74, loss: 19765.544073\n",
      "Epoch 75, loss: 19759.089856\n",
      "Epoch 76, loss: 19753.205724\n",
      "Epoch 77, loss: 19746.348934\n",
      "Epoch 78, loss: 19740.439781\n",
      "Epoch 79, loss: 19734.576714\n",
      "Epoch 80, loss: 19727.965314\n",
      "Epoch 81, loss: 19721.669639\n",
      "Epoch 82, loss: 19715.775581\n",
      "Epoch 83, loss: 19710.178297\n",
      "Epoch 84, loss: 19704.732694\n",
      "Epoch 85, loss: 19699.065685\n",
      "Epoch 86, loss: 19693.027143\n",
      "Epoch 87, loss: 19687.388593\n",
      "Epoch 88, loss: 19682.777006\n",
      "Epoch 89, loss: 19676.911819\n",
      "Epoch 90, loss: 19671.589609\n",
      "Epoch 91, loss: 19666.936506\n",
      "Epoch 92, loss: 19661.742760\n",
      "Epoch 93, loss: 19656.291003\n",
      "Epoch 94, loss: 19651.216622\n",
      "Epoch 95, loss: 19646.161461\n",
      "Epoch 96, loss: 19641.469956\n",
      "Epoch 97, loss: 19636.528117\n",
      "Epoch 98, loss: 19631.852242\n",
      "Epoch 99, loss: 19627.452280\n",
      "Epoch 100, loss: 19622.959845\n",
      "Epoch 101, loss: 19617.615895\n",
      "Epoch 102, loss: 19613.497788\n",
      "Epoch 103, loss: 19609.124835\n",
      "Epoch 104, loss: 19604.429321\n",
      "Epoch 105, loss: 19600.344717\n",
      "Epoch 106, loss: 19596.097483\n",
      "Epoch 107, loss: 19592.062713\n",
      "Epoch 108, loss: 19587.535755\n",
      "Epoch 109, loss: 19583.092965\n",
      "Epoch 110, loss: 19579.380399\n",
      "Epoch 111, loss: 19575.254081\n",
      "Epoch 112, loss: 19570.487803\n",
      "Epoch 113, loss: 19566.861554\n",
      "Epoch 114, loss: 19563.257536\n",
      "Epoch 115, loss: 19559.269412\n",
      "Epoch 116, loss: 19555.404856\n",
      "Epoch 117, loss: 19550.999235\n",
      "Epoch 118, loss: 19547.424554\n",
      "Epoch 119, loss: 19544.235909\n",
      "Epoch 120, loss: 19539.994288\n",
      "Epoch 121, loss: 19536.382119\n",
      "Epoch 122, loss: 19533.062025\n",
      "Epoch 123, loss: 19529.052069\n",
      "Epoch 124, loss: 19526.047736\n",
      "Epoch 125, loss: 19522.030138\n",
      "Epoch 126, loss: 19519.045580\n",
      "Epoch 127, loss: 19515.530977\n",
      "Epoch 128, loss: 19512.104380\n",
      "Epoch 129, loss: 19508.416423\n",
      "Epoch 130, loss: 19505.048162\n",
      "Epoch 131, loss: 19501.792178\n",
      "Epoch 132, loss: 19498.257291\n",
      "Epoch 133, loss: 19495.338025\n",
      "Epoch 134, loss: 19492.488837\n",
      "Epoch 135, loss: 19489.144396\n",
      "Epoch 136, loss: 19485.604858\n",
      "Epoch 137, loss: 19482.865991\n",
      "Epoch 138, loss: 19479.254159\n",
      "Epoch 139, loss: 19476.548712\n",
      "Epoch 140, loss: 19473.504172\n",
      "Epoch 141, loss: 19470.601429\n",
      "Epoch 142, loss: 19467.387913\n",
      "Epoch 143, loss: 19464.095080\n",
      "Epoch 144, loss: 19461.366060\n",
      "Epoch 145, loss: 19458.777225\n",
      "Epoch 146, loss: 19455.618891\n",
      "Epoch 147, loss: 19452.949783\n",
      "Epoch 148, loss: 19450.405683\n",
      "Epoch 149, loss: 19447.938509\n",
      "Epoch 150, loss: 19444.860385\n",
      "Epoch 151, loss: 19442.024763\n",
      "Epoch 152, loss: 19439.202217\n",
      "Epoch 153, loss: 19437.036400\n",
      "Epoch 154, loss: 19433.608885\n",
      "Epoch 155, loss: 19431.230538\n",
      "Epoch 156, loss: 19428.418084\n",
      "Epoch 157, loss: 19426.106283\n",
      "Epoch 158, loss: 19423.419415\n",
      "Epoch 159, loss: 19420.441687\n",
      "Epoch 160, loss: 19417.844073\n",
      "Epoch 161, loss: 19415.296675\n",
      "Epoch 162, loss: 19412.550508\n",
      "Epoch 163, loss: 19410.710994\n",
      "Epoch 164, loss: 19408.022999\n",
      "Epoch 165, loss: 19405.576467\n",
      "Epoch 166, loss: 19402.842954\n",
      "Epoch 167, loss: 19400.852509\n",
      "Epoch 168, loss: 19398.384721\n",
      "Epoch 169, loss: 19395.779389\n",
      "Epoch 170, loss: 19392.900361\n",
      "Epoch 171, loss: 19390.890125\n",
      "Epoch 172, loss: 19389.077132\n",
      "Epoch 173, loss: 19386.234394\n",
      "Epoch 174, loss: 19384.182115\n",
      "Epoch 175, loss: 19381.562279\n",
      "Epoch 176, loss: 19379.281041\n",
      "Epoch 177, loss: 19377.222755\n",
      "Epoch 178, loss: 19374.893444\n",
      "Epoch 179, loss: 19372.581773\n",
      "Epoch 180, loss: 19370.428849\n",
      "Epoch 181, loss: 19368.266429\n",
      "Epoch 182, loss: 19366.096386\n",
      "Epoch 183, loss: 19364.036346\n",
      "Epoch 184, loss: 19361.746691\n",
      "Epoch 185, loss: 19359.577738\n",
      "Epoch 186, loss: 19357.747667\n",
      "Epoch 187, loss: 19354.999182\n",
      "Epoch 188, loss: 19353.679104\n",
      "Epoch 189, loss: 19351.087354\n",
      "Epoch 190, loss: 19349.557997\n",
      "Epoch 191, loss: 19347.484645\n",
      "Epoch 192, loss: 19344.691270\n",
      "Epoch 193, loss: 19342.472500\n",
      "Epoch 194, loss: 19341.743482\n",
      "Epoch 195, loss: 19339.232501\n",
      "Epoch 196, loss: 19336.640129\n",
      "Epoch 197, loss: 19334.970480\n",
      "Epoch 198, loss: 19333.277414\n",
      "Epoch 199, loss: 19330.704488\n",
      "1e-05 1e-05 0.232\n",
      "Epoch 0, loss: 20714.289382\n",
      "Epoch 1, loss: 20688.380170\n",
      "Epoch 2, loss: 20664.260468\n",
      "Epoch 3, loss: 20641.872881\n",
      "Epoch 4, loss: 20618.930301\n",
      "Epoch 5, loss: 20597.181663\n",
      "Epoch 6, loss: 20576.050879\n",
      "Epoch 7, loss: 20554.699807\n",
      "Epoch 8, loss: 20534.667429\n",
      "Epoch 9, loss: 20514.690778\n",
      "Epoch 10, loss: 20495.552202\n",
      "Epoch 11, loss: 20476.437459\n",
      "Epoch 12, loss: 20457.819453\n",
      "Epoch 13, loss: 20439.580678\n",
      "Epoch 14, loss: 20421.963043\n",
      "Epoch 15, loss: 20404.056150\n",
      "Epoch 16, loss: 20387.474371\n",
      "Epoch 17, loss: 20369.714832\n",
      "Epoch 18, loss: 20353.154084\n",
      "Epoch 19, loss: 20337.322216\n",
      "Epoch 20, loss: 20321.627985\n",
      "Epoch 21, loss: 20305.703189\n",
      "Epoch 22, loss: 20290.742768\n",
      "Epoch 23, loss: 20275.208320\n",
      "Epoch 24, loss: 20260.443848\n",
      "Epoch 25, loss: 20246.363465\n",
      "Epoch 26, loss: 20232.072221\n",
      "Epoch 27, loss: 20217.864346\n",
      "Epoch 28, loss: 20204.117164\n",
      "Epoch 29, loss: 20190.769872\n",
      "Epoch 30, loss: 20177.487905\n",
      "Epoch 31, loss: 20164.832603\n",
      "Epoch 32, loss: 20151.903653\n",
      "Epoch 33, loss: 20139.508227\n",
      "Epoch 34, loss: 20126.706579\n",
      "Epoch 35, loss: 20114.583295\n",
      "Epoch 36, loss: 20102.432480\n",
      "Epoch 37, loss: 20090.905486\n",
      "Epoch 38, loss: 20079.295795\n",
      "Epoch 39, loss: 20067.407467\n",
      "Epoch 40, loss: 20056.507085\n",
      "Epoch 41, loss: 20045.499342\n",
      "Epoch 42, loss: 20034.936686\n",
      "Epoch 43, loss: 20023.866546\n",
      "Epoch 44, loss: 20013.729734\n",
      "Epoch 45, loss: 20003.497143\n",
      "Epoch 46, loss: 19993.299079\n",
      "Epoch 47, loss: 19983.508371\n",
      "Epoch 48, loss: 19973.561539\n",
      "Epoch 49, loss: 19963.706860\n",
      "Epoch 50, loss: 19954.359969\n",
      "Epoch 51, loss: 19944.947787\n",
      "Epoch 52, loss: 19935.573251\n",
      "Epoch 53, loss: 19926.514301\n",
      "Epoch 54, loss: 19918.045260\n",
      "Epoch 55, loss: 19909.592470\n",
      "Epoch 56, loss: 19900.668366\n",
      "Epoch 57, loss: 19892.168111\n",
      "Epoch 58, loss: 19883.995464\n",
      "Epoch 59, loss: 19875.640238\n",
      "Epoch 60, loss: 19867.765894\n",
      "Epoch 61, loss: 19859.365168\n",
      "Epoch 62, loss: 19851.627459\n",
      "Epoch 63, loss: 19844.157369\n",
      "Epoch 64, loss: 19836.178821\n",
      "Epoch 65, loss: 19828.624964\n",
      "Epoch 66, loss: 19820.932400\n",
      "Epoch 67, loss: 19814.005713\n",
      "Epoch 68, loss: 19806.747261\n",
      "Epoch 69, loss: 19799.900375\n",
      "Epoch 70, loss: 19792.794763\n",
      "Epoch 71, loss: 19786.244877\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72, loss: 19778.849663\n",
      "Epoch 73, loss: 19772.188392\n",
      "Epoch 74, loss: 19766.194995\n",
      "Epoch 75, loss: 19759.266750\n",
      "Epoch 76, loss: 19752.938985\n",
      "Epoch 77, loss: 19747.185769\n",
      "Epoch 78, loss: 19740.398119\n",
      "Epoch 79, loss: 19733.986696\n",
      "Epoch 80, loss: 19728.764545\n",
      "Epoch 81, loss: 19722.443030\n",
      "Epoch 82, loss: 19716.388142\n",
      "Epoch 83, loss: 19710.613776\n",
      "Epoch 84, loss: 19704.929480\n",
      "Epoch 85, loss: 19699.406267\n",
      "Epoch 86, loss: 19694.266768\n",
      "Epoch 87, loss: 19688.159972\n",
      "Epoch 88, loss: 19683.205372\n",
      "Epoch 89, loss: 19677.888216\n",
      "Epoch 90, loss: 19672.314187\n",
      "Epoch 91, loss: 19666.916545\n",
      "Epoch 92, loss: 19662.025516\n",
      "Epoch 93, loss: 19657.078306\n",
      "Epoch 94, loss: 19652.335276\n",
      "Epoch 95, loss: 19646.773029\n",
      "Epoch 96, loss: 19642.146672\n",
      "Epoch 97, loss: 19637.459346\n",
      "Epoch 98, loss: 19632.569234\n",
      "Epoch 99, loss: 19627.923446\n",
      "Epoch 100, loss: 19623.229555\n",
      "Epoch 101, loss: 19618.150020\n",
      "Epoch 102, loss: 19614.177490\n",
      "Epoch 103, loss: 19609.660955\n",
      "Epoch 104, loss: 19605.109907\n",
      "Epoch 105, loss: 19600.696684\n",
      "Epoch 106, loss: 19596.812984\n",
      "Epoch 107, loss: 19592.164646\n",
      "Epoch 108, loss: 19587.832107\n",
      "Epoch 109, loss: 19583.810695\n",
      "Epoch 110, loss: 19579.418642\n",
      "Epoch 111, loss: 19575.695189\n",
      "Epoch 112, loss: 19571.548611\n",
      "Epoch 113, loss: 19567.605865\n",
      "Epoch 114, loss: 19563.899941\n",
      "Epoch 115, loss: 19559.766286\n",
      "Epoch 116, loss: 19555.658576\n",
      "Epoch 117, loss: 19552.040449\n",
      "Epoch 118, loss: 19548.350830\n",
      "Epoch 119, loss: 19544.586777\n",
      "Epoch 120, loss: 19540.466336\n",
      "Epoch 121, loss: 19536.994633\n",
      "Epoch 122, loss: 19533.172035\n",
      "Epoch 123, loss: 19529.743581\n",
      "Epoch 124, loss: 19526.128335\n",
      "Epoch 125, loss: 19522.630481\n",
      "Epoch 126, loss: 19519.464845\n",
      "Epoch 127, loss: 19515.623904\n",
      "Epoch 128, loss: 19512.111617\n",
      "Epoch 129, loss: 19509.216270\n",
      "Epoch 130, loss: 19505.233644\n",
      "Epoch 131, loss: 19502.598253\n",
      "Epoch 132, loss: 19499.143653\n",
      "Epoch 133, loss: 19495.741687\n",
      "Epoch 134, loss: 19492.369363\n",
      "Epoch 135, loss: 19489.126029\n",
      "Epoch 136, loss: 19486.759040\n",
      "Epoch 137, loss: 19482.953960\n",
      "Epoch 138, loss: 19480.146717\n",
      "Epoch 139, loss: 19477.323742\n",
      "Epoch 140, loss: 19473.644573\n",
      "Epoch 141, loss: 19471.110412\n",
      "Epoch 142, loss: 19468.537693\n",
      "Epoch 143, loss: 19465.051037\n",
      "Epoch 144, loss: 19461.907003\n",
      "Epoch 145, loss: 19459.203321\n",
      "Epoch 146, loss: 19456.163578\n",
      "Epoch 147, loss: 19453.878080\n",
      "Epoch 148, loss: 19451.333327\n",
      "Epoch 149, loss: 19447.628802\n",
      "Epoch 150, loss: 19445.111900\n",
      "Epoch 151, loss: 19442.229005\n",
      "Epoch 152, loss: 19439.704165\n",
      "Epoch 153, loss: 19436.812910\n",
      "Epoch 154, loss: 19434.144759\n",
      "Epoch 155, loss: 19431.599527\n",
      "Epoch 156, loss: 19429.133031\n",
      "Epoch 157, loss: 19425.878230\n",
      "Epoch 158, loss: 19423.217755\n",
      "Epoch 159, loss: 19420.974464\n",
      "Epoch 160, loss: 19417.971779\n",
      "Epoch 161, loss: 19415.910816\n",
      "Epoch 162, loss: 19413.403992\n",
      "Epoch 163, loss: 19410.810801\n",
      "Epoch 164, loss: 19408.408135\n",
      "Epoch 165, loss: 19405.753273\n",
      "Epoch 166, loss: 19403.279869\n",
      "Epoch 167, loss: 19400.941492\n",
      "Epoch 168, loss: 19398.754470\n",
      "Epoch 169, loss: 19396.610330\n",
      "Epoch 170, loss: 19393.814825\n",
      "Epoch 171, loss: 19391.516634\n",
      "Epoch 172, loss: 19388.751192\n",
      "Epoch 173, loss: 19386.942532\n",
      "Epoch 174, loss: 19384.405291\n",
      "Epoch 175, loss: 19382.673576\n",
      "Epoch 176, loss: 19379.423509\n",
      "Epoch 177, loss: 19377.558505\n",
      "Epoch 178, loss: 19375.044358\n",
      "Epoch 179, loss: 19373.156000\n",
      "Epoch 180, loss: 19371.261029\n",
      "Epoch 181, loss: 19369.206374\n",
      "Epoch 182, loss: 19366.488377\n",
      "Epoch 183, loss: 19364.529649\n",
      "Epoch 184, loss: 19362.611729\n",
      "Epoch 185, loss: 19360.479080\n",
      "Epoch 186, loss: 19357.786831\n",
      "Epoch 187, loss: 19355.667860\n",
      "Epoch 188, loss: 19354.073303\n",
      "Epoch 189, loss: 19351.795486\n",
      "Epoch 190, loss: 19349.559825\n",
      "Epoch 191, loss: 19347.623710\n",
      "Epoch 192, loss: 19345.531499\n",
      "Epoch 193, loss: 19343.247343\n",
      "Epoch 194, loss: 19341.112101\n",
      "Epoch 195, loss: 19339.835481\n",
      "Epoch 196, loss: 19337.254888\n",
      "Epoch 197, loss: 19335.432367\n",
      "Epoch 198, loss: 19333.411239\n",
      "Epoch 199, loss: 19331.497866\n",
      "1e-05 1e-06 0.229\n",
      "best validation accuracy achieved: 0.254000\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 200\n",
    "batch_size = 300\n",
    "\n",
    "learning_rates = [1e-3, 1e-4, 1e-5]\n",
    "reg_strengths = [1e-4, 1e-5, 1e-6]\n",
    "\n",
    "best_classifier = None\n",
    "best_val_accuracy = None\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for reg_str in reg_strengths:\n",
    "        classifier = linear_classifer.LinearSoftmaxClassifier()\n",
    "        classifier.fit(train_X, train_y, epochs=num_epochs, learning_rate=lr, batch_size=batch_size, reg=reg_str)\n",
    "        pred = classifier.predict(val_X)\n",
    "        accuracy = multiclass_accuracy(pred, val_y)\n",
    "        if best_val_accuracy is None or best_val_accuracy < accuracy:\n",
    "            best_val_accuracy = accuracy\n",
    "            best_classifier = classifier\n",
    "        print(lr, reg_str, accuracy)\n",
    "            \n",
    "print('best validation accuracy achieved: %f' % best_val_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Какой же точности мы добились на тестовых данных?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear softmax classifier test set accuracy: 0.210000\n"
     ]
    }
   ],
   "source": [
    "test_pred = best_classifier.predict(test_X)\n",
    "test_accuracy = multiclass_accuracy(test_pred, test_y)\n",
    "print('Linear softmax classifier test set accuracy: %f' % (test_accuracy, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
